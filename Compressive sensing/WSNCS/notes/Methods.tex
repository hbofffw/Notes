\chapter{Methods}
\section{Compressive Wireless Sensing (CWS)}

Consider a wireless sensor network with $n$ nodes where each node takes a noisy sample of the form
\begin{equation}
    x_{j} = x^{*}_{j} + w_{j}, \qquad j = 1,\dots,n
    \label{eq2.1.3}
\end{equation}
and the errors ${w_j}_{j=1}^n$ are independent, zero-mean Gaussian random variables with variance $\sigma_w^2$. We can consider this data as a vector $x \in \mathbb{R}^{n}$ such that $x = x^*+w$, where $x^* \in \mathbb{R}$ is the noiseless data vector and $w \sim \mathcal{N}(0,\sigma^2_w\mathbf{I}_n)$. The author further assumed that $\left|x_j^*\right| \leq B, \ j=1,\dots,n$, for some constant $B > 0$, which is determined by the \textbf{\textcolor[rgb]{1,0,0}{sensing range of the sensors}}.

The existing  compression standards such as JPEG, MPEG and MP3, that data in real world often contain redundancies. Moreover, data collected at nearby nodes in a dense sensor network is expected to be highly correlated. Therefore, $x^*$ is assumed compressible in the sense that it is well-approximated by a linear combination of $k$ vectors taken from an orthonormal basis of $\mathbb{R}^n$. PS: (\emph{\textcolor[rgb]{1,0,0}{smooth signals tend to be compressible}} and \emph{\textcolor[rgb]{1,0,0}{piecewise smooth signals tend to be compressible in a wavelet/wedgelet basis}}). More precisely, let $\Psi \triangleq \{\psi_i\}^n_{i=1}$ be an orthonormal basis of $\mathbb{R}^n$. Denote by $\theta_i = \psi^T_i x^*$ (\emph{\textcolor[rgb]{1,0,0}{projection of $x^*$ onto $\psi_i$}}) the coefficients of $x^*$ in this new basis.
\begin{equation}
    |\theta_1| \geq |\theta_2| \geq \dots \geq |\theta_n|
    \label{eq2.1.4}
\end{equation}

The best $k-term$ approximation of $x^*$ in terms of $\Psi$ is given by 
\begin{equation}
    x^{*(k)} = \sum\limits_{i+1}^{k}\theta_i\psi_i 
    \label{eq2.1.5}
\end{equation}
then $x^*$ is \emph{\textcolor[rgb]{1,0,0}{$\alpha-$compressible}} in $\Psi$ (\emph{\textcolor[rgb]{1,0,0}{or that $\Psi$ is the compressing basis of $x^*$}}) if the \textcolor[rgb]{1,0,0}{average squared-error} behaves like
\begin{equation}
    \dfrac{\left\|x^*-x^{*(k)}\right\|^2}{n} \triangleq \dfrac{1}{n} \sum\limits_{j=1}^{n}\left( x^* - x_j^{8(k)} \right)^2 = O (k^{-2\alpha})
    \label{eq2.1.6}
\end{equation} 
the parameter $\alpha$ governs the degree to which $x^*$ is compressible with respect to $\Psi$. The ordering of coefficients in \cref{eq2.1.4} may be a function of the underlying signal $x^*$ and in such cases, could never be known a priori. The goal is to compute a reconstruction $\hat{x}$ of $x^*$ at FC with a small latency and expected squared-error \[
    D = \mathbb{E}[\dfrac{1}{n}\|\hat{x}-x^*\|^2] 
\], meanwhile consuming minimal amount of total power $P_{tot}$.

\textbf{\textcolor[rgb]{1,0,0}{Ideal Centralized Estimation}}

Given $x$, a centralized estimator $\hat{x}_{cen}$ at the FC can be easily constructed by projecting $x$ onto the first $k$ elements of $\Psi$

\begin{eqnarray}
    \label{eq2.1.7}
    \hat{x}_{cen} &=& \sum\limits_{i = 1}^{k} \left( \psi_i^T x\right)\psi_i \notag \\
    &=& x^{*(k)} + \sum\limits_{i=1}^{k}\left( \psi_i^T w \right)\psi_i 
\end{eqnarray}
this equation can be deducted from \cref{eq2.1.3},\cref{eq2.1.4}, \cref{eq2.1.5}.

A bias/variance trade-off then
\begin{equation}
    D_{cen} = \mathbb{E}\left[ \dfrac{1}{n}\|\hat{x}_{cen}-x^*\|^2 \right] \preceq k^{-2\alpha}+\left( \dfrac{k}{n} \right)\sigma_w^2
    \label{eq2.1.8}
\end{equation}
where the first term is \emph{\textcolor[rgb]{1,0,0}{the squared bias}} and the second is \emph{\textcolor[rgb]{1,0,0}{the variance}}. The minimum is attained by setting $k \sim n^{1/(2\alpha+1)}$, resulting in
\begin{equation}
    D_{cen} \preceq n^{-2\alpha/(2\alpha+1)}
    \label{eq2.1.9}
\end{equation}

