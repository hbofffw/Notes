\chapter{Methods}
\section{Compressive Wireless Sensing (CWS)}

Consider a wireless sensor network with $n$ nodes where each node takes a noisy sample of the form
\begin{equation}
    x_{j} = x^{*}_{j} + w_{j}, \qquad j = 1,\dots,n
    \label{eq2.1.3}
\end{equation}
and the errors ${w_j}_{j=1}^n$ are independent, zero-mean Gaussian random variables with variance $\sigma_w^2$. We can consider this data as a vector $x \in \mathbb{R}^{n}$ such that $x = x^*+w$, where $x^* \in \mathbb{R}$ is the noiseless data vector and $w \sim \mathcal{N}(0,\sigma^2_w\mathbf{I}_n)$. The author further assumed that $\left|x_j^*\right| \leq B, \ j=1,\dots,n$, for some constant $B > 0$, which is determined by the \textbf{\textcolor[rgb]{1,0,0}{sensing range of the sensors}}.

The existing  compression standards such as JPEG, MPEG and MP3, that data in real world often contain redundancies. Moreover, data collected at nearby nodes in a dense sensor network is expected to be highly correlated. Therefore, $x^*$ is assumed compressible in the sense that it is well-approximated by a linear combination of $k$ vectors taken from an orthonormal basis of $\mathbb{R}^n$. PS: (\emph{\textcolor[rgb]{1,0,0}{smooth signals tend to be compressible}} and \emph{\textcolor[rgb]{1,0,0}{piecewise smooth signals tend to be compressible in a wavelet/wedgelet basis}}). More precisely, let $\Psi \triangleq \{\psi_i\}^n_{i=1}$ be an orthonormal basis of $\mathbb{R}^n$. Denote by $\theta_i = \psi^T_i x^*$ (\emph{\textcolor[rgb]{1,0,0}{projection of $x^*$ onto $\psi_i$}}) the coefficients of $x^*$ in this new basis.
\begin{equation}
    |\theta_1| \geq |\theta_2| \geq \dots \geq |\theta_n|
    \label{eq2.1.4}
\end{equation}

The best $k-term$ approximation of $x^*$ in terms of $\Psi$ is given by 
\begin{equation}
    x^{*(k)} = \sum\limits_{i+1}^{k}\theta_i\psi_i 
    \label{eq2.1.5}
\end{equation}
then $x^*$ is \emph{\textcolor[rgb]{1,0,0}{$\alpha-$compressible}} in $\Psi$ (\emph{\textcolor[rgb]{1,0,0}{or that $\Psi$ is the compressing basis of $x^*$}}) if the \textcolor[rgb]{1,0,0}{average squared-error} behaves like
\begin{equation}
    \dfrac{\left\|x^*-x^{*(k)}\right\|^2}{n} \triangleq \dfrac{1}{n} \sum\limits_{j=1}^{n}\left( x^* - x_j^{8(k)} \right)^2 = O (k^{-2\alpha})
    \label{eq2.1.6}
\end{equation} 
the parameter $\alpha$ governs the degree to which $x^*$ is compressible with respect to $\Psi$. The ordering of coefficients in \cref{eq2.1.4} may be a function of the underlying signal $x^*$ and in such cases, could never be known a priori. The goal is to compute a reconstruction $\hat{x}$ of $x^*$ at FC with a small latency and expected squared-error \[
    D = \mathbb{E}[\dfrac{1}{n}\|\hat{x}-x^*\|^2] 
\], meanwhile consuming minimal amount of total power $P_{tot}$.

\textbf{\textcolor[rgb]{1,0,0}{Ideal Centralized Estimation}}

Given $x$, a centralized estimator $\hat{x}_{cen}$ at the FC can be easily constructed by projecting $x$ onto the first $k$ elements of $\Psi$

\begin{eqnarray}
    \label{eq2.1.7}
    \hat{x}_{cen} &=& \sum\limits_{i = 1}^{k} \left( \psi_i^T x\right)\psi_i \notag \\
    &=& x^{*(k)} + \sum\limits_{i=1}^{k}\left( \psi_i^T w \right)\psi_i 
\end{eqnarray}
this equation can be deducted from \cref{eq2.1.3},\cref{eq2.1.4}, \cref{eq2.1.5}.

A bias/variance trade-off then
\begin{equation}
    D_{cen} = \mathbb{E}\left[ \dfrac{1}{n}\|\hat{x}_{cen}-x^*\|^2 \right] \preceq k^{-2\alpha}+\left( \dfrac{k}{n} \right)\sigma_w^2
    \label{eq2.1.8}
\end{equation}
where the first term is \emph{\textcolor[rgb]{1,0,0}{the squared bias}} and the second is \emph{\textcolor[rgb]{1,0,0}{the variance}}. The minimum is attained by setting $k \sim n^{1/(2\alpha+1)}$, resulting in
\begin{equation}
    D_{cen} \preceq n^{-2\alpha/(2\alpha+1)}
    \label{eq2.1.9}
\end{equation}

\begin{definition}
    \label{def2.1.1}
    Let $q\in\mathbb{R}^n$ and $S_p\ :\ \mathbb{R}^n \rightarrow P (\{1,\dots,n\})$, where $P(X)$ means power set of $X$. We call $S_{p}$ the sparsity map of $q$ if $S_q(q)=\{j \in \{1,\dots,n\}\ :\ q_j\neq0\}$ and $|S_p(q)|$ is a counting measure on $S_p(q)$.
\end{definition} 

Let $\varphi \in \mathbb{R}^N$, where $\|\varphi\|^2=1$, and $v = \varphi^Tx^* = \sum_{j=1}^{n}\varphi_jx_j^*$ be the projection of $x^*$ onto $\varphi$. Using the notion of sparsity map, denote $|S_p(\varphi)| = n_{\varphi}$. Since $\|\varphi\|^2 = 1$, this implies $|\varphi_j|^2 \approx \|\varphi\|^2/n_{\varphi}=1/n_{\varphi} \ \forall j \in S_p(\varphi)$. The goal is to compute an estimate $(\hat{v})$ of $v$ at the FC given $x$.

\emph{\textcolor[rgb]{1,0,0}{One possibility is to nominate a clusterhead in the network and then, assuming at the sensor nodes know $\varphi$ and have constructed routes which form a spanning tree through the network to the clusterhead, sensor nodes locally compute $\varphi_j x_j$ and aggregate these values up the tree to obtain $\hat{v} = \sum_{j=1}^{n}\varphi_jx_j$ at the clusterhead. However, even if we ignore the communication cost of delivering $\hat{v}$ from the clusterhead to the FC, it is easy to see that this shceme requires at least $n$ transmissions.}} 一个可能性是在网络中指定簇头，假设每个传感器节点已知$\varphi$并且可以生成到达簇头节点的生成树路径。传感器节点在本地计算$\varphi_jx_j$并且沿着树结构路径上传，在簇头得到一个聚合值$\hat{v}=\sum_{j=1}^{n}\varphi_jx_j$。然而，即便我们忽略了从簇头到FC传输$\hat{v}$的通信支出，也很容易看出该方案需要最少$n$个传输。

\emph{\textcolor[rgb]{1,0,0}{Another, more promising, alternative is to exploit recent results concerning uncoded coherent transmission schemes. The proposed distributed communication architecture involves phase-coherent, low-power, analog transmission of weighted sample values directly from the nodes in the network to the FC via the narrowband AWGN network-to-FC communication channel.To begin with, assume all the nodes in the network have knowledge of $\varphi$. Each node multiplies its measurement $x_j$ with $\sqrt{\rho}\varphi_jx_j$ to obtain $m_j=\sqrt{\rho}\varphi_jx_j$, where $\rho>0$ is a scaling factor used to satisfy sensors' transmit power constraint $P$, and all the nodes coherently transmit their respective $m_j$'s in an analog fashion over the network-to-FC communication channel}}.

$\mathbb{E}[|m_j|^2] \leq \rho(B^2+\sigma_w^2)/n_{\varphi}$ if $j\in S_p(x^*) \cap S_p(\varphi)$ and $\mathbb{E}[|m_j|^2] = \rho (B^2+\sigma_w^2)/n_{\varphi}$ if $j \in S_p(x^*)^c \cap S_p(\varphi)$. Thus, $\mathbb{E}[|m_j|^2] = \rho(B^2 + \sigma_w^2)/n_{\varphi} \ \forall\ j \in S_p(\varphi)$ and $m_j \equiv 0$ if $j \notin S_p(\varphi)$. Hence, the average transmission power for each sensor $(\in S_p(\varphi))$ is given by

\begin{equation}
    P_j \leq \rho(B^2+\sigma_w^2)/n_{\varphi}
    \label{eq2.1.10}
\end{equation}
and to satisfy the individual sensor transmit power constraint, we need to take $\rho = (n_{\varphi}\lambda P)/(B^2+\sigma_w^2)$ for $0<\lambda\leq 1$, resulting in $P_j \leq \lambda P(\leq P)$. 

The network-to-FC communication channel is effectively transformed into an AWGN MAC channel and the received signal at the FC is given by
\begin{eqnarray}
    \label{eq2.1.11}
    r &=& \sum\limits_{j=1}^{n}m_j+z = \sqrt{\rho}\sum\limits_{j=1}^{n}\varphi_jx_j+z \notag \\
    &=& \sqrt{\rho}\varphi^T(x^*+w)+z = \sqrt{\rho}(v+\tilde{w})+z
\end{eqnarray}
where $z \sim \mathcal{N}(0,\sigma^2_z)$ is the channel additive white Gaussian noise and $\tilde{w} \sim \mathcal{N}(0,\sigma_w^2)$. \emph{\textcolor[rgb]{1,0,0}{Strictly speaking, the received signal from each node, $m_j$, in the above expression should be scaled by an attenuation constant, $a_j \in (0,1)$, that depends on the distance $d_j$ between the node and FC and the path loss exponent}}.
\emph{\textcolor[rgb]{0,0,1}{However, under the assumption of identical path losses, the $a_j$'s are nearly the same and we ignore this uniform attenuation since it will uniformly increase the required power per node by a constant factor to attain a desired distortion}}.

The above setup corresponds to obtaining a noisy projection of x onto $\varphi$ at the FC that is scaled by $\sqrt{\rho}$ and given $r$, the FC can easily estimate $v$ as $\hat{v} = r /\sqrt{\rho}$ and the resulting distortion is given by 
\begin{eqnarray}
    \label{eq2.1.12}
    D_v &=& \mathbb{E}[|\hat{v} - v|^2] = \sigma_w^2 + \dfrac{\sigma_z^2}{\rho} \notag \\
    &=& \sigma_w^2 + \dfrac{\sigma_z^2(B^2+\sigma_w^2)}{n_{\varphi}\lambda P}
\end{eqnarray}
the first term is due to the \textcolor[rgb]{1,0,0}{measurement noise} and the second term is due to the \textcolor[rgb]{1,0,0}{communication noise} and the key question becomes: \textcolor[rgb]{1,0,0}{What is the necessary and sufficient value of $\lambda$ (and correspondingly of $\rho$) to make the distortion in \cref{eq2.1.12} as small as possible?}

\begin{theorem}
    \label{thm2.1.1}
    Given the observation model of \cref{eq2.1.3}, it is possible to obtain an estimate $(\hat{v})$ of the projection of sensor network data onto any normalized vector in $\mathbb{R}^n$, such that $D_v \sim \sigma_w^2$, by using only a fixed amount of total power, $P_v = O(1)$, independent of the number of nodes in the network and the structure of the vector of the vectors on which data is projected.
\end{theorem}

\begin{proof}
    The first term in \cref{eq2.1.12} is unaffected by the proposed communication scheme and the second term decays as $1/\lambda$. Both the terms in \cref{eq2.1.12} must be of the same order for fastest distortion:
    \begin{equation}
        \sigma_w^2 \sim \dfrac{\sigma_z^2(B^2+\sigma_w^2)}{n_{\varphi}\lambda P} \Longleftrightarrow \lambda \sim \dfrac{\sigma_z^2(B^2+\sigma_w^2)}{n_{\varphi} \sigma_w^2 P}
        \label{eq2.1.13}
    \end{equation}
    Hence, the necessary and sufficient $\lambda$ to obtain the optimal distortion should be chosen as:
\begin{equation}
    \lambda \sim \dfrac{\sigma_z^2(B^2+\sigma_w^2)}{n_{\varphi}\sigma_w^2 P} \sim \dfrac{1}{n_{\varphi}}
    \label{eq2.1.14}
\end{equation}

and from \cref{eq2.1.12}, this would result in $D_v \sim \sigma_w^2$. Moreover, since a total of $n_{\varphi}$ nodes used the communication channel during this distributed projection, necessary and sufficient total power ($P_v$) involved in obtaining $\hat{v}$ at the FC would behave as 
\begin{equation}
    \sum_{j=1}^{n}P_j \leq (\lambda P) \sim \dfrac{\sigma_z^2(B^2+\sigma_w^2)}{\sigma_w^2} = O(1)
    \label{eq2.1.15}
\end{equation}
\end{proof}


\begin{remark}
    \label{rmk2.1.1}
    Given the observation model of \cref{eq2.1.3}, it is easy to see that $D_v \sim \sigma_w^2$ is the best that any (centralized or distributed) scheme can hope to achieve in terms of distortion and \cref{thm2.1.1} shows that the distributed scheme described in this paper can achieve that by using \textcolor[rgb]{1,0,0}{only a fixed amount of power}.
\end{remark}

Let $\Psi = \{\psi_i\}^n_{i=1}$ be orthonormal basis of $\mathbb{R}^n$ such that $\|x^*-x^{*(k)}\|^2 /n = O(k^{-2\alpha})$, where $x^{*(k)} = \sum_{i=1}^k \theta_i \psi_i$ (perhaps after re-labeling the indices $i$) and each coefficients $\theta_i$ is computed as a projection of the form $\theta_i = \psi_i^T x^* = \sum_{j=1}^n \psi_{ij}x_j^*$. The sensor network can compute $k$ projections of $x$ onto $\{\psi_i\}_{i=1}^k$ by employing the scheme in $k$ consecutive channel uses. Thus, \textcolor[rgb]{1,0,0}{at the end of $k$ channel uses, each one corresponding to a projection of $x$ onto an element of $\Psi$, FC has access to the estimates of $k$ projection coefficients given by}
\begin{equation}
    \hat{\theta} = r_i/\sqrt{\rho_i} = \theta_i = \psi_i^T w + z_i /\sqrt{\rho_i}, \quad i = 1,\dots,k
    \label{eq2.1.16}
\end{equation}

\subsection{Practical data compression in wireless sensor networks: A survey}

\textbf{Data compression algorithm classification}

The aggregation techniques are usually adopted in dense sensor networks with multi-hop topology which require routing algorithms. This aggregation function in existing literature was usually \emph{\textcolor[rgb]{1,0,0}{performed by extracting maximum, minimum and average values of aggregated data}}. In such way, it can \emph{\textcolor[rgb]{1,0,0}{reduce the amount of communicating data}} in dense networks which affect the power consumption but \textcolor[rgb]{0,0,1}{lose much of the original structure in the extracted data} because it \textcolor[rgb]{0,0,1}{provides only coarse statistics without local variations} such as \textcolor[rgb]{1,0,0}{data distribution over an area}\cite{1307317}. A number of papers solved this issue by using \textcolor[rgb]{1,0,0}{rules or factors to tune the degree of aggregation. This means that the aggregation is considered to operate based on the network's information, such as the differences of consecutive data, location of nodes and network capacity}. However, these improved schemes have the possibility of \textcolor[rgb]{0,0,1}{losing their original data structure} when there is a high degree of aggregation. 

One set of existing algorithms have been applied to solve original data structure loss problems. Since these compression algorithms were extended from aggregation techniques which operate on multi-hop network topology, they usually \textcolor[rgb]{1,0,0}{distribute a compression algorithm throughout the network}. Another set of existing algorithms are performed \textcolor[rgb]{1,0,0}{at each local node independently of other nodes}. The algorithms described above can be classified into two categories: \emph{\textcolor[rgb]{1,0,0}{distributed data compression approach}} and \emph{\textcolor[rgb]{1,0,0}{local data compression approach}}. 

\textbf{Data compresion algorithm in wireless sensor networks for real-world requirements}

Two sets of requirements in real-world WSN applications:
\begin{enumerate}
    \item The first set contains the common constraints of wireless sensor networks that should be considered in designing a data compression algorithm. 第一个集合包含了在设计无线传感网络数据压缩算法时，需要考虑的一般约束条件。
    \item The second set identifies the unique requirements related to the data compression designed for each particular existing real-world application. 第二个集合确定了，每一个特定的应用场景中设计压缩算法时，相应的独特唯一的必要条件。
\end{enumerate}

\emph{Common requirements in real-world WSN applications}

An efficient data compression algorithm needs to be designed by considering the trade-off between the computational cost and the power saving from compression ratio.

\emph{\textcolor[rgb]{1,0,0}{Distributed data compression approaches}}

\begin{enumerate}
    \item distributed source modeling (DSM);
    \item distributed transform coding (DTC);
    \item distributed source coding (DSC)
    \item compressed sensing (CS) techniques. 
\end{enumerate}

\subsubsection{Distributed source modeling (DSM)}
It aims to search for a function/model that best fits a set of input measurements acquired by a specific group of sensor nodes using \emph{\textcolor[rgb]{1,0,0}{parametric modeling}} and \emph{\textcolor[rgb]{1,0,0}{non-parametric modeling}}. 

\emph{\textcolor[rgb]{1,0,0}{parametric modeling}}

Using parametric modeling, an algorithm treats sensor data as a \textcolor[rgb]{1,0,0}{random process that has to be optimally estimated when knowing tis statistical parameters}, such as mean and variance. Parametric modeling could yield superior performance when the statistical structure of the random process (sensor data) being observed is known. 

\emph{\textcolor[rgb]{1,0,0}{Non-parametric modeling}}

Non-parametric modeling utilizes kernel-based regression to represent the sensor data where the regression coefficients are learned by treating the sensor data as input-output example pairs of some deterministic function observed in noise. In this case, it requires very little prior information about the nature of the data in the random process and is considered to be very robust. 

\subsubsection{Compressed sensing (CS)}

Given a set of sparse signal $\mathbf{x} = (x_{ij})_{n \times 1}$, we can find a random projection matrix $\mathbf{A} = (A_{i,j})_{k \times n}$ with far fewer rows than columns to obtain a small compressed data set as 
\begin{equation}
    \mathbf{y} = (y_{i,j})_{k \times 1} = \mathbf{Ax}.
    \label{eq2.3.33}
\end{equation}

Then, we can reconstruct an estimation of $\mathbf{x}$ by solving an optimization problem as follows:
\begin{equation*}
    \hat{\mathbf{x}} = \mathop{\mathrm{argmin}}\limits_z \|\mathbf{z}\|_1 \text{ subject to  }\mathbf{y} = \mathbf{Az},
\end{equation*}
where $\|\mathbf{z}\|_1 = \sum_{i=1}^{N}\left|z_i\right|$ denotes the $\ell_1$-norm.


A WSN with four sensor nodes and one sink node. If the projection vector is $\mathbf{A} = [0.2,0.1,0.3,0.4]$ and the projected value is $\mathbf{y} = \mathbf{Ax} = [0.2x_1+0.1x_2+0.3x_3+0.4x_4]$. The sink node can obtain this projected value without the sensor nodes sending their sensor readings to the sink node. This can be achieved by the sink node passing a message along the route $S-1-2-4-3-S$ using source routing in the WSN. This paper considered a case of projection vector $\mathbf{A} = [0.2,0,0.3,0]$. This sink node obtains the projected value $\mathbf{y} = \mathbf{Ax} = 0.2x_1+0.3x_3$ and the route is $S-1-3-S$. Thus, $\mathbf{A}$ is one of the key parameters that should be considered correctly in this scheme. Matrix $\mathbf{A}$ is mentioned in \cite{5044947} referred to as a \emph{\textcolor[rgb]{1,0,0}{routing matrix}}. When in some WSN applications (monitoring or data gathering), the observed $x$ may be not always sparse. Another key parameter is a \textcolor[rgb]{1,0,0}{suitable transformation that makes the signal sparse int its domain}. There must exist an invertible transformation matrix $\mathbf{T}$ of size $n \times n$ such that we can write
\begin{equation}
    \mathbf{x} = \mathbf{Ts}.
    \label{eq2.3.34}
\end{equation}
where $\mathbf{s}$ is sparse. Substituting \cref{eq2.3.34} to \cref{eq2.3.33}, the compressed data can be obtained as 
\begin{equation*}
    \mathbf{y} = \mathbf{Ax} = \mathbf{ATs}
\end{equation*}
Then the optimization problem for obtaining a recovered version of $\mathbf{s}$ can be written as: 
\begin{equation}
    \hat{\mathbf{s}}=\mathop{\mathrm{argmin}}\limits_z \|\mathbf{z}\|_1 \text{ subject to } \mathbf{y} = \mathbf{ATz}.
    \label{eq2.3.35}
\end{equation}
It was shown that the $\ell_1$-minimization could be solved with a linear programming (LP) technique \cite{Haupt2008}. The complexity \textcolor[rgb]{0,0,1}{increases dramatically when $n$ is very large} although the reconstruction complexity of the LP-based decoder is polynomial. There are several ongoing researches looking for low-complexity reconstruction techniques \cite{Tropp2007, Blumensath2008}. Once we obtained a sparse solution $\hat{\mathbf{s}}$, the original data $\mathbf{x}$ can be recovered through \cref{eq2.3.34}. 
If the projection matrix is $\mathbf{A} = [0.2, 0.1, 0.3, 0.4]$, the sink node obtains the projected value as $\mathbf{y} = \mathbf{Ax} = 0.2x_1+0.1x_2+0.3x_3+0.4x_4$ by passing a message along the route $S-1-2-4-3-S$ using source routing in the WSNs. 

\textbf{Some literatures}

An overview of the compressed sensing for network data can be found in \cite{Haupt2008} which also introduces the \emph{\textcolor[rgb]{1,0,0}{potential of using the compressive sampling theory for data aggregation in a multi-hop WSN}}. Based on the review above, the compressed sensing scheme does not require complicated computation at the sensor node. Moreover, the total amount of communication can be reduced greatly while the amount of information at the sink node is guaranteed. 

In \cite{5044947} and \cite{Luo2009}, the authors addressed the data gathering problem in WSNs, where \textcolor[rgb]{1,0,0}{routing was used in conjunction with compression to transport random projections of the data}. While the work of \cite{Luo2009} simulated the proposed concept using synthetic sample signals, the work of \cite{5044947} quantified the performance of compressed sensing in multi-hop wireless networks using both synthetic signals and real signals. Using synthetic signals which has sufficient sparsity, both works reported that compressed sensing achieved substantial gains compared to plain routing schemes. However, the work of \cite{5044947} considered real signals from different environmental phenomena. The main problem is to find \textcolor[rgb]{1,0,0}{the transform matrix $\mathbf{T}$ that makes the signal sparse}. In their experiments, \emph{\textcolor[rgb]{1,0,0}{Discrete Cosine Transformation (DCT)}} and \emph{\textcolor[rgb]{1,0,0}{Haar Wavelet transformation}} were tested. Neither of them were able to sparsify the data while \textcolor[rgb]{1,0,0}{at the same time being incoherent with respect to matrix $\mathbf{A}$ or the routing matrix}. They also pointed out that finding a suitable transformation with good sparsity and incoherence properties remained an open problem for data gathering in static WSNs. 

In addition to sparsity, the gathering cost also depends on the position of the sensors whose samples are aggregated in the measurements. Based on this motivation, authors in \cite{Lee2009} proposed a framework for efficient data gathering in WSNs using spatially localized sparse projections. For the purpose of designing \textcolor[rgb]{1,0,0}{distributed measurement strategies that are both sparse and spatially localized}, the scheme divides the network into \textcolor[rgb]{1,0,0}{clusters of adjacent nodes and forces projections to be obtained only from nodes within a cluster}. This means that routing matrix $\mathbf{A}$ has \textcolor[rgb]{1,0,0}{a diagonal structure}. This scheme introduces overlapping energy measurement among transform functions $\mathbf{T}$ and based on their analysis and experiment of independent and joint reconstruction methods, a joint reconstruction method is adopted for localized projections. Thus, it can exploit measurements in multiple clusters corresponding to energy in a given transform function $\mathbf{T}$ that overlaps those clusters. Comparing to the approach of \cite{5044947}, the method proposed in \cite{Lee2009} achieved \textcolor[rgb]{1,0,0}{power saving with localized aggregation and captured more evenly distributed energy of transform functions}. 

While most of the previous works proposed \textcolor[rgb]{0,0,1}{static compressed sensing} for WSNs, the author in \cite{Chou2009} proposed an adaptive algorithm based on the recently developed theory of adaptive compressive sensing 
