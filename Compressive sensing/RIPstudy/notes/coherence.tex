\chapter{Coherence}
\label{coherence}
\section{Definitions and Basic Properties}

\begin{definition}
    \label{def1.1}
    Let $\mathbf{A} \in \mathbb{C}^{m \times N}$ be a matrix with $\ell_2$-normalized columns $a_1,\dots,a_N$, i.e., $\|a_i\|_2=1$ for all $i \in [N]$. The \emph{coherence} $\mu = \mu(A)$ of the matrix $\mathbf{A}$ is defined as
    \begin{equation}
        \mu := \max\limits_{1 \leq i \neq j \leq N} \left|\left<a_i,a_j\right>\right|.
        \label{eq1.1}
    \end{equation}
\end{definition}

Next the more general concept of $\ell_1$-coherence function is introduced, which incorporates the usual coherence as the particular value $s=1$ of its argument.

\begin{definition}
    \label{def1.2}
    Let $\mathbf{A} \in \mathbb{C}^{m \times N}$ be a matrix with $\ell_2$-normalized columns $a_1,\dots,a_N$. The $\ell_1$-\emph{coherence function} $\mu_1$ of the matrix $\mathbf{A}$ is defined for $s \in [N-1]$ by
    \begin{equation*}
        \mu_1(s) := \max\limits_{i \in [N]} \max\left\{\sum\limits_{j \in S}\left|\left<a_i,a_j\right>\right|, S \subset [N], card(S) = s, i \notin S \right\}. 
    \end{equation*}
\end{definition}

It is straightforward to observe that, for $1 \leq s \leq N-1$,
\begin{equation}
    \mu \leq \mu_1(s) \leq s\mu,
    \label{eq1.2}
\end{equation}
and more generally that, for $1 \leq s, t \leq N-1$ with $s+t \leq N-1$,
\begin{equation}
    \max \{\mu_1(s), \mu_1(t)\} \leq \mu_1(s+t) \leq \mu_1(s) + \mu_1(t).
    \label{eq1.3}
\end{equation}

The coherence, more generally the $\ell_1$-coherence function, is invariant under multiplication on the left by a \emph{\textcolor[rgb]{1,0,0}{unitary matrix $\mathbf{U}$}}, for the columns of $\mathbf{UA}$ are the \emph{\textcolor[rgb]{1,0,0}{$\ell_2$-normalized vectors $\mathbf{U}a_1,\dots,\mathbf{U}a_N$}} and they satisfy $\left<\mathbf{U}a_i,\mathbf{U}a_j\right> = \left<a_i,a_j\right>$. Moreover, because of the \emph{Cauchy-Schwarz} inequality $\left|\left<a_i,a_j\right>\right| \leq \|a_i\|_2\|a_j\|_2$, it is clear that the coherence of a matrix is bounded above by one, i.e.,
\[\mu \leq 1\].

We observe that $\mu = 0$ if and only if the columns of $\mathbf{A}$ from an orthonormal system. In particular, in the case of a square matrix, we have $\mu= 0$ if and only if $\mathbf{A}$ is a \emph{\textcolor[rgb]{1,0,0}{unitary matrix}}. But from the \emph{compressive sensing} point of view, we only consider matrices $\mathbf{A} \in \mathbb{C}^{m \times N}$ with $m < N$. $\mathbf{A}_S$ denotes the matrix formed by the columns of $\mathbf{A} \in \mathbb{C}^{m \times N}$ indexed by \emph{a subset $S$ of $[N]$}. 

\begin{theorem}
    Let $\mathbf{A} \in \mathbb{C}^{m \times N}$ be a matrix with $\ell_2$-normalized columns and let $s \in [N]$. For all $s$-sparse vectors $x \in \mathbb{C}^{N}$,
    \[(1-\mu_1(s-1))\|x\|_2^2 \leq \|\mathbf{A}x\|_2^2 \leq (1+\mu_1(s-1))\|x\|_2^2,\footnote{It looks like RIP}\]
    or equivalently, for each set $S \subset [N]$ with card$(S) \leq s$, the eigenvalues of the matrix $\mathbf{A}_S^*\mathbf{A}_S$ lie in the interval $[1-\mu_1(s-1), 1+\mu_1(s-1)]$. In particular, if $\mu_1(s-1) < 1$, then $\mathbf{A}_S^*\mathbf{A}_S$ is invertible.
    \label{th1.3}
\end{theorem}

\begin{proof}
    For a set $S \subset [N]$ with $card(S) \leq s$, since the matrix $\mathbf{A}_{S}^*\mathbf{A}_S$ is positive semidefinite, it has an orthonormal basis of eigenvectors associated with real, positive eigenvalues. The minimal eigenvalue is denoted by $\lambda_{min}$ and the maximal eigenvalue by $\lambda_{max}$. Then, since $\mathbf{A}x = \mathbf{A}_Sx_S$ for any $x \in \mathbb{C}^N$ supported on $S$, it is easy to see that the maximum of 
    \[\|\mathbf{A}x\|_2^2 = \left<\mathbf{A}_Sx_S, \mathbf{A}_S x_S\right> = \left<\mathbf{A}_s^*\mathbf{A}_S x_S, x_S\right>\]
    over the set $\{x \in \mathbb{C}^N, supp x \subset S, \|x\|_2 = 1\}$ is $\lambda_{max}$ and that its minimum is $\lambda_{min}$. Due to the normalizations $\|a_j\|_2=1 $ for all $j \in [N]$, the diagonal entries of $\mathbf{A}_S^* \mathbf{A}_S$ all equal one. By \emph{Gershgorin's disk theorem \cref{tha.11}}, the eigenvalues of $\mathbf{A}_S^* \mathbf{A}_S$ are contained in the union of the disks centered at $1$ with radii
    \[r_j := \sum\limits_{\ell_S,\ell \neq j}\left|(\mathbf{A}_S^*\mathbf{A}_S)_{j,\ell}\right| = \sum\limits_{\ell \in S,\ell \neq j} \left|\left<a_{\ell},a_j\right>\right| \leq \mu_1(s-1), \qquad j\in S.\]
    \emph{\textcolor[rgb]{1,0,0}{Since these eigenvalues are real, they must lie in $\left[1- \mu_1(s-1), 1+\mu_1(s-1)\right]$, as announced.}}

    \begin{theorem}
        Let $\lambda$ be an eigenvalue of a square matrix $\mathbf{A} \in \mathbb{C}^{n \times n}$. There exists an index $j \in [n]$ such that 
        \[|\lambda - A_{j,j}| \leq \sum\limits_{\ell \in [n] \backslash \{j\}} |A_{j,\ell}|.\]
    \label{tha.11}
    \end{theorem}
\end{proof}
\begin{corollary}
    \label{cr1.4}
    Given a matrix $\mathbf{A} \in \mathbb{C}^{m \times N}$ with $\ell_2$-normalized columns and an integer $s \geq 1$, if 
    \[\mu_1(s)+\mu_1(s-1) < 1,\]
    then, for each set $S \subset [N]$ with card$(S) \leq 2s$, the matrix $\mathbf{A}_S^*\mathbf{A}_S$ is invertible and the matrix $\mathbf{A}_S$ injective. In particular, the conclusion holds if 
    \[\mu < \dfrac{1}{2s-1}.\]
\end{corollary}

\section{Matrices with Small Coherence}
The lower bounds is given for the coherence and for the $\ell_1$-coherence function of a matrix $\mathbf{A} \in \mathbb{C}^{m \times N}$ with $m < M$. An example of a matrix is given with an almost minimal coherence. The analysis is carried out for matrices $\mathbf{A} \in \mathbb{K}^{m \times N}$, where the field $\mathbb{K}$ can either be $\mathbb{R}$ or $\mathbb{C}$, because the matrices \emph{\textcolor[rgb]{1,0,0}{achieving the lower bounds have different features in the real and complex settings}}. In both cases, however, \emph{their columns are equiangular \textcolor[rgb]{1,0,0}{tight frames}, which are defined below}.
\begin{definition}
    \label{def1.5}
    A system of $\ell_2$-normalized vectors $(a_1,\dots,a_N)$ in $\mathbb{K}^m$ is called \textcolor[rgb]{1,0,0}{equiangular} if there is a constant $c \geq 0$ such that
    \begin{equation*}
        \left|\left<a_i,a_j\right>\right| = c \qquad \text{for all }i, j \in [N], i \neq j.
    \end{equation*}
\end{definition}

Tight frames can be defined by several conditions.

\begin{definition}
    \label{def1.6}
    A system of vectors $(a_1,\dots,a_N)$ in $\mathbb{K}^m$ is called a \textcolor[rgb]{1,0,0}{tight frame} if there exists a constant $\lambda > 0$ such that one of the following equivalent conditions holds:
    \begin{enumerate} [(a)]
        \item $\|x\|_2^2 = \lambda \sum\limits_{j = 1}^N\left|\left<x, a_j\right>\right|^2$ for all $x\in \mathbb{K}^m$,
        \item $x = \lambda \sum\limits_{j=1}^N\left<x,a_j\right>a_j$ for all $x \in \mathbb{K}^m$,
        \item $\mathbf{AA}^* = \dfrac{1}{\lambda}\mathbf{Id}_m$, where $\mathbf{A}$ is the matrix with columns $a_1,\dots,a_N$.
    \end{enumerate}
\end{definition}

A system of $\ell_2$-normalized vectors is called an \emph{\textbf{\textcolor[rgb]{1,0,0}{equiangular tight frame}}} if it is both an equiangular system and a tight frame. Such systems are the ones achieving the lower bound given below and known as the \emph{\textcolor[rgb]{1,0,0}{Welch bound}}.
\begin{theorem}
    \label{th1.7}
    The coherence of a matrix $\mathbf{A}\ in \mathbb{K}^{m \times N}$ with $\ell_2$-normalized columns satisfies
    \begin{equation}
        \label{eq1.4}
        \mu \geq \sqrt{\dfrac{N-m}{m(N-1)}}.
    \end{equation}
    Equality holds if and only if the columns $a_1,\dots,a_N$ of the matrix $\mathbf{A}$ form an equiangular tight frame.
\end{theorem}

\begin{proof}
    The \emph{\textcolor[rgb]{1,0,0}{Gramm matrix}} $\mathbf{G} := \mathbf{A}^*\mathbf{A} \in \mathbb{K}^{N \times N}$ of the system $(a_1,\dots,a_N)$ is introduced, which has entries
\begin{equation*}
    G_{i,j} = \overline{\left<a_i,a_j\right>} = \left<a_j,a_i\right>, \qquad i,j \in [N],
\end{equation*}
and the matrix $\mathbf{H} := \mathbf{AA}^* \in \mathbb{K}^{m \times m}$. On the other hand, since the system $(a_1,\dots,a_N)$ is $\ell_2$-normalized, we have 
\begin{equation}
    \label{eq1.5}
    tr(\mathbf{G}) = \sum\limits_{i = 1}^N \|a_i\|_2^2 = N.
\end{equation}
On the other hand, since the inner product
\begin{equation*}
    \left<\mathbf{U}, \mathbf{V}\right>_F := tr(\mathbf{UV}^*) = \sum\limits_{i,j=1}^n U_{i,j} \overline{V_{i,j}}
\end{equation*}
induces the so-called \emph{\textcolor[rgb]{1,0,0}{Froebenius norm}} $\|\cdot\|_F$ on $\mathbb{K}^{n \times n}$, the \emph{\textcolor[rgb]{1,0,0}{Cauchy-Schwarz inequality}} yields
\begin{equation}
    \label{eq1.6}
    tr(\mathbf{H}) = \left<\mathbf{H},\mathbf{Id}_m\right>_F \leq \|\mathbf{H}\|_F \|\mathbf{Id}_m\|_F = \sqrt{m} \sqrt{tr(\mathbf{HH}^*)}.
\end{equation}
Now it is observed that
\begin{eqnarray}
    tr(\mathbf{HH}^*) &=& tr(\mathbf{AA}^*\mathbf{AA}^*) = tr(\mathbf{A}^*\mathbf{AA}^*\mathbf{A}) =tr(\mathbf{GG}^*) = \sum\limits_{i,j = 1}^N \left|\left<a_i,a_j\right>\right|^2 \\
    &=& \sum\limits_{i=1}^N \|a_i\|_2^2 + \sum\limits_{i,j = 1, i \neq j}^N \left|\left<a_i,a_j\right>\right|^2 = N + \sum\limits_{i,j =1, i \neq j}^N \left|\left<a_i,a_j\right>\right|^2.
    \label{eq1.7}
\end{eqnarray}
In view of $tr(G) = tr(H)$, combining \cref{eq1.5,eq1.6,eq1.7} yields 
\begin{equation}
    N^2 \leq m \left( N+ \sum\limits_{i,j \neq 1, i\neq j}^N \left|\left<a_i,a_j\right>\right|^2 \right).
    \label{eq1.8}
\end{equation}
Taking into account that
\begin{equation}
    \left|\left<a_i,a_j\right>\right| \leq \mu \qquad \text{for all }i,j \in [N], i \neq i,
    \label{eq1.9}
\end{equation}
we obtain
\[
    N^2 \leq m(N+(N^2 - N)\mu^2),
\]
\end{proof}

The \emph{\textcolor[rgb]{1,0,0}{Welch bound}} can be extended to the $\ell_1$-coherence function for small values of its argument.

\begin{theorem}
    \label{1.8}
    The $\ell_1$coherence function of a matrix $\mathbf{A} \in \mathbb{K}^{m \times N}$ with $\ell_2$-normalized columns satisfies 
    \begin{equation}
        \mu_1(s) \geq s \sqrt{\dfrac{N-m}{m(N-1)}} \qquad whenever \qquad s < \sqrt{N-1}.
        \label{eq1.10}
    \end{equation}
    Equality holds if and only if the columns $a_1,\dots,a_N$ of the matrix $\mathbf{A}$ form an equiangular tight frame.
\end{theorem}

\textcolor[rgb]{1,0,0}{The proof is based on the following lemma}.

\begin{lemma}
    For $k<\sqrt{n}$, if the finite sequence $(\alpha_1,\alpha_2,\dots,\alpha_n)$ satisfies 
    \begin{equation*}
        \alpha_1 \geq \alpha_2 \geq \dots \geq \alpha_n \geq 0 \qquad and \qquad \alpha_1^2 + \alpha_2^2 + \dots + \alpha_n^2 \geq \dfrac{n}{k^2},
    \end{equation*}
    then 
    \begin{equation*}
        \alpha_1 + \alpha_2 + \dots + \alpha_k \geq 1,
    \end{equation*}
    with equality if and only if $\alpha_1 = \alpha_2 = \dots = \alpha_n = 1/k$.
    \label{lm1.9}
\end{lemma}
\begin{proof}
    The equivalent statement
    \begin{equation*}
        \left.
        \begin{array}{c}
            \alpha_1 \geq \alpha_2 \geq \dots \geq \alpha_n \geq 0\\
            \alpha_1 + \alpha_2 + \dots +\alpha_k \leq 1
        \end{array}
    \right\} \Longrightarrow \alpha^2_1 + \alpha_2^2 + \dots + \alpha_n^2 \leq \dfrac{n}{k^2},
    \end{equation*}
    with equality if and only if $\alpha_1 = \alpha_2 = \dots = \alpha_n = 1/k$. This is the problem of maximizing the convex function
    \begin{equation*}
        f(\alpha_1, \alpha_2, \dots, \alpha_n) := \alpha_1^2 + \alpha_2^2 + \dots + \alpha_n^2
    \end{equation*}
    over the convex polygon
    \begin{equation*}
        \mathcal{C} := \{(\alpha_1,\dots,\alpha_n) \in \mathbb{R}^n : \alpha_1 \geq \dots \geq \alpha_n \geq 0 \quad and \quad \alpha_1 + \dots + \alpha_k \leq 1\}. 
    \end{equation*}
    Because any point in $\mathcal{C}$ is a convex combination of its vertices (so that the extreme points of $\mathcal{C}$ are vertices) and because the function o$f$ is convex, the maximum is attained at a vertex of $\mathcal{C}$ by \cref{thb.16}. The vertices of $\mathcal{C}$ are obtained as intersections of $n$ hyperplanes arising by turning $n$ of the $(n+1)$ inequality constraints into equalities. Thus, we have the following possibilities:
\begin{enumerate}
    \item If $\alpha_1=\alpha_2=\dots=`a_n=0$, then $f(\alpha_1,\alpha_2,\dots,\alpha_n) = 0$.
    \item If $\alpha_1+\dots+\alpha_k = 1$ and $\alpha_1=\dots=\alpha_{\ell} > \alpha_{\ell+1}=\dots=\alpha_n$ for $1 \leq \ell \leq k$, then $\alpha_1 = \dots = \alpha_{\ell} = 1/\ell$, and consequently $f(\alpha_1,\alpha_2,\dots,\alpha_n)=1/\ell$.
    \item If $\alpha_1+\dots+\alpha_k = 1$ and $\alpha_1=\dots=\alpha_{\ell}>\alpha_{\ell+1}=\dots=\alpha_n=0$ for $k<\ell<n$, then $\alpha_1=\dots=\alpha_{\ell}=1/k$, and consequently $f(\alpha_1,\alpha_2,\dots,\alpha_n) = \ell/k^2$.
\end{enumerate}
Taking $k<\sqrt{n}$ into account, it follows that
\[
    \max\limits_{(\alpha_1,\dots,\alpha_n)\in\mathcal{C}} f(\alpha_1,\dots,\alpha_n) = \max\left\{ \max\limits_{1 \leq \ell \leq k}\dfrac{1}{\ell}, \max\limits_{k<\ell<n}\dfrac{\ell}{k^2} \right\} \max\left\{ 1, \dfrac{n}{k^2} \right\} = \dfrac{n}{k^2},
\]
with equality only in the case $\ell=n$ where $\alpha_1=\alpha_2=\dots=\alpha_n=1/k$.
    \begin{mdframed}
        \begin{theorem}
            \label{thb.16}
            Let $K \subset \mathbb{R}^N$ be a compact convex set and let $F := K \rightarrow \mathbb{R}$ be a convex function. Then $F$ attains its maximum at an extreme point of $K$.
        \end{theorem}
    \end{mdframed}
\end{proof}

The case of equality in \cref{lm1.9} implies that $\left|\left<a_{i^*}, a_j\right>\right| = \sqrt{(N-m)/(m(N-1))}$ for all $j \in [N], j\neq i^*$. Since the index $i^*$ can be arbitrarily chosen in $[N]$, the system $(a_1,\dots,a_N)$ is also equiangular. Conversely, the proof that equiangular tight frames yields equality in \cref{eq1.10} follows easily from \cref{th1.7} and \cref{eq1.2}. 

\begin{theorem}
    \label{th1.10}
    The cardinality N of an equiangular system $(a_1,\dots,a_N)$ of $\ell_2$-normalized vectors in $\mathbb{K}^m$ satisfies
    \begin{align*}
        N &\leq \dfrac{m(m+1)}{2}    &when\ \mathbb{K} = \mathbb{R},\\
        N &\leq m^2     &when\ \mathbb{K} = \mathbb{C}.
    \end{align*}
    If equality is achieved, then the system $(a_1,\dots,a_N)$ is also tight frame.
\end{theorem}

\begin{theorem}
    \label{1.12}
    For $m \geq 3$, if there is an equiangular system of $m(m+1)/2$ vectors in $\mathbb{R}^m$, then $m+2$ is necessarily the square of an odd integer.
\end{theorem}

\begin{proposition}
    \label{pr1.13}
    For each prime number $m \geq 5$, there is an explicit $m \times m^2$ complex matrix with coherence $\mu = 1/\sqrt{m}$.
\end{proposition}

\subsection{Analysis of Orthogonal Matching Pursuit}
\begin{theorem}
    \label{th1.14}
    Let $\mathbf{A} \in \mathbb{C}^{m \times N}$ be a matrix with $\ell_2$-normalized columns. If 
    \begin{equation}
        \mu_1(s) + \mu_1(s-1) < 1
        \label{eq1.11}
    \end{equation}
    then every s-sparse vector $x \in \mathbb{C}^N$ is exactly recoved from the measurement vector $y = \mathbf{A}x$ after at most s iterations of orthogonal matching pursuit.
\end{theorem}

\subsection{Analysis of Basis Pursuit}
As a matter of fact, any condition guaranteeing the success of the recovery of all vecotrs supported on a set $S$ via card$(S)$ iterations of orthogonal matching pursuit also guarantees the success of the recovery of all vectors supported on $S$ via basis pursuit. Indeed, given $\mathbf{v} \in ker\ \mathbf{A} \backslash \{0\}$, we have $\mathbf{A}_S\mathbf{v}_S = -\mathbf{A}_{\overline{S}} \mathbf{v}_{\overline{S}}$, and \cref{moore-penrose} 
\[
    \|\mathbf{v}\|_1 = \|\mathbf{A}_S^{\dagger} \mathbf{A}_S \mathbf{v}_S\|_1 = \|\mathbf{A}_S^{\dagger}\mathbf{A}_{\overline{S}}\mathbf{v}_{\overline{S}}\|_1 \leq \|\mathbf{A}_S^{\dagger} \mathbf{A}_{\overline{S}}\|_{1 \rightarrow 1}\|\mathbf{v}_{\overline{S}}\|_1 < \|\mathbf{v}_{\overline{S}}\|_1
\]

\begin{theorem}
    \label{th1.15}
    Let $\mathbf{A} \in \mathbb{C}^{m \times N}$ be a matrix with $\ell_2$-normalized columns. If 
    \begin{equation}
        \mu_1(s) + \mu_1(s-1) < 1,
        \label{eq1.13}
    \end{equation}
    then every s-sparse vector $x \in \mathbb{C}^N$ is exactly recovered from the measurement vecotr $y = \mathbf{A}x$ via basis pursuit.
\end{theorem}

\subsection{Analysis of Thresholding Algorithms}

\begin{theorem}
    \label{th1.16}
    Let $\mathbf{A} \in \mathbb{C}^{m \times N}$ be a matrix with $\ell_2$-normalized columns and let $x \in \mathbb{C}^N$ be a vector supported on a set $S$ of size s. If 
    \begin{equation}
        \mu_{1}(s) + \mu_1(s-1) < \dfrac{\min_{i\in S}|x_i|}{\max_{i \in S}|x_i|},
        \label{eq1.16}
    \end{equation}
    then the vecotr $x \in \mathbb{C}^N$ is exactly recovered from the measurement vecotr $y = \mathbf{A}x$ via basic thresholding.
\end{theorem}

\begin{theorem}
    \label{th1.17}
    Let $\mathbf{A} \in \mathbb{C}^{m \times N}$ be a matrix with $\ell_2$-normalized columns. If 
    \[
        2\mu_1(s) + \mu_1(s-1) < 1,
    \]
    then every s-sparse vector $x \in \mathbb{C}^N$ is exactly recoved from the measurement vecotr $y = \mathbf{A}x$ after at most s iterations of hard thresholding prusuit.
\end{theorem}
\begin{note}
    The conclusion of \cref{th1.17} can be achieved under the sufficient condition $\mu < 1/(3s-1)$. Similarly, the conclusion of \cref{th1.15} can be achieved under the sufficient condition $\mu < 1/(2s-1)$. 

    \cref{th1.14} and \cref{th1.15} in their present form were established by Tropp. What we call $\ell_1$-coherence function here is called \emph{\textcolor[rgb]{1,0,0}{cumulative coherence function}} there. This concept also appears under the name \emph{\textcolor[rgb]{1,0,0}{Babel function}}. A straightforward extension to any $p>0$ would be the $\ell_p$-coherence function of a matrix $\mathbf{A}$ with $\ell_2$-normalized columns $a_1,\dots,a_N$ defined by
    \[
        \mu_p(s) := \max\limits_{i \in [N]} \max \left\{ \left( \sum\limits_{j \in S}\left|\left<a_i, a_j\right>\right|^p \right)^{1/p}, S \subseteq [N], card(S) = s, i \notin S \right\}.
    \]
\end{note}

