\chapter{Math Basics}
\label{mathbase}
\section{Background in Linear Algebra}
\subsection{Types of Matrices}
\begin{enumerate}
    \item Symmetric matrices: $A^T = A$.
    \item Hermitian matrices: $A^H = A$.
    \item Skew-symmetric matrices: $A^T = -A$.
    \item Skew-Hermitian matrices: $A^H = -A$.
    \item Normal matrices: $A^H A = A A^H$.
    \item Nonnegative matrices: $a_{ij}\geq 0, i,j=1,\dots,n$ (similar definition for nonpositive, positive, and negative matrices).
    \item Unitary matrices: $Q^H Q = I$.
\end{enumerate}

\section{Moore-Penrose pseudoinverse}
\label{moore-penrose}
\subsection{Notation}
\begin{itemize}
    \item $K$ will denote one of the fields of real or complex numbers, denoted $\mathbb{R},\mathbb{C}$, respectively. The vector space of $m\times n$ matrices over $K$ is denoted by $M(m,n;K)$.
    \item For $A \in M(m,n;K)$, $A^T$ and $A^*$ denote the transpose and Hermitian transpose (also called conjugate transpose) respectively. If $K = \mathbb{R}$, then $A^* = A^T$.
    \item For $A \in M(m,n;K)$, then im$(A)$ denotes the range of $A$ (the space spanned by the column vectors of $A$) and ker$(A)$ \textcolor[rgb]{1,0,0}{denotes the kernel (null space)} of $A$.
    \item Finally, for any positive integer $n, I_n \in M(n,n;K)$ denotes the $n \times n$ identity matrix.
\end{itemize}

\subsection{Definition}

For $A \in M(m,n;K)$, a pseudoinverse of $A$ is defined as a matrix $A^{+} \in M(n,m;K)$ satisfying all of the following four criteria:
\begin{enumerate}
    \item $AA^+A = A$ ($AA^+$ need not be the general identity matrix, but it maps all column vectors of $A$ to themselves);
    \item $A^+AA^+ = A^+$ ($A^+$ is a weak inverse for the multiplicative semigroup);
    \item $(AA^+)^* = AA^+$ ($AA^+$ is Hermitian); and 
    \item $(A^+A)^* = A^+A$ ($A^+A$ is also Hermitian).
\end{enumerate}

Matrix $A^+$ exists for any matrix $A$, but when the latter has full rank, $A^+$ can be expressed as a \textcolor[rgb]{1,0,0}{simple algebra formula}.

In particular, when $A$ has \emph{full column rank} (and thus matrix $A^*A$ is invertible), $A^+$ can be computed as:
\begin{equation}
    A^+ = (A^*A)^{-1} A^*.
    \label{eq0.2.1}
\end{equation}
This particular pseudoinverse constitutes a \emph{\textcolor[rgb]{1,0,0}{left inverse}}, since, in this case, $A^+A = I$.

When $A$ has \emph{full row rank} (matrix $AA^*$ is invertible), $A^+$ can be computed as:
\begin{equation}
    A^+ = A^*(AA^*)^{-1}.
    \label{eq0.2.2}
\end{equation}
This is a \emph{\textcolor[rgb]{1,0,0}{right inverse}}, as $AA^+=I$.

\subsection{Properties}

\begin{itemize}
    \item If $A$ has real entries, then so does $A^+$.
    \item If $A$ is invertible, its pseudoinverse is its inverse. That is: $A^+ = A^{-1}$.
    \item The pseudoinverse of a zero matrix is its transpose.
    \item The pseudoinverse of the pseudoinverse is the original matrix: $(A^+)^+ = A$.
    \item Pseudoinverse commutes with transposition, conjugation, and taking the conjugate transpose:
        \begin{equation}
            (A^T)^+ = (A^+)^T, \overline{A}^+ = \overline{A^+}, (A^*)^+ = (A^+)^*.
            \label{eq0.2.3}
        \end{equation}
    \item The pseudoinverse of a scalar multiple of $A$ is the reciprocal multiple of $A^+$:
        \begin{equation}
            (\alpha A)^+ = \alpha^{-1} A^+ for \alpha \neq 0.
            \label{eq0.2.4}
        \end{equation}
\end{itemize}

\subsubsection{Identities}
\begin{eqnarray}
    A^{+} &=& A^{+} \quad A^{+*} \quad A^{*} \notag \\
    A^{+} &=& A^{*} \quad A^{+*} \quad A^{+} \notag\\
    A^{} &=& A^{+*} \quad A^{*} \quad A^{} \notag\\
    A^{} &=& A^{} \quad A^{*} \quad A^{+*} \notag\\
    A^{*} &=& A^{*} \quad A^{} \quad A^{+} \notag\\
    A^{*} &=& A^{+} \quad A^{} \quad A^{*} 
    \label{eq0.2.5}
\end{eqnarray}


\subsection{Reduction to Hermitian case}
The computation of the pseudoinverse is reducible to its construction in the Hermitian case. This is possible through the equivalences:
\begin{itemize}
    \item $A^+ = (A^*A)^+A^*$
    \item $A^+ = A^*(AA^*)^+$
\end{itemize}
as $A^*A$ and $AA^*$ are obviously Hermitian.

\subsection{Products}

If $A \in M(m,n;K)$, $B \in M(n,p;K)$ and either, 
\begin{itemize}
    \item $A$ has orthonormal columns (i.e. $A^*A = I_n$) or, 
    \item $B$ has orthonormal rows (i.e. $BB^* = I_n$) or,
    \item $A$ has all columns linearly independent (full column rank) and $B$ has all rows linearly independent (full row rank) or,
    \item $B = A^*$ (i.e. $B$ is the conjugate transpose of $A$),
\end{itemize}
then $(AB)^+ = B^+A^+$.

The last property yields the equivalences:

\begin{eqnarray}
    \begin{gathered}
        (AA^*)^+ = A^{+*}A^+  \notag\\
        (A^*A)^+ = A^+ A^{+*}
    \end{gathered}
    \label{eq0.2.6}
\end{eqnarray}

\section{Basic Algorithms}

The algorithms are divided into three categories: \emph{\textcolor[rgb]{1,0,0}{optimization methods, greedy methods, and thresholding-based methods}}. 

\subsection{Optimization Methods}

An \emph{\textcolor[rgb]{1,0,0}{optimization problem}} is a problem of the type

\[
    \mathop{\text{minimize}}\limits_{x \in \mathbb{R}^N} F_0(x) \quad \text{ subject to } F_i(x) \leq b_i,\ i \in [n],
\]
where the function $F_0 : \mathbb{R}^N \rightarrow \mathbb{R}$ is called an \emph{\textcolor[rgb]{1,0,0}{objective function}} and the functions $F_1,\dots,F_n : \mathbb{R}^N \rightarrow \mathbb{R}$ are called \emph{\textcolor[rgb]{1,0,0}{constrained functions}}. This general framework also encompasses equality constraints of the type $G_i(x) = c_i$. Since the equality $G_i(x) = c_i$ is equivalent to the inequalities $G_i(x) \leq c_i$ and $-G_i(x) \leq -c_i$. If $F_0, F_1, \dots, F_n$ are all convex functions, then the problem is called a \emph{\textcolor[rgb]{1,0,0}{convex optimization problem}}. If $F_0, F_1, \dots, F_n$ are all linear functions, then the problem is called a \emph{\textcolor[rgb]{1,0,0}{linear program}}. The sparse recovery problem is in fact an optimization problem, since it translates into
\begin{equation}
    \mathop{\mathrm{minimize}}\|z\|_0 \quad \text{subject to } \mathbf{A}z = y. \tag{$P_0$}
    \label{eq0.p0}
\end{equation}
This is a nonconvex problem and NP-hard in general. However, keeping in mind that $\|z\|_q^q$ approaches $\|z\|_0$ as $q>0$ tends to zero, we can approximate \cref{eq0.p0} by the problem
\begin{equation}
    \mathop{\mathrm{minimize}}\|z\|_q \quad \text{subject to } \mathbf{A}z = y. \tag{$P_q$}
    \label{eq0.pq}
\end{equation}

and \emph{\textcolor[rgb]{1,0,0}{basis pursuit or $\ell_1$-minimization}}:
\begin{equation}
    \mathop{\mathrm{minimize}} \|z\|_1 \quad \text{subject to } \mathbf{A}z = y. \tag{$P_1$}
    \label{eq0.p1}
\end{equation}

\begin{mdframed}
    \label{bp}
    \begin{center}
        \textbf{\textcolor[rgb]{1,0,0}{Basis pursuit}}
    \end{center}
    \emph{Input:} measurement matrix $\mathbf{A}$, measurement vector $y$.

    \emph{Instruction:} 
    \begin{equation}
        x^{\sharp} = \mathop{\mathrm{argmin}} \|z\|_1 \quad \text{subject to } \mathbf{A}z = y. \tag{BP}
        \label{eqbp}
    \end{equation}

    \emph{Output:} the vector $x^{\sharp}$.
\end{mdframed}

\begin{theorem}
    \label{th0.3.1}
    Let $\mathbf{A} \in \mathbb{R}^{m \times N}$ be a measurement matrix with columns $a_1, \dots, a_N$. Assuming the uniqueness of a minimizer $x^{\sharp}$ of 
    \[
        \mathop{\mathrm{minimize}}\limits_{z \in \mathbb{R}^N} \|z\|_1 \quad \text{subject to } \mathbf{A}z = y,
    \]
    the system $\{a_j, j\in \mathop{\mathrm{supp}}(x^{\sharp})\}$ is linearly independent, and in particular 
    \[
        \|x^{\sharp}\|_0 = \mathop{\mathrm{card}}(\mathop{\mathrm{supp}}(x^{\sharp})) \leq m.
    \]
\end{theorem}

A general $\ell_1$-minimization taking measurement error into account:
\begin{equation}
    \mathop{\mathrm{minimize}}\|z\|_1 \quad \text{subject to } \|\mathbf{A}z-y\|_2 \leq \eta \tag{$P_{1,\eta}$}
    \label{eq0.p1n}
\end{equation}
Given a vector $z \in \mathbb{C}^N$, we introduce its real and imaginary parts $\mathbf{u,v} \in \mathbb{R}^N$ and a vector $\mathbf{c} \in \mathbb{R}^N$ such that $c_j \geq \left|z_j\right| = \sqrt{u_j^2 + v_j^2}$ for all $j \in [N]$. The problem \cref{eq0.p1n} is then equivalent to the following problem with optimization variables $\mathbf{c,u,v} \in \mathbb{R}^N$:
\begin{equation}
    \mathop{\mathrm{minimize}}\limits_{\mathbf{c,u,v} \in \mathbb{R}^N} \sum\limits_{j=1}^N c_j \quad \text{subject to } \left\|\left[ 
        \begin{array}{cc}
            Re(\mathbf{A}) & -Im(\mathbf{A}) \\
            Im(\mathbf{A}) & Re(\mathbf{A})
        \end{array}
        \right]
        \left[ 
         \begin{array}{c}
            \mathbf{u}\\
            \mathbf{v}
        \end{array}
    \right]\right\|_2 \leq \eta, \tag{$P_{1,\eta}'$}
    \label{eq0.p1n'}
\end{equation}
\[
    \begin{array}{c}
        \sqrt{u_1^2 + v_1^2} \leq c_1, \\
        \vdots \\
        \sqrt{u_N^2 + v_N^2} \leq c_N.
    \end{array}
\]
This is an instance of a \emph{\textcolor[rgb]{1,0,0}{second-order cone problem}};

\begin{mdframed}
    \label{qbp}
    \begin{center}
        \textbf{\textcolor[rgb]{1,0,0}{Quadratically constrained basis pursuit}}
    \end{center}
    \emph{Input:} measurement matrix $\mathbf{A}$, measurement vector $y$, noise level $\eta$.

    \emph{Instruction:}
    \begin{equation}
        x^{\sharp} = \mathop{\mathrm{argmin}} \|z\|_1 \qquad \text{subject to } \|\mathbf{A}z - y\|_2 \leq \eta. \tag{$BP_{\eta}$}
        \label{eqbpn}
    \end{equation}

    \emph{Output:} the vector  $x^{\sharp}$.
\end{mdframed}
    The solution $x^{\sharp}$ of 
    \begin{equation}
        \mathop{\mathrm{minimize}}\limits_{z \in \mathbb{C}^{N}}\|z\|_1 \qquad \text{subject to }\|\mathbf{A}z - y\|_2 \leq \eta
        \label{eq0.3.1}
    \end{equation}
    for some parameter $\lambda \geq 0$,
    \begin{equation}
        \mathop{\mathrm{minimize}}\limits_{z \in \mathbb{C}^N} \lambda\|z\|_1 + \|\mathbf{A}z - y\|_2^2.
        \label{eq0.3.2}
    \end{equation}
    The solution of \cref{eq0.3.1} is also related to the output of the \emph{LASSO}, which consists in solving, for some parameter $\tau \geq 0$,
    \begin{equation}
        \mathop{\mathrm{minimize}}\limits_{z \in \mathbb{C}^N} \|\mathbf{A}z - y\|_2 \qquad \text{subject to }\|z\|_1 \leq \tau.
        \label{eq0.3.3}
    \end{equation}
    Precisely, some links between the three approaches are given below.


\begin{proposition}
    \label{pr0.3.2}
    \begin{enumerate}[(a)]
        \item If $x$ is a minimizer of the basis pursuit denoising \cref{eq0.3.2} with $\lambda > 0$, then there exists $\eta = \eta_x \geq 0$ such that $x$ is a minimizer of the quadratically constrained basis pursuit \cref{eq0.3.1}.
        \item If $x$ is unique minimizer of the quadratically constrained basis pursuit \cref{eq0.3.1} with $\eta \geq 0$, then there exists $\tau = \tau_x \geq 0$ such that $x$ is a unique minimizer of the LASSO \cref{eq0.3.3}.
        \item If $x$ is a minimizer of the LASSO \cref{eq0.3.3} with $\tau \geq 0$, then there exists $\lambda = \lambda_x \geq 0$ such that $x$ is a minimizer of the basis pursuit denoising \cref{eq0.3.2}.
    \end{enumerate}
\end{proposition}

Another type of $\ell_1$-minimization problem is the \emph{\textcolor[rgb]{1,0,0}{Dantzig selector}},
\begin{equation}
    \mathop{\mathrm{minimize}}\limits_{z \in \mathbb{C}^N} \|z\|_1 \qquad \text{subject to } \|\mathbf{A}^*(\mathbf{A}z - y)\|_{\infty} \leq \tau.
    \label{eq0.3.4}
\end{equation}
This is again a convex optimization problem. The intuition for the constraints is that the residual $r = \mathbf{A}z -y $ should have small correlation with all columns $a_j$ of the matrix $\mathbf{A}$---indeed, $\|\mathbf{A}^*(\mathbf{A}z -y)\|_{\infty} = \max_{j \in [N]} \left|\left<r, a_j\right>\right|$.  

\subsection{Greedy Methods}

\begin{mdframed}
    \label{OMP}
    \begin{center}
        \textbf{Orthogonal matching pursuit (OMP)}
    \end{center}
    \emph{Input:} measurement matrix $\mathbf{A}$, measurement vector $y$.

    \emph{Initialization: } $S^0 = \cancel{0}, x^0 = 0$. 
    
    \emph{Iteration:} repeat until a stopping criterion is met at $n = \bar{n}$:
    \begin{equation}
        S^{n+1} = S^n \cup \{j_{n+1}\}, j_{n+1} := \mathop{\mathrm{argmax}}\limits_{j \in [N]} \left\{ \left|(\mathbf{A}^*(y-\mathbf{A}x^n))_j\right| \right\}, \tag{$OMP_1$}
        \label{eqOMP1}
    \end{equation}
    \begin{equation}
        x^{n+1} = \mathop{\mathrm{argmin}}\limits_{z \in \mathbb{C}^N} \left\{ \|y - \mathbf{A}z\|_2,\mathop{\mathrm{supp}}(z) \subset S^{n+1} \right\}. \tag{$OMP_2$}
        \label{OMP2}
    \end{equation}

    \emph{Output:} the $\bar{n}$-sparse vector $x^{\sharp} = x^{\bar{n}}$.
\end{mdframed}

The projection step \cref{OMP2} is the most costly part of the orthogonal matching pursuit algorithm, which can be accelerated by using the $QR$-decomposition of $\mathbf{A}_{S_n}$. 

The choice of the index $j_{n+1}$ is dictated by a greedy strategy where one aims to reduce the $\ell_2$-norm of the residual $y - \mathbf{A}x^n$ as much as possible at each iteration. 

\begin{lemma}
    \label{lm0.3.3}
    Let $\mathbf{A} \in \mathbb{C}^{m \times N}$ be a matrix with $\ell_2$-normalized columns. Given $S \subset [N]$, $v$ supported on $S$, and $j \in [N]$, if 
    \[
        w := \mathop{\mathrm{argmin}}\limits_{z \in \mathbb{C}^N} \left\{ \|y-\mathbf{A}z\|_2. \mathop{\mathrm{supp}}(z) \subset S \cup \{j\} \right\},
    \]
    then
    \[
        \|y - \mathbf{A}w\|_2^2 \leq \|y-\mathbf{A}v\|_2^2- \left|(\mathbf{A}^*(y - \mathbf{A}v))_j\right|^2.
    \]
\end{lemma}

\begin{proof}
    Since any vector of the form $v + te_j$ with $t \in \mathbb{C}$ is supported on $S \cup \{j\}$, we have
    \[
        \|y - \mathbf{A}w\|_2^2 \leq \min\limits_{t \in \mathbb{C}} \|y - \mathbf{A}(v+te_j)\|_2^2.
    \]
    Writing $t = \rho e^{i\theta}$ with $\rho \geq 0$ and $\theta \in [0,2\pi)$, we compute
        \begin{eqnarray*}
            \|y - \mathbf{A}(v+te_j)\|_2^2 &=& \|y- \mathbf{A}v - t\mathbf{A}e_j\|_2^2 \\
            &=& \|y - \mathbf{A}v\|_2^2 + \left|t\right|^2 \|\mathbf{A}e_j\|_2^2 - 2 \mathop{\mathrm{Re}}\left( \bar{t}\left<y - \mathbf{A}v, \mathbf{A}e_j\right> \right) \\
            &=& \|y - \mathbf{A}v\|_2^2 + \rho^2 - 2\mathop{\mathrm{Re}}\left( \rho e^{-i\theta}(\mathbf{A}^*(y-\mathbf{A}v))_j \right) \\
            &\geq& \|y - \mathbf{A}v\|_2^2 + \rho^2 - 2\rho\left|(\mathbf{A}^*(y-\mathbf{A}v))_j\right|,
        \end{eqnarray*}
        with equality for a property chosen $\theta$. As a quadratic polynomial in $\rho$, the latter expression is minimized when $\rho = \left|(\mathbf{A}^* (y - \mathbf{A}u))_j\right|$. This shows that
        \[
            \min\limits_{t \in \mathbb{C}} \|y-\mathbf{A}(v + t e_j)\|_2^2 = \| y - \mathbf{A} v\|_2^2 - \left|(\mathbf{A}^*(y - \mathbf{A}u))_j\right|^2,
        \]
        which concludes the proof.
\end{proof}

The step \cref{OMP2} also reads as 
\[
    x_{S^{n+1}}^{n+1} = \mathbf{A}_{S^{n+1}}^{\dagger}y,
\]
where $x_{S^{n+1}}^{n+1}$ denotes the restriction of $x^{n+1}$ to its support set $S^{n+1}$ and where $\mathbf{A}_{S^{n+1}}^{\dagger}$ is the pseudo-inverse of $\mathbf{A}_{S^{n+1}}$. This simply says that $z = x_{S^{n+1}}^{n+1}$ is a solution of $\mathbf{A}_{S^{n+1}}^{n+1}\mathbf{A}_{S^{n+1}}z = \mathbf{A}_{S^{n+1}}^*y$. 

\begin{lemma}
    \label{lm0.3.1}
    Given an index set $S \subset [N]$, if 
    \[
        v := \mathop{\mathrm{argmin}}\limits_{z \in \mathbb{C}^N} \left\{ \|y - \mathbf{A}z\|_2, \mathop{\mathrm{supp}}(z) \subset S \right\},
    \]
    then 
    \begin{equation}
        (\mathbf{A}^*(y - \mathbf{A}v))_S = 0.
        \label{eq0.3.5}
    \end{equation}
\end{lemma}

\begin{proposition}
    \label{pr0.3.5}
    Given a matrix $\mathbf{A} \in \mathbb{C}^{m \times N}$, every nonzero vector $x \in \mathbb{C}^N$ supported on a set $S$ of size $s$ is recovered from $y = \mathbf{A}x$ after at most $s$ iterations of orthogonal matching pursuit if and only if the matrix $\mathbf{A}_S$ is injective and 
    \begin{equation}
        \max\limits_{j \in S}\left|(\mathbf{A}^*r)_j\right| > \max\limits_{\ell \in \overline{S}} \left|(\mathbf{A}^*r)_{\ell}\right|
        \label{eq0.3.6}
    \end{equation}
    for all nonzero $r \in \left\{ \mathbf{A}z, \mathop{\mathrm{supp}}(z) \subset S \right\}$.
\end{proposition}

\begin{remark}
    \label{rmk0.3.6}
    A more concise way to formulate the necessary and sufficient conditions of \cref{pr0.3.5} is the \textcolor[rgb]{1,0,0}{exact recovery condition}, which reads
    \begin{equation}
        \|\mathbf{A}_S^{\dagger}\mathbf{A}_{\overline{S}}\|_{1 \rightarrow 1} < 1;
        \label{eq0.3.7}
    \end{equation}
    see A.1 for the definition of matrix norms. Implicitly, the existence of the pseudo-inverse $\mathbf{A}_S^{\dagger} = (\mathbf{A}_S^*\mathbf{A}_S)^{-1} \mathbf{A}_S^*$ is equivalent to the injectivity of $\mathbf{A}_S$. 
\end{remark}

\begin{mdframed}
    \label{CoSaMP}
    \begin{center}
        \textbf{\textcolor[rgb]{1,0,0}{Compressive sampling matching pursuit (CoSaMP)}}
    \end{center}
        \emph{Input:} measurement matrix $\mathbf{A}$, measurement vector $y$, sparsity level $s$.

        \emph{Initialization:} $s$-sparse vector $x^0$, typically $x^0 = 0$.

        \emph{Iteration:} repeat until a stopping criterion is met at $n = \bar{n}$:
        \begin{equation}
            U^{n+1} = \mathop{\mathrm{supp}}(x^n) \cup L_{2s}(\mathbf{A}^*(y - \mathbf{A}x^n)), \tag{CoSaMP$_1$} 
            \label{eqcosamp1}
        \end{equation}
        \begin{equation}
            u^{n+1} = \mathop{\mathrm{argmin}}\limits_{z \in \mathbb{C}^N} \left\{ \|y - \mathbf{A}z\|_{2}, \mathop{\mathrm{supp}}(z) \subset U^{n+1} \right\}, \tag{CoSaMP$_2$}
            \label{eqcosamp2}
        \end{equation}
        \begin{equation}
            x^{n+1} = H_s(u^{n+1}). \tag{CoSaMP$_3$}
            \label{eqcosamp3}
        \end{equation}
        
        \emph{Output:} the $s$-sparse vector $x^{\sharp} = x^{\bar{n}}$.
\end{mdframed}

\subsection{Thresholding-Based Methods}

\begin{mdframed}
    \label{basicthresholding}
\begin{center}
    \textbf{\textcolor[rgb]{1,0,0}{Basic thresholding}}
\end{center}
    \emph{Input:} measurement matrix $\mathbf{A}$, measurement vector $y$, sparsity level $s$.

    \emph{Instruction:} 
    \begin{equation}
        S^{\sharp} = L_s(\mathbf{A}^*y), \tag{BT$_1$}
        \label{eqbt1}
    \end{equation}
    \begin{equation}
        x^{\sharp} = \mathop{\mathrm{argmin}}\limits_{z \in \mathbb{C}^N} \left\{ \|y = \mathbf{A}z\|_2, \mathop{\mathrm{supp}}(z) \subset S^{\sharp} \right\}. \tag{BT$_2$}
        \label{eqbt2}
    \end{equation}

    \emph{Output:} the $s$-sparse vector $x^{\sharp}$.
\end{mdframed}

\begin{proposition}
    \label{pr0.3.7}
    A vector $x \in \mathbb{C}^{N}$ supported on a set $S$ is recovered from $y = \mathbf{A}x$ via basic thresholding if and only if 
    \begin{equation}
        \min\limits_{j \in S}\left|(\mathbf{A}^* y)_j\right| > \max\limits_{\ell \in \overline{S}} \left|(\mathbf{A}^* y )_{\ell}\right|.
        \label{eq0.3.10}
    \end{equation}
\end{proposition}
\begin{proof}
    It is clear that the vector $x$ is recovered if and only if the index set $S^{\sharp}$ defined in \cref{eqbt1} coincides with the set $S$, that is to say, if and only if any entry of $\mathbf{A}^*y$ on $S$ is greater than any entry of $\mathbf{A}^*y$ on $\overline{S}$. This is property \cref{eq0.3.10}.
\end{proof}

The more elaborate \emph{\textcolor[rgb]{1,0,0}{iterative hard thresholding algorithm}} is an iterative algorithm to solve the rectangular system $\mathbf{A}z = y$, knowing that the solution is $s$-sparse. We shall solve the square system $\mathbf{A}^*\mathbf{A}z = \mathbf{A}^*y$ instead, which can be interpreted as the fixed-point equation $z = (\mathbf{Id} - \mathbf{A}^*\mathbf{A})z + \mathbf{A}^*y$. Classical iterative methods suggest the \emph{\textcolor[rgb]{1,0,0}{fixed-point iteration}} $x^{n+1} = (\mathbf{Id} - \mathbf{A}^*\mathbf{A})x^n + \mathbf{A}^*y$. Since we target $s$-sparse vectors, we only keep the \emph{\textcolor[rgb]{1,0,0}{$s$ largest absolute entries of $(\mathbf{Id} - \mathbf{A}^*\mathbf{A})x^n + \mathbf{A}^*y = x^n + \mathbf{A}^*(y - \mathbf{A}x^n)$ at each iteration}}. The resulting algorithm:

\begin{mdframed}
    \label{IHT}
    \begin{center}
        \textbf{\textcolor[rgb]{1,0,0}{Iterative hard thresholding (HIT)}}
    \end{center}
    \emph{Input:} measurement matrix $\mathbf{A}$, measurement vector $y$, sparsity level $s$.

    \emph{Initialization:} $s$-sparse vector $x^0$, typically $x^0 = 0$.

    \emph{Iteration:} repeat until a stopping criterion is met at $n = \bar{n}$:
    \begin{equation}
        x^{n+1} = H_s(x^n + \mathbf{A}^*(y - \mathbf{A}x^n)). \tag{IHT}
        \label{eqiht}
    \end{equation}
    
    \emph{Output:} the $s$-sparse vector $x^{\sharp} = x ^{\bar{n}}$.
\end{mdframed}

The above algorithm does not require computation of any orthogonal projection. If the orthogonal projection needs to be paid attention, like in the greedy methods, \emph{\textcolor[rgb]{1,0,0}{it makes sense to look at the vector with the same support as $x^{n+1}$ that best fits the measurements}}. This leads to the \emph{\textcolor[rgb]{1,0,0}{hard thresholding pursuit algorithm}}:
\begin{mdframed}
    \label{HTP}
    \begin{center}
        \textbf{\textcolor[rgb]{1,0,0}{Hard thresholding pursuit (HTP)}}
    \end{center}
    \emph{Input:} measurement matrix $\mathbf{A}$, measurement vector $y$, sparsity level $s$.

    \emph{Initialization:} $s$-sparse vector $x^0$, typically $x^0 = 0$.

    \emph{Iteration:} repeat until stopping criterion is met an $n = \bar{n}$:
    \begin{equation}
        S^{n+1} = L_s(x^n + \mathbf{A}^*(y = \mathbf{A}x^n)), \tag{HTP$_1$}
        \label{eqhtp1}
    \end{equation}
    \begin{equation}
        x^{n+1} = \mathop{\mathrm{argmin}}\limits_{z \in \mathbb{C}^N} \left\{ \|y - \mathbf{A}z\|_2, \mathop{\mathrm{supp}}(z) \subset S^{n+1} \right\}. \tag{HTP$_2$}
        \label{eqhtp2}
    \end{equation}

    \emph{Output:} the $s$-sparse vector $x^{\sharp} = x^{\bar{n}}$.
\end{mdframed}

\begin{mdframed}
    \label{subspacepursuit}
    \begin{center}
        \textbf{\textcolor[rgb]{1,0,0}{Subspace Pursuit}}
    \end{center}

    \emph{Input:} measurement matrix $\mathbf{A}$, measurement vector $y$, sparsity level $s$.

    \emph{Initialization:} $s$-sparse vector $x^0$, typically $x^0 = 0, S^0 = \mathop{\mathrm{supp}}(x^0)$.

    \emph{Iteration:} repeat until a stopping criterion is met at $n = \bar{n}$:
    \begin{equation}
        U^{n+1} = S^n \cup L_s(\mathbf{A}^*(y-\mathbf{A}x^n)), \tag{SP$_1$}
        \label{eqsp1}
    \end{equation}
    \begin{equation}
        u^{n+1} = \mathop{\mathrm{argmin}}\limits_{z \in \mathbb{C}^N} \left\{ \|y-\mathbf{A}z\|_2, \mathop{\mathrm{supp}}(z) \subset U^{n+1} \right\}, \tag{SP$_2$}
        \label{eqsp2}
    \end{equation}
    \begin{equation}
        S^{n+1} = L_{s} (u^{n+1}), \tag{SP$_3$}
        \label{eqsp3}
    \end{equation}
    \begin{equation}
        x^{n+1} = \mathop{\mathrm{argmin}}\limits_{z \in \mathbb{C}^N} \left\{ \|y - \mathbf{A}z\|_2, \mathop{\mathrm{supp}}(z) \subset S^{n+1} \right\}. \tag{SP$_4$}.
        \label{eqsp4}
    \end{equation}

    \emph{Output:} the $s$-sparse vector $x^{\sharp} = x^{\bar{n}}$.
\end{mdframed}

\subsection{Notes}

\subsubsection{Which Algorithm Should One Choose?}
\begin{itemize}
    \item The minimal number $m$ of measurements for a sparsity $s$ and a signal length $N$ may vary with each algorithm. Comparing the recovery rates and identifying the best algorithm is a matter of numerical tests. 
        For this criterion, the recovery performance of \emph{basic thresholding} is significantly worse than the one of other algorithms, although it is the fastest algorithm since it identifies the support in only one step.
    \item The speed of the algorithm is also a criterion, and it is also a matter of numerical tests. If the sparsity $s$ is quite small, then \emph{orthogonal matching pursuit} is extremely fast because the speed essentially depends on \emph{the number of iterations, which typically equals $s$ when the algorithm succeeds}. \emph{Compressive sampling matching pursuit and hard thresholding pursuit} are fast for small $s$, because each step involves the computation of an orthogonal projection relative to $\mathbf{A}_S$ with small $S \subset [N]$. But if the sparsity $s$ is not that small compared to $N$, then orthogonal matching pursuit may nonetheless require a significant time. The same applies to the homotopy method which builds the support of an $\ell_1$-minimizer iteratively. The runtime of iterative hard thresholding is almost not influenced by the sparsity $s$ at all.

        Basis pursuit \cref{bp}, per se, is not an algorithm, so the runtime depends on the actual algorithm that is used for the minimization. For \emph{\textcolor[rgb]{1,0,0}{Chambolle and Pock's primal dual algorithm}}, which constructs a sequence converging to an $\ell_1$-minimizer, the sparsity $s$ has no serious influence on the speed. Hence, for \textcolor[rgb]{1,0,0}{mildly large $s$}, it can be significantly faster than orthogonal matching pursuit. A point of view of regarding the orthogonal matching pursuit are always faster than $\ell_1$-minimization --- is only true for small sparsity. \textcolor[rgb]{1,0,0}{The iteratively reweighted least squares method} may also be a good alternative for mildly large sparsity.

    \item Another Important feature of an algorithm is the \emph{\textcolor[rgb]{1,0,0}{possibility to exploit fast matrix-vector multiplication routines}} that are available for $\mathbf{A}$ and $\mathbf{A}^*$. In principle, any of the proposed methods can be sped up in this case, but the task is complicated if \emph{orthogonal projection} steps are involved. Fast matrix-vector multiplications are easily integrated in the \emph{iterative hard thresholding algorithm and in Chambolle and Pock's primal dual algorithm for $\ell_1$-minimization}. The acceleration achieved in this context depends on the algorithm, and the fastest algorithm should again be determined by numerical tests in the precise situation.
\end{itemize}

\section{Basis Pursuit}

\subsection{Null Space Property}

The null space property is a necessary and sufficient condition for exact recovery of sparse vectors via basis pursuit. 

\begin{definition}
    \label{def0.4.1}
    A matrix $\mathbf{A} \in \mathbb{K}^{m \times N}$ is said to satisfy the \textcolor[rgb]{1,0,0}{null space property} relative to a set $S \subset [N]$ if 
    \begin{equation}
        \|v_S\|_1 < \|v_{\bar{S}}\|_1 \quad \text{for all } v \in \mathop{\mathrm{ker}}\mathbf{A} \backslash \{0\}.
        \label{eq0.4.1}
    \end{equation}
\end{definition}

It is to satisfy the null space property of order $s$ if it satisfies the null space property relative to any set $S \subset [N]$ with card$(S) \leq s$.

\begin{remark}
    \label{rmk0.4.2}
    It is important to observe that, for a given $v \in \mathop{\mathrm{ker}}\mathbf{A} \backslash \{0\}$, the condition $\|v_S\|_1 < \|v_{\bar{S}}\|_1$ holds for any set $S \subset [N]$ with card$(S) \leq s$ as soon as it holds for an index set of $s$ largest (in modulus) entries of $v$.
\end{remark}

\begin{mdframed}
    \begin{definition}
        \label{def2.2ofbook}
        For $p>0$, the $\ell_p$-error of best $s$-term approximation to a vector $x \in \mathbb{C}^{N}$ is defined by
        \[
            \sigma_s(x)_p := \inf \left\{ \|x-z\|_p, z \in \mathbb{C}^N \text{ is $s$-sparse} \right\}.
        \]
    \end{definition}
\end{mdframed}


\begin{remark}
    \label{rmk0.4.3}
    There are two convenient reformulations of the null space property. The first one is obtained by adding $\|v_S\|_1$ to both sides of the inequality $\|v_S\|_1 < \|v_{\bar{S}}\|_1$. Thus, the null space property relative to $S$ reads 
    \begin{equation}
        2\|v_S\|_1 < \|v\|_1 \quad \text{for all } v \in \mathop{\mathrm{ker}} \mathbf{A} \backslash \{0\}.
        \label{eq0.4.2}
    \end{equation}
    The second one is obtained by choosing $S$ as an index set of $s$ largest (in modulus) entries of $v$ and this time by adding $\|v_{\bar{S}}\|_1$ to both sides of the inequality. Thus, the null space property of order $s$ reads
    \begin{equation}
        \|v\|_1 < 2 \sigma_s(v)_1 \quad \text{for all } v \in \mathop{\mathrm{ker}}\mathbf{A} \backslash \{0\},
        \label{eq0.4.3}
    \end{equation}
    where we recall from \cref{def2.2ofbook} that, for $p>0$, the $\ell_p$-error of \textcolor[rgb]{1,0,0}{\emph{best $s$-term approximation}} to $x \in \mathbb{K}^N$ is defined by 
    \[
        \sigma_s(x)_p = \inf\limits_{\|z\|_0 \leq s} \|x-z\|_p.
    \]
\end{remark}

\begin{theorem}
    \label{th0.4.4}
    Given a matrix $\mathbf{A} \in \mathbb{K}^{m \times N}$, every vector $x \in \mathbb{K}^N$ supported on a set $S$ is the unique solution of \cref{eq0.p1} with $y = \mathbf{A}x$ if and only if $\mathbf{A}$ satisfies the null space property relative to $S$.
\end{theorem}

\begin{remark}
    \label{rmk0.4.6}
    \begin{enumerate}[(a)]
        \item This theorem \cref{th4.5frombook} shows that for every $y = \mathbf{A}x$ with $s$-sparse vector $x$ the $\ell_1$-minimization strategy \cref{eq0.p1} actually solves the $\ell_0$-minimization problem \cref{eq0.p0} when the null space property of order $s$ holds. Indeed, assume that every $s$-sparse vector $x$ is recovered via $\ell_1$-minimization from $y = \mathbf{A}x$. Let $z$ be the minimizer of the $\ell_0$-minimization problem \cref{eq0.p0} with $y = \mathbf{A}x$ then $\|z\|_0 \leq \|x\|_0$ so that also $z$ is $s$-sparse. But since every $s$-sparse vector is the unique $\ell_1$-minimizer, it follows that $x=z$.
        \item It is desirable for any reconstruction scheme to preserve sparse recovery if some measurements are rescaled, reshuffled, or added. Basis pursuit actually features such properties. Indeed, mathematically speaking, these operations consist in replacing the original measurement matrix $\mathbf{A}$ by new measurement matrices $\hat{\mathbf{A}}$ and $\hat{\mathbf{A}}$ defined by 
            \[
                \hat{\mathbf{A}} := \mathbf{GA}, \quad \text{where $\mathbf{G}$ is some invertible $m\times m$ matrix},
            \]
            \[
                \hat{\mathbf{A}} := \left[ \frac{\mathbf{A}}{\mathbf{B}} \right], \quad \text{where $\mathbf{B}$ is some $m' \times N$ matrix}.
            \]
            One can observe that ker$\hat{\mathbf{A}} = $ ker$\mathbf{A}$ and ker$\hat{\mathbf{A}} \subset $ ker$\mathbf{A}$, hence the null space property for the matrices $\hat{\mathbf{A}}$ and $\hat{\mathbf{A}}$ remains fulfilled if it is satisfied for the matrix $\mathbf{A}$. It is not true that the null space property remains valid if we multiply on the right by an invertible matrix.
    \end{enumerate}
\end{remark}

The \emph{\textcolor[rgb]{1,0,0}{real null space relative to a set $S$}}:
\begin{equation}
    \sum\limits_{j \in S} \left|v_j\right| < \sum\limits_{\ell \in \bar{S}} \left|v_{\ell}\right| \quad \text{for all } v \in \text{ker}_{\mathbb{R}} \mathbf{A}, v \neq 0,
    \label{eq0.4.4}
\end{equation}
and, on the other hand, to the \emph{\textcolor[rgb]{1,0,0}{complex null space property relative to $S$}}:
\begin{equation}
    \sum\limits_{j \in S} \sqrt{v_j^2 + w_j^2} < \sum\limits_{\ell \in \bar{S}} \sqrt{v_{\ell}^2 + w_{\ell}^2} \quad \text{for all } v,w \in \text{ker}_{\mathbb{R}} \mathbf{A},(v,w) \neq (0,0).
    \label{eq0.4.5}
\end{equation}
The real and complex version are in fact equivalent. These vectors can be interpreted as real or as complex vectors. The followed theorem explains why we usually work in the complex setting.
\begin{theorem}
    \label{th0.4.7}
    Given a matrix $\mathbf{A} \in \mathbb{R}^{m\times N}$, the real null space property \cref{eq0.4.4} relative to a set $S$ is equivalent to the complex null space property \cref{eq0.4.5} relative to this set $S$.

    In particular, the real null space property of order $s$ is equivalent to the complex null space property of order $s$.
\end{theorem}

\begin{proof}
    \cref{eq0.4.4} immediately follows from \cref{eq0.4.5} by setting $w = 0$. So one can assume that \cref{eq0.4.4} holds. We consider $v,w \in \text{ker}_{\mathbb{R}\mathbf{A}}$ with $(v,w) \neq (0,0)$. If $v$ and $w$ are linearly dependent, then the inequality $\sum_{j \in S} \sqrt{v_j^2} _+ w_j^2 < \sum_{\ell \in \bar{S} \sqrt{v_{\ell}^2 + w_{\ell}^2}}$ is clear, sow we suppose that they are linearly independent. Then $u := \cos\theta v + \sin \theta w \in \text{ker}_{\mathbb{R}}\mathbf{A}$ is nonzero, and \cref{eq0.4.4} yields, for any $\theta \in \mathbb{R}$, 
    \begin{equation}
        \sum\limits_{j \in S} \left|\cos \theta v_j + \sin \theta w_j\right| < \sum\limits_{\ell \in \bar{S}} \left|\cos \theta v_{\ell} + \sin \theta w_{\ell}\right|.
        \label{eq0.4.6}
    \end{equation}
    For each $k \in [N]$, we define $\theta_k \in [-\pi, \pi]$ by the equalities
    \[
        v_k = \sqrt{v_k^2 + w_k^2} \cos \theta_k, \qquad w_k = \sqrt{v_k^2 + w_k^2} \sin \theta_k,
    \]
    so that \cref{eq0.4.6} reads
    \[
        \sum\limits_{j \in S} \sqrt{v_j^2 + w_j^2}\left|\cos (\theta - \theta_j)\right| < \sum\limits_{\ell \in \bar{S}} \sqrt{v_{\ell}^2 + w_{\ell}^2} \left|\cos(\theta - \theta_{\ell})\right|.
    \]
    We now integrate over $\theta \in [-\pi, \pi]$ to obtain
    \[
        \sum\limits_{j \in S} \sqrt{v_j^2 + w_j^2} \int_{-\pi}^{\pi} \left|\cos(\theta - \theta_j)\right|d\theta < \sum\limits_{\ell \in \bar{S}} \sqrt{v_{\ell}^2 + w_{\ell}^2} \int_{-\pi}^{\pi}\left|\cos(\theta - \theta_{\ell})\right|d\theta.
    \]
    For the inequality $\sum_{j \in S} \sqrt{v_j^2+w_j^2} < \sum_{\ell \in \bar{S}} \sqrt{v_{\ell}^2+ w_{\ell}^2}$, it remains to observe that 
    \[
        \int_{-\pi}^{\pi}\left|\cos(\theta - \theta')\right|d\theta
    \]
    is a positive constant independent of $\theta'\in [-\pi, \pi]$---namely,4. 
\end{proof}

\subsubsection{Nonconvex Minimization}

The number of nonzero entries of a vector $z \in \mathbb{C}^N$ is approximated by the $q$th power of its $\ell_q$-quasinorm,
\[
    \sum\limits_{j = 1}^N \left|z_j\right|^q \xrightarrow[q \rightarrow 0]{} \sum\limits_{j=1}^N 1_{\{z_j \neq 0\}} = \|z\|_0. 
\]
This observation suggests to replace the $\ell_0$-minimization problem \cref{eq0.p0} by the optimization problem
\begin{equation}
    \mathop{\mathrm{minimize}}_{z \in \mathbb{C}^N} \|z\|_q \quad \text{subject to }\mathbf{A}z = y. \tag{P$_q$}
    \label{eqpq}
\end{equation}

The problem \cref{eqpq} does not provide a worse approximation of the original problem \cref{eq0.p0} when $q$ gets smaller even though it becomes nonconvex and even NP-hard. An analog is needed to justify the null space property for $0 < q < 1$.

\begin{theorem}
    \label{th0.4.9}
    Given a matrix $\mathbf{A} \in \mathbb{C}^{m \times N}$ and $0 <q \leq 1$, every $s$-sparse vector $x \in \mathbb{C}^N$ is the unique solution of \cref{eqpq} with $y = \mathbf{A}x$ if and only if, for any set $S\subset [N]$ with card$(S) \leq s$,
    \[
        \|v_S\|_q < \|v_{\bar{S}}\|_q \quad \text{for all }v \in \text{ker} \mathbf{A} \backslash \{0\}.
    \]
\end{theorem}

\begin{theorem}
    \label{th0.4.10}
    Given a matrix $\mathbf{A} \in \mathbb{C}^{m \times N}$ and $0 < p<q\leq1$, if every $s$-sparse vector $x \in \mathbb{C}^N$ is the unique solution of \cref{eqpq}(P$_q$) with $y = \mathbf{A}x$, then every $s$-sparse vector $x \in \mathbb{C}^N$ is also the unique solution of (P$_p$) with $y = \mathbf{A}x$.
\end{theorem}

\subsection{Stability}
The vectors we aim to recover via basis pursuit--or other schemes, for that matter--are sparse only in idealized situations. In more realistic scenarios, we can only claim that they are close to sparse vectors. In such cases, we would like to recover a vector $x \in \mathbb{C}^N$ with an error controlled by its distance to $s$-sparse vectors. This property: \emph{\textcolor[rgb]{1,0,0}{stability}} of the reconstruction scheme with respect to sparsity defect. 
\begin{definition}
    \label{def0.4.11}
    A matrix $\mathbf{A} \in \mathbb{C}^{m\times N}$ is said to satisfy the \textcolor[rgb]{1,0,0}{stable null space property} with constant $0 < \rho <1$ relative to a set $S \subset [N]$ if 
    \[
        \|v_S\|_1 \leq \rho\|v_{\bar{S}}\|_1 \quad \text{for all } v \in \text{ker}\mathbf{A}.
    \]
    It is to satisfy the stable null space property of order $s$ with constant $0< \rho <1$ if it satisfies the stable null space property with constant $0 <\rho < 1$ relative to any set $S \subset [N]$ with card$(S) \leq s$.
\end{definition}

\begin{theorem}
    \label{th0.4.12}
    Suppose that a matrix $\mathbf{A} \in \mathbb{C}^{m \times N}$ satisfies the stable null space property of order $s$ with constant $0 < \rho <1$. Then, for any $x \in \mathbb{C}^N$, a solution $x^{\sharp}$ of \cref{eq0.p1} with $y = \mathbf{A}x$ approximates the vector $x$ with $\ell_1$-error
    \begin{equation}
        \|x - x^{\sharp}\|_1 \leq \frac{2(1+\rho)}{(1-\rho)} \sigma_s(x)_1.
        \label{eq0.4.9}
    \end{equation}
\end{theorem}

\begin{remark}
    \label{rmk0.4.13}
    In contrast to \cref{th0.4.4} we cannot guarantee uniqueness of the $\ell_1$-minimizer anymore---although nonuniqueness is rather pathological. In any case, even when the $\ell_1$-minimizer is not unique, the theorem above states that \emph{\textcolor[rgb]{1,0,0}{every solution $x^{\sharp}$ of \cref{eq0.p1} with $y = \mathbf{A}x$ satisfies \cref{eq0.4.9}}}.
\end{remark}

Apart from improving \cref{th0.4.12}, the result also says that, under the stable null space property relative to $S$, the distance between a vector $x \in \mathbb{C}^{N}$ supported on $S$ and a vector $z \in \mathbb{C}^N$ satisfying $\mathbf{A}z = \mathbf{A}x$ is controlled by the difference between their norms.

\begin{theorem}
    \label{th0.4.14}
    The matrix $\mathbf{A} \in \mathbb{C}^{m \times N}$ satisfies the stable null space property with constant $0<\rho<1$ relative to $S$ if and only if 
    \begin{equation}
        \|z-x\|_1 \leq \frac{1+\rho}{1-zr}\left( \|z\|_1 - \|x\|_1 +2\|x_{\bar{S}}\|_1 \right)
        \label{eq0.4.10}
    \end{equation}
    for all vectors $x,z \in \mathbb{C}^N$ with $\mathbf{A}z = \mathbf{A}x$.
\end{theorem}

\textcolor[rgb]{1,0,0}{The error bound \cref{eq0.4.9}} follows from \cref{th0.4.14} as follows: Take $S$ to be a set of $s$ largest absolute coefficients of $x$, so that $\|x_{\bar{S}}\|_1 = \sigma_s(x)_1$. If $x^{\sharp}$ is a minimizer of \cref{eq0.p1}, then $\|x^{\sharp}\|_1 \leq \|x\|_1$ and $\mathbf{A}x^{\sharp} = \mathbf{A}x$. The right-hand side of inequality \cref{eq0.4.10} with $z = x^{\sharp}$ can therefore be estimated by the right hand of \cref{eq0.4.9}.

\begin{lemma}
    \label{lm0.4.15}
    Given a set $S \subset [N]$ and vectors $x,z \in \mathbb{C}^N$,
    \[
        \|(x-z)_{\overline{S}}\|_1 \leq \|z\|_1 - \|x\|_1 + \|(x-z)_S\|_1 + 2\|x_{\overline{S}}\|_1
    \]
\end{lemma}

We assume that the matrix $\mathbf{A}$ satisfies \cref{eq0.4.10} for all vectors $x,z \in \mathbb{C}^N$ with $\mathbf{A}z = \mathbf{A}x$. Given a vector $v \in \text{ker}\mathbf{A}$, since $\mathbf{A}v_{\overline{S}} = \mathbf{A}_{(-v_S)}$, we can apply \cref{eq0.4.10} with $x = -v_S$ and $z = v_{\overline{S}}$. It yields
\[
    \|v\|_1 \leq \frac{1+\rho}{1-\rho}\left( \|v_{\overline{S}}\|_1 - \|v_S\|_1 \right).
\]
This can be written as 
\[
    (1-\rho)(\|v_S\|_1 + \|v_{\overline{S}}\|_1) \leq (1+\rho)(\|v_{\overline{S}}\|_1 - \|v_S\|_1).
\]
After rearranging:
\[
    \|v_S\|_1 \leq \rho \|v_{\overline{S}}\|_1,
\]
and we recognize the stable null space property with constant $0 < \rho <1$ relative to $S$.

Conversely, we now assume that the matrix $\mathbf{A}$ satisfies the stable null space property with constant $0<\rho<1$ relative to $S$. For $x,z \in \mathbb{C}^N$ with $\mathbf{A}z = \mathbf{A}x$, since $v := z-x \in \text{ker}\mathbf{A}$, the stable null space property yields
\begin{equation}
    \|v_S\|_1 \leq \rho \|v_{\overline{S}}\|_1
    \label{eq0.4.11}
\end{equation}
Moreover, \cref{lm0.4.15} gives
\begin{equation}
    \|v_{\overline{S}}\|_1 \leq \|z\|_1 - \|x\|_1 + \|v_S\|_1 + 2\|x_{\overline{S}}\|_1.
    \label{eq0.4.12}
\end{equation}
Substituting \cref{eq0.4.11} into \cref{eq0.4.12}:
\[
    \|v_{\overline{S}}\|_1 \leq \|z\|_1 - \|x\|_1 + \rho \|v_{\overline{S}}\|_1 + 2\|x_{\overline{S}}\|_1.
\]
Since $\rho < 1$, this can be rewritten as:
\[
    \|v_{\overline{S}}\|_1 \leq \frac{1}{1-\rho}\left( \|z\|_1 -\|x\|_1 + 2\|x_{\overline{S}}\|_1 \right),
\]
Using \cref{eq0.4.11} again:
\[
    \|v_1\| = \|v_{\overline{S}}\|_1 + \|v_S\|_1 \leq (1+\rho)\|v_{\overline{S}}\|_1 \leq \frac{1+\rho}{1-\rho}\left( \|z\|_1 - \|x\|_1 + 2\|x_{\overline{S}}\|_1 \right),
\]
which proves the \cref{th0.4.14}.


\begin{remark}
    \label{rmk0.4.16}
    Given the matrix $\mathbf{A} \in \mathbb{C}^{m \times N}$, we consider, for each index set $S \subset [N]$ with card$(S) \leq s$, the operator $\mathbf{R}_S$ defined on ker$\mathbf{A}$ by $\mathbf{R}_S(v) = v_S$. The formulation \cref{eq0.4.2} of the null space property says that
    \[
        \mu := \max\left\{ \|\mathbf{R}_S\|_{1\rightarrow 1} : S \subset [N], \text{card}(S) \leq s \right\} < 1/2.
    \]
    It then follows that $\mathbf{A}$ satisfies the stable null space property with constant $\rho := \mu / (\bigcap\mu)<1$. Thus, the stability of the basis pursuit comes for free if sparse vectors are exactly recovered. However, the constant $1(1+\rho)/(1-\rho)$ in \cref{eq0.4.9} may be very large if $\rho$ is close to one.
\end{remark}

\subsection{Robustness}

In realistic situations, it is also inconceivable to measure a signal $x \in \mathbb{C}^N$ with infinite precision. This means that the measurement vector $y \in \mathbb{C}^m$ is only an approximation of the vector $\mathbf{A}x \in \mathbb{C}^m$, with
\[
    \|\mathbf{A}x - y\| \leq \eta
\]
for some $\eta \geq 0$ and for some norm $\|\cdot\|$ on $\mathbb{C}^m$---usually the $\ell_2$-norm, but the $\ell_1$-norm will also be considered. In this case, the reconstruction scheme should be required to output a vector $x^* \in \mathbb{C}^N$ whose distance to the original vector $x \in \mathbb{C}^N$ is controlled by the measurement error $\eta \geq 0$, which is referred to as the \emph{\textcolor[rgb]{1,0,0}{robustness}} of the reconstruction scheme with respect to measurement error. Then the \cref{eq0.p1} is replaced by \cref{eq0.p1n}.

\begin{definition}
    \label{def0.4.17}
    The matrix $\mathbf{A} \in \mathbb{C}^{m \times N}$ is said to satisfy the \textcolor[rgb]{1,0,0}{robust null space property} (with respect to $\|\cdot\|$) with constants $0<\rho<1$ and $\tau>0$ relative to a set $S \subset [N]$ if 
    \begin{equation}
        \|v_S\|_1 \leq \rho \|v_{\overline{S}}\|_1 + \tau\|\mathbf{A}v\| \quad \text{for all } v \in \mathbb{C}^N.
        \label{eq0.4.13}
    \end{equation}
\end{definition}
\begin{remark}
    \label{rmk0.4.18}
    Observe that the above definition does not require that $v$ is contained in $ker\mathbf{A}$. In fact, if $v \in ker\mathbf{A}$, then the term $\|\mathbf{A}v\|$ in \cref{eq0.4.13} vanishes, and we see that the \textcolor[rgb]{1,0,0}{robust null space property implies the stable null space property in \cref{def0.4.11}}.
\end{remark}

The theorem followed constitutes the first main result of this section. It incorporates the conclusion of \cref{th0.4.12} as the special case $\eta = 0$. The special case of an $s$-sparse vector $x \in \mathbb{C}^N$ is also worth a separate look.

\begin{theorem}
    \label{th0.4.19}
    Suppose that a matrix $\mathbf{A} \in \mathbb{C}^{m\times N}$ satisfies the robust null space property of order $s$ with constants $0<\rho<1$ and $\tau>0$. Then, for any $x \in \mathbb{C}^N$, a solution $x^{\sharp}$ of \cref{eq0.p1n} with $y = \mathbf{A}x + e$ and $\|e\| \leq \eta$ approximates the vector $x$ with $\ell_1$-error
    \[
        \|x - x^{\sharp}\|_1 \leq \frac{2(1+\rho)}{(1-\rho)} \sigma_s(x)_1 + \frac{4 \tau}{1-\rho}\eta.
    \]
\end{theorem}

A stronger ``if and only if'' statement valid for any index set $S$ is proved as follows:
\begin{theorem}
    \label{th0.4.20}
    The matrix $\mathbf{A} \in \mathbb{C}^{m \times N}$ satisfies the robust null space property with constants $0<\rho<1$ and $\tau>0$ relative to $S$ if and only if 
    \begin{equation}
        \|z-x\|_1 \leq \frac{1+\rho}{1-\rho}(\|z\|_1 - \|x\|_1 + 2\|x_{\overline{S}}\|_1) + \frac{2 \tau}{1-\rho}\|\mathbf{A}(z-x)\|
        \label{eq0.4.14}
    \end{equation}
    for all vectors $x, z \in \mathbb{C}^N$.
\end{theorem}


The second main result enhances the previous robustness result by \emph{\textcolor[rgb]{1,0,0}{replacing the $\ell_2$-error estimate by and $\ell_p$-error estimate for $p \geq 1$}}. A final strengthening of the null space property is required. The corresponding property could be defined relative to any fixed set $S \subset [N]$, but it is not introduced as such because this will not be needed later. 

\begin{definition}
    \label{def0.4.21}
    Given $q \geq 1$, the matrix $\mathbf{A} \in \mathbb{C}^{m \times N}$ is said to satisfy the \emph{\textcolor[rgb]{1,0,0}{$\ell_q$-robust null space property}} of order $s$ (with respect to $\|\cdot\|$) with constants $0<\rho<1$ and $\tau>0$ if, for any set $S\subset [N]$ with $card(S) \leq s$,
    \[
        \|v_S\|_q \leq \frac{\rho}{s^{1-1/q}} \|V_{\bar{S}}\|_1 + \tau \|\mathbf{A}v\| \quad \text{for all } v \in \mathbb{C}^N.
    \]
\end{definition}

In view of the \emph{\textcolor[rgb]{1,0,0}{inequality $\|v_S\|_p \leq s^{1/p-1/q}\|v_S\|_q$ for $1 \leq p \leq q$}}, we observe that the $\ell_p$-robust null space property with constants $ 0 <\rho < 1$ and $\tau > 0$ implies that, for any set $S \subset [N]$ with card$(S) \leq s$,
\[
    \|v_S\|_p \leq \frac{\rho}{s^{1-1/p}} \|v_{\overline{S}}\|_1 + \tau s^{1/p-1/q}\|\mathbf{A}v\| \quad \text{for all } v \in \mathbb{C}^N.
\]
Thus, for $1 \leq p \leq q$, the $\ell_q$-robust null space property implies the $\ell_p$-robust null space property with identical constants, modulo the change of norms $\|\cdot\| \leftarrow s^{1/p - 1/q}\|\cdot\|$. \textcolor[rgb]{1,0,0}{This justifies in particular that the $\ell_q$-robust null space property is a strengthening of the previous robust null space property}. The robustness of the quadratically constrained basis pursuit algorithm is then deduced according to the following theorem:
\begin{theorem}
    \label{th0.4.22}
    Suppose that the matrix $\mathbf{A} \in \mathbb{C}^{m \times N}$ satisfies the $\ell_2$-robust null space property of order $s$ with constants $0 <\rho <1$ and $\tau > 0$. Then, for any $x \in \mathbb{C}^N$, a solution $x^{\sharp}$ of \cref{eq0.p1n} with $\|\cdot\| = \|\cdot\|_2, y = \mathbf{A}x + e$, and $\|e\|_2 \leq \eta$ approximates the vector $x$ with $\ell_p$-error
    \begin{equation}
        \|x-x^{\sharp}\|_p \leq \frac{C}{s^{1-1/p}} \sigma_s(x)_1 + D s^{1/p-1/2}\eta, \qquad 1 \leq q \leq 2,
        \label{eq0.4.15}
    \end{equation}
    for some constants $C,D>0$ depending only on $\rho$ and $\tau$.
\end{theorem}

The estimates for the extremal values $p=1$ and $p=2$ are the most familiar. They read
\begin{eqnarray}
    \label{eqn0.4.16}
    \|x - x^{\sharp}\|_1 &\leq& C \sigma_s(x)_1 + D \sqrt{s}\eta, \notag \\
    \|x-x^{\sharp}\|_2 &\leq& \frac{C}{\sqrt{s}} \sigma_s(x)_1 + D \eta.
\end{eqnarray}
The coefficient of $\sigma_s(x)_1$ \textcolor[rgb]{1,0,0}{is a constant for $p=1$ and scales like $1/\sqrt{s}$ for $p=2$}, while the coefficient of $\eta$ \textcolor[rgb]{1,0,0}{scales like $\sqrt{s}$ for $p=1$ and is a constant for $p=2$}. 

\begin{remark}
    \label{rmk0.4.23}
    Let us comment on the fact that, regardless of the $\ell_p$-sparse in which the error is estimated, the best $s$-term approximation error $\sigma_s(x)_1$ with respect to the $\ell_1$-norm always appears on the right-hand side. One may wonder why \emph{\textcolor[rgb]{1,0,0}{the error estimate in $\ell_2$ does not involve $\sigma_s(x)_1/\sqrt{s}$.}} In fact, we will see in \cref{th11.5frombook} that such an estimate is impossible in parameter regimes of $(m, N)$ that are interesting for Compressive sensing. Besides, we have seen that unit $\ell_p$-balls with $q<1$ provide good models for compressible vectors by virtue of \cref{th2.3frombook} and its refinement \cref{th2.5frombook}. Indeed, if $\|x\|_q \leq 1$ for $q<1$, then, for $p \geq 1$,
    \[
        \sigma_s(x)_p \leq s^{1/p-1/q}.
    \]
\end{remark}
\begin{mdframed}
    \begin{theorem}
        \label{th11.5frombook}
        If a pair of measurement matrix and reconstruction map is $\ell_2$-instance optimal of order $s \geq 1$ with constant $C$, then
        \begin{equation}
            m \geq cN
            \label{eq11.3frombook}
        \end{equation}
        for some constant $c$ depending only on $C$.
    \end{theorem}
    \begin{theorem}
        \label{th2.3frombook}
        For any $q>p>0$ and any $x \in \mathbb{C}^N$,
        \[
            \sigma_s(x)_q \leq \frac{1}{s^{1/p-1/q}} \|x\|_p.
        \]
    \end{theorem}
    \begin{theorem}
        \label{th2.5frombook}
        For any $q>p>0$ and any $x \in \mathbb{C}^N$, the inequality 
        \[
            \sigma_s(x)_q \leq \frac{c_{p,q}}{s^{1/p-1/q}} \|x\|_p
        \]
        holds with
        \[
            c_{p,q} := \left[ \left( \dfrac{p}{q} \right)^{p/q} \left( 1- \dfrac{p}{q}  \right)^{1-p/q} \right]^{1/p} \leq 1.
        \]
    \end{theorem}
\end{mdframed}

Assuming perfect measurements (that is, $\eta$), the error bound \cref{eq0.4.15} yields
\[
    \|x-x^{\sharp}\|_p \leq \frac{C}{s^{1-1/p}} \sigma_s(x)_1 \leq Cs^{1/p-1/q}, \quad 1 \leq p \leq 2.
\]

\begin{remark}
    \label{rmk0.4.24}
    The $\ell_q$-robust null space property may seem mysterious at first sight, but it is necessary---save for the condition $\rho<1$---to obtain estimates of the type
    \begin{equation}
        \|x-x^{\sharp}\|_q \leq \frac{C}{s^{1-1/q}} \sigma_s(x)_1 + D \eta,
        \label{eq0.4.17}
    \end{equation}
    where $x^{\sharp}$ is a minimizer of \cref{eq0.p1n} with $y = \mathbf{A}x + e \leq \eta$. Indeed, given $v \in \mathbb{C}^N$ and $S \subset [N]$ with $card(S) \leq s$, we apply \cref{eq0.4.17} with $x=v, e= -\mathbf{A}v$ and $\eta=\|\mathbf{A}v\|$, so that $x^{\sharp} = 0$, to obtain
    \[
        \|v\|_q \leq \frac{C}{s^{1-1/q}} \|v_{\overline{S}}\|_1 + D \|\mathbf{A}v\|,
    \]
    and in particular
    \[
        \|v_S\|_q \leq \frac{C}{s^{1-1/q}} \|v_{\overline{S}}\|_1 + D \|\mathbf{A}v\|.
    \]
\end{remark}

\begin{theorem}
    \label{th0.4.25}
    Given $1 \leq p \leq q$, suppose that the matrix $\mathbf{A} \in \mathbb{C}^{m \times N}$ satisfies the $\ell_q$-robust null space property of order $s$ with constants $0<\rho<1$ and $\tau > 0$. Then, for any $x,z \in \mathbb{C}^N$,
    \[
        \|z-x\|_p \leq \frac{C}{s^{1-1/p}}\left( \|z\|_1 - \|x\|_1 + 2\sigma_s(x)_1 \right) + D s^{1/p-1/q} \|\mathbf{A}(z-x)\|,
    \]
    where $C := (1+\rho)^2/(1-\rho)$ and $D := (3+\rho)\tau/(1-\rho)$.
\end{theorem}

\begin{proof}
    Let us first remark that the $\ell_p$-robust null space properties imply the $\ell_1$-robust and $\ell_p$-robust null space property $(p \leq q)$ in the forms
    \begin{equation}
        \|v_S\|_1 \leq \rho \|v_{\overline{S}}\|_1 + \tau s^{1-1/q}\|\mathbf{A}v\|,
        \label{eq0.4.18}
    \end{equation}
    \begin{equation}
        \|v_S\|_p \leq \frac{\rho}{s^{1-1/p}}\|v_{\overline{S}}\|_1 + \tau s^{1/p-1/q} \|\mathbf{A}v\|,
        \label{eq0.4.19}
    \end{equation}
    for all $v \in \mathbb{C}^N$ and all $S \subset [N]$ with card$(S) \leq s$. Thus, in view of \cref{eq0.4.18}, applying \cref{th0.4.20} with $S$ chosen as an index set of $s$ largest (in modulus) entries of $x$ leads to 
    \begin{equation}
         \|z-x\|_1 \leq \frac{1+\rho}{1-\rho}(\|z\|_1 - \|x\|_1 + 2\sigma_s(x)_1) + \frac{2\tau`}{1-\rho}s^{1-1/q} \|\mathbf{A}(z-x)\|.
        \label{eq0.4.20}
    \end{equation}
    Then, choosing $S$ as an index of $s$ largest (in modulus) entries of $z-x$, we use \cref{th2.5frombook} to notice that
    \[
        \|z-x\|_p \leq \|(z-x)_{\overline{S}}\|_p + \|(z-x)_S\|_p \leq \frac{1}{s^{1-1/p}} \|z-x\|_1 + \|(z-x)_S\|_p.
    \]
    In view of \cref{eq0.4.19}, we derive
    \begin{eqnarray}
        \label{eq0.4.21}
        \|z-x\|_p &\leq& \frac{1}{s^{1-1/p}} \|z-x\|_1 + \frac{\rho}{s^{1-1/p}} \|(z-x)_{\overline{S}}\|_1 + \tau s^{1/p-1/q} \|\mathbf{A}(z-x)\| \notag \\
        &\leq& \frac{1+\rho}{s^{1-1/p}} \|z-x\|_1 + \tau s^{1/p-1/q} \|\mathbf{A}(z-x)\|.
    \end{eqnarray}
\end{proof}

\subsection{Recovery of Individual Vectors}
In some case, specific sparse vectors need to be dealt with rather than with all vectors supported on a given set or all vectors with a given sparsity. Therefore, some recovery conditions finer than the null space property are required. Such conditions are provided as follows, with a subtle difference between the real and complex settings, due to the fact the \emph{sign} of a number $z$, defined as
\begin{equation*}
    \mathop{\mathrm{sng}}(z) :=
    \left\{ 
        \begin{array}{cc}
            \dfrac{z}{\left|z\right|} \quad & if \ z\neq 0, \\
            0 \quad & if \ z=0,
        \end{array}
    \right.
\end{equation*}
\emph{\textcolor[rgb]{1,0,0}{is a discrete quantity when $z$ is real, but it is not when $z$ is complex}}. For a vector $x \in \mathbb{C}^N$, we denote by $\mathop{\mathrm{sgn}}(x) \in \mathbb{C}^N$ the vector with components $\mathop{\mathrm{sgn}}(x_j), j \in [N]$. The following contents start with the complex version of a recovery condition valid for sparse vectors.
\begin{theorem}
    \label{th0.4.26}
    Given a matrix $\mathbf{A} \in \mathbb{C}^{m \times N}$, a vector $x \in \mathbb{C}^N$ with support $S$ is the unique minimizer of $\|z\|_1$ subject to $\mathbf{A}z = \mathbf{A}x$ if one of the following equivalent conditions holds:
    \begin{enumerate}[(a)]
        \item $\left|\sum\limits_{j \in S}^{} \overline{\mathop{\mathrm{sgn}}(x_j)v_j}\right| < \|v_{\overline{S}}\|_1$ for all $v \in \mathop{\mathrm{ker}} \mathbf{A} \backslash \{0\}$,
        \item $\mathbf{A}_S$ is injective, and there exists a vector $h \in \mathbb{C}^m$ such that
            \[
                (\mathbf{A}^*h)_j = \mathop{\mathrm{sgn}}(x_j), \quad j \in S, \qquad \left|(\mathbf{A}^*h)_{\ell}\right| < 1, \quad \ell \in \overline{S}.
            \]
    \end{enumerate}
\end{theorem}
\begin{proof}
    Let us start by proving that (a) implies that $x$ is the unique minimizer of $\|z\|_1$ subject to $\mathbf{A}z = \mathbf{A}x$. For a vector $z \neq x$ such that $\mathbf{A}z = \mathbf{A}x$, we just have to write, with $v := x-z \in \text{ker}\mathbf{A} \backslash \{0\}$,
    \begin{eqnarray*}
        \|z\|_1 &=& \|z_S\|_1 + \|z_{\overline{S}}\|_1 = \|(x-v)_S\|_1 + \|v_{\overline{S}}\|_1 \\
        &>& \left|\left<x - v, \text{sgn}(x)_S\right>\right| + \left|\left<v,\text{sgn}(x)_S\right>\right| \geq \left|\left<x, \text{sgn}(x)_S\right>\right| = \|x\|_1.
    \end{eqnarray*}

    The implication $(b) \Rightarrow (a)$ is also simple. Indeed, observing that $\mathbf{A}v_S = -\mathbf{A}v_{\overline{S}}$ for $v \in \text{ker}\mathbf{A} \backslash \{0\}$, we write
    \begin{eqnarray*}
        \left|\sum\limits_{j \in S}^{}\overline{\text{sgn}(x_j)}v_j\right| &=& \left|\left<v_S, \mathbf{A}^*h\right>\right| = \left|\left<\mathbf{A}v_S, h\right>\right| = \left|\left<\mathbf{A}v_{\overline{S}}, h\right>\right| \\
        &=& \left|\left<v_{\overline{S}, \mathbf{A}^*h}\right>\right| \leq \max\limits_{\ell \in \bar{S}}^{} \left|(\mathbf{A}^*h)_{\ell}\right| \|v_{\overline{S}}\|_1 < \|v_{\overline{S}}\|_1.
    \end{eqnarray*}
    The strict inequality holds since $\|v_{\overline{S}}\|_1 > 0$; otherwise, the nonzero vector $v \in \text{ker}\mathbf{A}$ would be supported on $S$, contradicting the injectivity of $\mathbf{A}_S$. 
    
    The remaining implication $(a) \Rightarrow (b)$ requires more work. We start by noticing that $(a)$ implies $\|v_{\overline{S}}\| > 0$ for all $v \in \text{ker}\mathbf{A} \backslash \{0\}$. It follows that the matrix $\mathbf{A}_S$ is injective. Indeed, assume $\mathbf{A}_S v_S = 0$ for some $v_S \neq 0$ and complete $v_S$ to a vector $v \in \mathbb{C}^N$ by setting $v_{\overline{S}} = 0$. Then $v$ is contained in ker$\mathbf{A} \backslash \{0\}$, which is in contradiction with $\|v_{\overline{S}} > 0\|$ for all $v \in \text{ker}\mathbf{A}\backslash \{0\}$. Next, since the continuous function $v \mapsto\left|\left<v, \text{sgn}(x)_S\right>\right|/\|v_{\overline{S}}\|_1$ takes values less than one on the unit sphere of ker$\mathbf{A}$, which is compact, its maximum $\mu$ satisfies $\mu<1$. By homogeneity, we deduce
    \[
        \left|\left<v, \text{sgn}(x)_S\right>\right| \leq \mu \|v_{\overline{S}}\|_1 \quad \text{for all } v \in \text{ker}\mathbf{A}.
    \]
    We then define, for $\mu<\nu<1$, the convex set $\mathcal{C}$ and the affine set $\mathcal{D}$ by
    \begin{eqnarray*}
        \mathcal{C} &:=& \left\{ z \in \mathbb{C}^N : \|z_S\|_1 + \nu \|z_{\overline{S}}\|_1 \leq \|x\|_1 \right\}, \\
        \mathcal{D} &:=& \left\{ z \in \mathbb{C}^N : \mathbf{A}z = \mathbf{A}x \right\}.
    \end{eqnarray*}
    The intersection $\mathcal{C} \cup \mathcal{D}$ reduces to $\{x\}$. Indeed, we observe that $x \in \mathcal{C} \cup \mathcal{D}$, and if $z \neq x$ belongs to $\mathcal{C}\cup \mathcal{D}$, setting $v := x-z \in \text{ker}\mathbf{A} \backslash \{0\}$, we obtain a contradiction from 
    \begin{eqnarray*}
        \|x\|_1 &\geq& \|z_S\|_1 + \nu\|z_{\overline{S}}\|_1 = \|(x-v)_S\|_1 + \nu\|v_{\overline{S}}\|_1 \\
        &>& \|(x-v)_S\|_1 + \mu\|v_{\overline{S}}\|_1 \geq \left|\left<x-v, \text{sgn}(x)_S\right>\right| + \left|\left<v, \mathop{\mathrm{sgn}}(x)_S\right>\right| \\
        &\geq& \left|\left<x, \mathop{\mathrm{sgn}}(x)_S\right>\right| = \|x\|_1.
    \end{eqnarray*}
    Thus, by the separation of convex sets via hyperplanes there exists a vector $w \in \mathbb{C}^N$ such that
    \begin{eqnarray}
        \label{eq0.4.22}
        \mathcal{C} \subset \left\{ z \in \mathbb{C}^N : \mathop{\mathrm{Re}}\left<z,w\right> \leq \|x\|_1 \right\}, \\
        \label{eq0.4.23}
        \mathcal{D} \subset \left\{ z \in \mathbb{C}^N : \mathop{\mathrm{Re}}\left<z,w\right> = \|x\|_1 \right\}.
    \end{eqnarray}
    In view of \cref{eq0.4.22}, we have
    \begin{eqnarray*}
        \|x\|_1 &\geq& \max\limits_{\|z_S+\nu z_{\overline{S}}\|_1 \leq \|x\|_1}^{} \mathop{\mathrm{Re}}\left<z,w\right> \\
        &=& \max\limits_{\|z_S + \nu z_{\overline{S}}\|_1 \leq \|x\|_1}^{} \mathop{\mathrm{Re}} \left( \sum\limits_{j \in S}^{} z_j \overline{w_j} + \sum\limits_{j \in \overline{S}}^{} \nu z_j \overline{w_j}/\nu \right) \\
        &=& \max\limits_{\|z_S + \nu z_{\overline{S}}\|_1 \leq \|x\|_1}^{} \mathop{\mathrm{Re}} \left<z_S + \nu z_{\overline{S}}, w_s +(1/\nu)w_{\overline{S}}\right> \\
        &=&  \|x\|_1 \|w_S + (1/\nu)w_{\overline{S}}\|_{\infty} = \|x\|_1 \max \{\|w_S\|_{\infty}, (1/\nu)\|w_{\overline{S}}\|_{\infty}\}.
    \end{eqnarray*}
    Setting aside the case $x \neq 0$ (where the choice $h=0$ would do), we obtain $\|w_S\|_{\infty} \leq `$ and $\|w_{\overline{S}}\|_{\infty} \leq \nu <`$. From \cref{eq0.4.23}, we derive $\mathop{\mathrm{Re}}<x,w> = \|x\|_1$, i.e., $w_j = \text{sgn}(x_j)$ for all $j \in S$, and also Re$<v,w> = 0$ for all $v \in \text{ker}\mathbf{A}$, i.e., $w \in (\text{ker}\mathbf{A})^{\perp}$. Since (ker$\mathbf{A}$)$^{\perp} = \text{ran}\mathbf{A}^*$, we write $w = \mathbf{A}^*h$ for some $h \in \mathbb{C}^m$. This establishes ($b$).

\end{proof}

\begin{remark}
    \label{rmk0.4.27}
    \begin{enumerate}[(a)]
        \item If a vector $x \in \mathbb{C}^N$ with support $S$ satisfies condition (a) of the previous theorem, then all vectors $x' \in \mathbb{C}^N$ with support $S' \subset S$ and $\text{sgn}(x')_{S'}$ are also recovered via basis pursuit. Indeed, for $v \in \text{ker}\mathbf{A} \backslash \{0\}$,
            \begin{eqnarray*}
                \left|\sum\limits_{j\in S'}^{}\text{sgn}(x_j')v_j\right| &=& \left|\sum\limits_{j \in S}^{}\text{sgn}(x_j)v_j - \sum\limits_{j \in S\backslash S'}^{}\text{sgn}(x_j)v_j\right| \\
                &\leq& \left|\sum\limits_{j \in S}^{}\text{sgn}(x_j)v_j \right| + \sum\limits_{j \in S \backslash S'}^{}|v_j| < \|v_{\overline{S}}\|_1 + \|v_{S \backslash S'}\|_1 = \|v_{\overline{S'}}\|_1.
            \end{eqnarray*}
        \item \cref{th0.4.26} can be made stable under noise on the measurements and under passing to compressible vectors. However, the resulting error bounds are slightly weaker than the ones of \cref{th0.4.25} under the $\ell_2$-robust null space property.
    \end{enumerate}
\end{remark}

\begin{corollary}
    \label{clr0.4.28}
    let $a_1,\dots,a_N$ be the columns of $\mathbf{A} \in \mathbb{C}^{c \times N}$. For $x \in \mathbb{C}^N$ with support $S$, if the matrix $\mathbf{A}_S$ is injective and if 
    \begin{equation}
        \left|\left<\mathbf{A}_S^{\dagger}a_{\ell}, \text{sgn}(x_S)\right>\right| < 1 \quad \text{for all } \ell \in \overline{S},
        \label{eq0.4.24}
    \end{equation}
    then the vector $x$ is the unique solution of \cref{eq0.p1} with $y = \mathbf{A}x$.
\end{corollary}

\begin{remark}
    \label{rmk0.4.29}
    In general, there is no converse to \cref{th0.4.26}. Let us consider, for instance,
    \[
        \mathbf{A} := \left[ 
            \begin{array}[]{ccc}
                1 & 0 & -1 \\
                0 & 1 & -1
            \end{array}
        \right],
        x = \left[ 
            \begin{array}[]{c}
                e^{-\pi i/3} \\
                e^{\pi i/3} \\
                0
            \end{array}
        \right].
    \]
\end{remark}
We can verify that $x$ is the unique minimizer of $\|z\|_1$ subject to $\mathbf{A}z = \mathbf{A}x$; However, (a) in \cref{th0.4.26} fails. Indeed, for a vector $v = [\zeta,\zeta,\zeta] \in \mathop{\mathrm{ker}}\mathbf{A} \backslash \{0\}$, we have $|\overline{\mathop{\mathrm{sgn}}(x_1)}v_1 + \overline{\mathop{\mathrm{sgn}}(x_2)}v_2| = |(e^{\pi i/3} + e^{-\pi i/3})\zeta| = |\zeta|$ while $\|v_{\{3\}}\|_1 = |\zeta|$. A converse to \cref{th0.4.26} holds in the real setting.

\begin{theorem}
    \label{th0.4.30}
    Given a matrix $\mathbf{A} \in \mathbb{R}^{m \times N}$, a vector $x \in \mathbb{R}^N$ with support $S$ is the  unique minimizer of $\|z\|_1$ subject to $\mathbf{A}z = \mathbf{A}x$ if and only if one of the following equivalent conditions holds:
    \begin{enumerate}[(a)]
        \item $\left|\sum\limits_{j \in X}^{} \mathop{\mathrm{sgn}}(x_j)v_j\right| < \|v_{\overline{S}}\|_1$ for all $v \in \mathop{\mathrm{ker}} \mathbf{A} \backslash \{0\}$.
        \item $\mathbf{A}_S$ is injective, and there exists a vector $h \in \mathbb{R}^m$ such that 
            \[
                (\mathbf{A}^T h)_j = \mathop{\mathrm{sgn}}(x_j), \quad j \in S, \qquad \left|(\mathbf{A}^T h)_{\ell}\right| < 1, \quad \ell \in \overline{S}.
            \]
    \end{enumerate}
\end{theorem}

\begin{remark}
    \label{rmk0.4.31}
    \cref{th0.4.30} shows that in the real setting the recovery of a given vector via basis pursuit depends only on its sign pattern, but not on the magnitude of its entries. Moreover, by \cref{rmk0.4.27}(a), if a vector $x \in \mathbb{R}^N$ with support $S' \subset S$ and sgn$(x')_{S'} = \mathop{\mathrm{sgn}}(x)_{S'}$ are slso exactly recovered via basis pursuit.
\end{remark}

The construction of the ``dual vector'' $h$ described in property (b) of \cref{th0.4.26} and \cref{th0.4.30} is not always straightforward. The following condition involving an ``inexact dual vector'' is somtimes easier to verify.

\begin{theorem}
    \label{th0.4.32}
    Let $a_1,\dots,a_N$ be the columns of $\mathbf{A} \in \mathbb{C}^{m \times N}$ and let $x \in \mathbb{C}^N$ with support $S$. For $\alpha,\beta,\gamma,\theta \geq 0$, assume that
    \begin{equation}
        \|(\mathbf{A}^*_S \mathbf{A}_S)^{-1}\|_{2 \rightarrow 2} \leq \alpha, \qquad \max\limits_{\ell \in \overline{S}}^{}\|\mathbf{A}^*_S a_{\ell}\|_2 \leq \beta,
        \label{eq0.4.25}
    \end{equation}
    and that there exists a vector $u = \mathbf{A}^*h \in \mathbb{C}^N$ with $h \in \mathbb{C}^m $ such that 
    \begin{equation}
        \|u_S - \mathop{\mathrm{sgn}}(x_S)\|_2 \leq \gamma \quad and \quad \|u_{\overline{S}}\|_{\infty} \leq \theta.
        \label{eq0.4.26}
    \end{equation}
    If $\theta + \alpha\beta\gamma<1$, then $x$ is the unique minimizer of $\|z\|_1$ subject to $\mathbf{A}z = \mathbf{A}x$. 
\end{theorem}

\begin{theorem}
    \label{th0.4.33}
    Let $a_1,\dots,a_N$ be the columns of $\mathbf{A} \in \mathbb{C}^{m \times N}$, let $x \in \mathbb{C}^N$ with $s$ largest absolute entries supported on $S$, and let $y = \mathbf{A}x+e$ with $\|e\|_2 \leq \eta$. For $\delta,\beta,\gamma,\tau\geq 0$ with $\delta<1$, assume that
    \begin{equation}
        \|\mathbf{A}^*_S \mathbf{A}_S - \mathbf{Id}\|_{2 \rightarrow 2} \leq \delta, \qquad \|\mathbf{A}^*_S a_{\ell}\|_2 \leq \beta,
        \label{eq0.4.27}
    \end{equation}
    and that there exists a vector $u = \mathbf{A}^*h \in \mathbb{C}^N$ with $h \in \mathbb{C}^m$ such that
    \begin{equation}
        \|u_S - \mathop{\mathrm{sgn}}(x_S)\|_2 \leq \gamma, \quad \|u_{\overline{S}}\|_{\infty} \leq \theta, \quad and \quad \|h\|_2 \leq \tau\sqrt{s}.
        \label{eq0.4.28}
    \end{equation}
    if $\rho := \theta + \beta \gamma/(1-\delta)<1$, then a minimizer $x^{\sharp}$ of $\|z\|_1$ subject to $\|\mathbf{A}z - y\|_2 \leq \eta$ satisfies
    \[
        \|x - x^{\sharp}\|_2 \leq C_1 \sigma_s(x)_1 + (C_2 + C_3 \sqrt{s})\eta
    \]
    for some constant $C_1,C_2,C_3 > 0$ depending only on $\delta,\beta,\gamma,\theta,\tau$.
\end{theorem}

\begin{remark}
    \label{rmk0.4.34}
     The proof reveals explicit values of the constants, namely,
     \[
         C_1 = \frac{2}{1-\rho}\left( 1+\frac{\beta}{1-\delta} \right), \qquad C_2 = \frac{2\sqrt{1+\delta}}{1-\delta}\mu\left( \frac{\gamma}{1-\rho}\left( 1+\frac{\beta}{1-\delta} \right)+1 \right),
     \]
     \[
         C_3 = \frac{2\tau}{1-\rho}\left( 1+\frac{\beta}{1-\delta} \right).
     \]
     For instance, the specific choice $\delta = \beta = \gamma = 1/2, \theta = 1/4$, and $\tau = 2$, for which $\rho = 3/4$, results in $C_1 \approx 16, C_2 = 10\sqrt{6} \approx 24.49,$ and $C_3 \approx 32.$ 
\end{remark}

\begin{proof}
    Observe that $x$ is feasible for the quadratically constrained $\ell_1$-minimization problem due to the assumed $\ell_2$-bound of the perturbation $e$. Setting $v:=x^{\sharp} - x$, the minimality of $\|x^{\sharp}\|_1$ implies
    \begin{eqnarray*}
        \|x\|_1 &\geq& \|x^{\sharp}\|_1 = \|x+v\|_1 = \|(x+v)_S\|_1 + \|(x+v)_{\overline{S}}\|_1 \\
        &\geq& \mathop{\mathrm{Re}}\left<(x+v)_S, \mathop{\mathrm{sgn}}(x_S)\right> +\|v_{\overline{S}}\|_1 - \|x_{\overline{S}}\|_1 \\
        &=& \|x_S\|_1 + \mathop{\mathrm{Re}}\left<v_S, \mathop{\mathrm{sgn}}(x_S)\right> + \|v_{\overline{S}}\|_1 - \|x_{\overline{S}}\|_1.
    \end{eqnarray*}
    Rearranging and using the fact that $\|x\|_1 = \|x_S\|_1 + \|x_{\overline{S}}\|_1$ yields 
    \begin{equation}
        \|v_{\overline{S}}\|_1 \leq 2\|x_{\overline{S}}\|_1 + \left|\left<v_S\right>, \mathop{\mathrm{sgn}}(x_S)\right|.
        \label{eq0.4.29}
    \end{equation}
    In view of \cref{eq0.4.28}, we have
    \begin{eqnarray}
        \label{eq0.4.30}
        \left|\left<v_S, \mathop{\mathrm{sgn}}(x_S)\right>\right| &\leq& \left|\left<v_S, \mathop{\mathrm{sgn}}(x_S) - u_S\right>\right| + \left|\left<v_S, u_S\right>\right| \notag \\
        &\leq&\gamma \|v_S\|_2 + \left|\left<v,u\right>\right| + \left|\left<v_{\overline{S}}, u_{\overline{S}}\right>\right|.
    \end{eqnarray}
    The first inequality of \cref{eq0.4.27} guarantees that $\|(\mathbf{A}^*_S \mathbf{A}_S)^{-1}\|_{2\rightarrow 2} \leq 1/(1-\delta)$ and $\|\mathbf{A}^*_S\|_{2 \rightarrow 2} \leq \sqrt{1+\delta}$. Hence,
    \begin{eqnarray}
        \label{eq0.4.31}
        \|v_S\|_2 &\leq& \frac{1}{1-\delta} \|\mathbf{A}^*_S\mathbf{A}_S v_S\|_2 \leq \frac{1}{1-\delta} \|\mathbf{A}^*_S \mathbf{A}_{\overline{S}}v_{\overline{S}}\|_2 + \frac{1}{1-\delta} \|\mathbf{A}^*_S \mathbf{A} v\|_2 \\
        &\leq& \frac{1}{1-\delta}\sum\limits_{\ell \in \overline{S}}^{}\left|v_{\ell}\right\| \left\|\mathbf{A}^*_S a_{\ell}\right\|_2 + \frac{\sqrt{1+\delta}}{1-\delta} \|\mathbf{A}v\|_2 \\
        &\leq& \frac{\beta}{1-\delta}\|v_{\overline{S}}\|_1 + \frac{2\sqrt{1+\delta}}{1-\delta} \eta.
    \end{eqnarray}
    The last step involved the inequality $\|\mathbf{A}v\|_2 \leq 2\eta$, which follows from the optimization constraints as 
    \[
        \|\mathbf{A}v\|_2 = \|\mathbf{A}(x^{\sharp} -x)\|_2 \leq \|\mathbf{A}x^{\sharp} -y\|_2 + \|y-\mathbf{A}x\|_2 \leq 2\eta.
    \]
    The latter inequality combined with $\|h\|_2 \leq \tau\sqrt{s}$ also gives
    \[
        \left|\left<v,u\right>\right| = \left|\left<v, \mathbf{A}^*h\right>\right| = \left|\left<\mathbf{A}v, h\right>\right| \leq \|\mathbf{A}v\|_2 \|h\|_2 \leq 2\tau\eta\sqrt{s},
    \]
    while $\|u_{\overline{S}}\|_{\infty} \leq \theta$ implies $\left|\left<v_{\overline{S}}, u_{\overline{S}}\right>\right| \leq \theta \|v_{\overline{S}}\|_1$. Substituting these estimates in \cref{eq0.4.30} and in turn in \cref{eq0.4.29} yields
    \[
        \|v_{\overline{S}}\|_1 \leq 2\|\overline{S}\|_1 + \left( \theta+\frac{\beta\gamma}{1-\delta} \right)\|v_{\overline{S}}\|_1 + \left( 2\gamma\frac{\sqrt{1+\delta}}{1-\delta}+2\tau\sqrt{s} \right)\eta.
    \]
    Since $\rho = \theta + \beta\gamma/(1-\delta) < 1$, this can be rearranged as 
    \begin{equation}
        \|v_{\overline{S}}\|_1 \leq \frac{2}{1-\rho}\|x_{\overline{S}}\|_1 + \frac{2(\mu\gamma+\tau\sqrt{s})}{1-\rho}\eta,
        \label{eq0.4.32}
    \end{equation}
    where $\mu := \sqrt{1+\delta}/(1-\delta)$. Using \cref{eq0.4.31} once again, we derive
    \begin{equation}
        \|v_S\|_2 \leq \frac{2\beta}{(1-\rho)(1-\delta)} \|x_{\overline{S}}\|_1 + \left( \frac{2\beta(\mu\gamma+\tau\sqrt{s})}{(1-\rho)(1-\delta)}+2\mu \right)\eta.
        \label{eq0.4.33}
    \end{equation}
    Finally, combining \cref{eq0.4.32} and \cref{eq0.4.33}, we obtain
    \begin{eqnarray*}
        \|v\|_2 &\leq& \|v_{\overline{S}}\|_2 + \|v_S\|_2 \leq \|v_{\overline{S}}\|_1 + \|v_S\|_2 \\
        &\leq&\frac{2}{1-\rho}\left( 1+\frac{\beta}{1-\delta} \right) \|x_{\overline{S}}\|_1 + \left( \frac{2(\mu\gamma+\tau\sqrt{s})}{1-\rho} \left( 1+\frac{\beta}{1-\delta} \right) + 2\mu \right)\eta.
    \end{eqnarray*}
    Taking $\|x_{\overline{S}}\|_1 = \sigma_s(x)_1$ into account, we arrive at the desired result.
\end{proof}

The next characterization of exact recovery via $\ell_1$-minimization involves \emph{\textcolor[rgb]{1,0,0}{tangent cones}} to the $\ell_1$-ball. For a vector $x \in \mathbb{R}^N$, we introduce the convex cone
\begin{equation}
    T(x) = \mathop{\mathrm{cone}}\left\{ z-x : z\in \mathbb{R}^N, \|z\|_1 \leq \|x\|_1 \right\},
    \label{eq0.4.34}
\end{equation}
where the notation \emph{\textcolor[rgb]{1,0,0}{cone represents the conic hll;}}
\begin{mdframed}
    \label{cone}
    The \emph{\textcolor[rgb]{1,0,0}{norm cone}} associated with the norm $\|\cdot\|$ is the set 
    \[
        \mathcal{C} = \left\{ (x,t) \  | \  \|x\| \leq t \right\} \subseteq \mathbf{R}^{n+1}.
    \]
    It is (as the name suggests) a \textbf{\textcolor[rgb]{1,0,0}{convex cone}}.
    \textbf{Example 2.3} \emph{The second-order cone} is the norm cone for the Euclidean norm, i.e.,
    \begin{eqnarray*}
        \mathcal{C} &=&  \left\{ (x,t) \in \mathbf{R}^{n+1} \ | \ \|x\|_2 \leq t \right\} \\
        &=& \left\{ \left[ 
            \begin{array}[]{c}
                x \\
                t
            \end{array}
        \right] \ \left| \ \left[ 
            \begin{array}[]{c}
                x \\
                t
            \end{array}
        \right]^T \right. 
        \left[ 
            \begin{array}[]{cc}
                I & 0 \\
                0 & -1
            \end{array}
        \right]
        \left[ 
            \begin{array}[]{c}
                x \\
                t
            \end{array}
        \right]
        \leq 0, t \geq 0
    \right\}.
    \end{eqnarray*}
    The second-order cone is also known by several other names. It is called the \emph{\textbf{\textcolor[rgb]{1,0,0}{quadratic cone}}}, since it is defined by a quadratic inequality. It is also called the \emph{\textbf{\textcolor[rgb]{1,0,0}{Lorentz cone}}} or \emph{\textbf{\textcolor[rgb]{1,0,0}{ice-cream cone}}}. 
\end{mdframed}

\begin{theorem}
    \label{th0.4.35}
    For $\mathbf{A} \in \mathbb{R}^{m \times N}$, a vector $x \in \mathbb{R}^N$ is the unique minimizer of $\|z\|_1$ subject to $\mathbf{A}z = \mathbf{A}x$ if and only if $ker\mathbf{A} \cup T(x) = \{0\}$.
\end{theorem}

\begin{theorem}
    \label{th0.4.36}
    For $\mathbf{A} \in \mathbb{R}^{m \times N}$, let $x \in \mathbb{R}^N$ and $y= \mathbf{A}x + e \in \mathbb{R}^m$ with $\|e\|_2 \leq \eta$. If 
    \[
        \inf\limits_{v \in T(x), \|v\|_2=1} \|\mathbf{A}v\|_2 \geq \tau
    \]
    for some $\tau > 0$, then a minimizer $x^{\sharp}$ of $\|z\|_1$ subject to $\|\mathbf{A}z - y\|_2 \leq \eta$ satisfies 
    \begin{equation}
        \|x - x^{\sharp}\|_2 \leq \frac{2\eta}{\tau}
        \label{eq0.4.35}
    \end{equation}
\end{theorem}

\begin{proof}
    The inequality $\|x^{\sharp}\|_1 \leq \|x\|_1$ yields $v := (x^{\sharp} - x)/ \|x^{\sharp} - x\|_2 \in T(x)$---note that $x^{\sharp} - x \neq 0$ can be safely assumed. Since $\|v\|_2 = 1$, the assumption implies $\|\mathbf{A}v\|_2 \geq \tau$, i.e., $\|\mathbf{A}(x^{\sharp} - x)\|_2 \geq \tau \|x^{\sharp} - x\|_2$. It remains to remark that
    \[
        \|\mathbf{A}(x^{\sharp} - x)\|_2 \leq \|\mathbf{A}x^{\sharp} - y\|_2 + \|\mathbf{A}x - y\|_2 \leq 2\eta
    \]
    in order to obtain the desired result.
\end{proof}

\subsection{The Projected Cross-Polytope}
\begin{mdframed}
    \label{polytope}
    A \emph{\textcolor[rgb]{1,0,0}{polyhedron}} is defined as the solution set of a finite number of linear equalities and inequalities:
    \label{eqpolytope}
    \[
        \mathcal{P} = \left\{ x \ | \ a_j^T x \leq b_j, j =1,\dots,m, c_j^T x = d_j, j =1,\dots,p \right\}.
    \]
    A polyhedron is thus the intersection of a finite number of halfspaces and hyperplanes. Affine sets (e.g., subspaces, hyperplanes, lines), rays, line segments, and halfspaces are all \emph{\textcolor[rgb]{1,0,0}{polyhedra}}. It is easily shown that polyhedra are convex sets. A bounded polyhedron is sometimes called a \emph{\textcolor[rgb]{1,0,0}{polytope}}, but some authors use the opposite convention (i.e., polytope for any set of the form \cref{eqpolytope}, and polyhedron when it is bounded).
\end{mdframed}

A \emph{\textcolor[rgb]{1,0,0}{convex polytope}} in $\mathbb{R}^n$ can be viewed either as the convex hull of a finite set of points or as a bounded intersection of finitely many half-spaces. For instance, with $(e_1,\dots,e_N)$ denoting the canonical basis of $\mathbb{R}^N$, the unit ball of $\ell_1^N$ described as 
\[
    B_1^N := \mathop{\mathrm{conv}} \left\{ e_1, -e_1,\dots,e_N,-e_N \right\} = \bigcap\limits_{\varepsilon \in \{-1,1\}^N} \left\{ z \in \mathbb{R}^N\ :\ \sum\limits_{i = 1}^{N} \varepsilon_i z_i \leq 1 \right\}
\]
is a convex polytope which is sometimes called \emph{\textcolor[rgb]{1,0,0}{cross-polytope}}. 


\subsection{Low-Rank Matrix Recovery}
\begin{mdframed}
    \label{singular}
    If $A$ and $X$ are in $\mathbb{R}^{n \times n}$ and satisfy $AX=I$, then $X$ is the \emph{inverse} of $A$ and is denoted by $A^{-1}$. If $A^{-1}$ exists, then $A$ is said to be \emph{\textcolor[rgb]{1,0,0}{nonsigular}}. Otherwise, we say $A$ is \emph{\textcolor[rgb]{1,0,0}{singular}}. The inverse of a product is the reverse product of the inverses:
    \[
        (AB)^{-1} = B^{-1}A^{-1}.
    \]
    Likewise, the transpose of the inverse is the inverse of the transpose:
    \[
        (A^{-1})^T = (A^T)^{-1} \equiv A^{-T}
    \]
    The eigenvalues of $A^H A$ are nonnegative. Their square roots are called \emph{\textcolor[rgb]{1,0,0}{singular values}} of $A$ and are denoted by $\sigma_i,\ i=1,\dots,m$. Thus:
    \begin{eqnarray*}
        \|A\|_1 &=& 
    \end{eqnarray*}<++>
\end{mdframed}<++>
In this context, the number of nonzero singular values---the rank of a matrix---replaces the number of nonzero entries: the sparsity of a vector.

We suppose that a matrix $\mathbf{X} \in \mathbb{C}^{n_1 \times n_2}$ of rank at most $r$ is observed via the measurement vector $y = \mathbf{\mathcal{A}}(\mathbf{X})\in \mathbb{C}^m$ where $\mathbf{\mathcal{A}}$ is a linear map from $\mathbb{C}^{n_1 \times n_2}$ to $\mathbb{C}^m$. As in the vector case the first approach to this problem that probably comes to mind is to solve the rank-minimization problem
\[
    \mathop{\mathrm{minimize}}\limits_{\mathbf{Z} \in \mathbb{C}^{n_1 \times n_2}} \mathop{\mathrm{rank}}(\mathbf{Z}) \quad \text{subject to } \mathbf{\mathcal{A}}(\mathbf{X}) = y.
\]
Unfortunately, like $\ell_0$-minimization this problem is NP-hard. In fact, the rank of $\mathbf{Z}$ equals the $\ell_0$-norm of the vector $[\sigma_1(\mathbf{Z}),\dots,\sigma_n(\mathbf{Z})]^T$ of singular values of $\mathbf{Z}$. 


\subsection{Notes}

Throughout the section, we have insisted on sparse vectors to be unique solutions of \cref{eq0.p1}. If the uniqueness requirement is dropped, then a necessary and sufficient condition for every $s$-sparse vector to be a solution of \cref{eq0.p1} would be a \textcolor[rgb]{1,0,0}{\emph{weak null space property where the strict inequality is replaced by a weak inequality sign}}.

The null space property is somewhat folklore in the compressive sensing literature. It appears implicitly in works in \cite{Donoho2003}, in \cite{Donoho2001}, and in \cite{Elad2002}. In \cite{Gribonval2003} the notion was also isolated. The name was first used in \cite{Cohen2009}, albeit for a property slightly more general then \cref{eq0.4.3}, namely, $\|v\|_1 \leq C\sigma_s(v)_1$ for all $v \in \text{ker}\mathbf{A}$, where $C \geq 1$ is an unspecified constant. 

The equivalence between the real and complex null space properties was established in \cite{Foucart2010}. using a \emph{\textcolor[rgb]{1,0,0}{different argument}} than the one of \cref{th0.4.7}. The result was generalized in \cite{Lai2011}. The proof of \cref{th0.4.7} follows their argument.

The term \emph{\textcolor[rgb]{1,0,0}{instance optimality}} is sometimes also used for what we called stability. The stability and robustness of sparse reconstruction via basis pursuit, as stated after \cref{th0.4.22}, were established in \cite{Candes2006a} under a restricted isometry property---condition on the measurement matrix.

The fact that sparse recovery via $\ell_p$-minimization implies sparse recovery via $\ell_p$-minimization whenever $0<p<q\leq 1$ was proved in \cite{Gribonval2007}. 








