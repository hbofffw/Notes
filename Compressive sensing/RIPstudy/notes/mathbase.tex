\chapter{Math Basics}
\label{mathbase}
\section{Background in Linear Algebra}
\subsection{Types of Matrices}
\begin{enumerate}
    \item Symmetric matrices: $A^T = A$.
    \item Hermitian matrices: $A^H = A$.
    \item Skew-symmetric matrices: $A^T = -A$.
    \item Skew-Hermitian matrices: $A^H = -A$.
    \item Normal matrices: $A^H A = A A^H$.
    \item Nonnegative matrices: $a_{ij}\geq 0, i,j=1,\dots,n$ (similar definition for nonpositive, positive, and negative matrices).
    \item Unitary matrices: $Q^H Q = I$.
\end{enumerate}

\section{Moore-Penrose pseudoinverse}
\label{moore-penrose}
\subsection{Notation}
\begin{itemize}
    \item $K$ will denote one of the fields of real or complex numbers, denoted $\mathbb{R},\mathbb{C}$, respectively. The vector space of $m\times n$ matrices over $K$ is denoted by $M(m,n;K)$.
    \item For $A \in M(m,n;K)$, $A^T$ and $A^*$ denote the transpose and Hermitian transpose (also called conjugate transpose) respectively. If $K = \mathbb{R}$, then $A^* = A^T$.
    \item For $A \in M(m,n;K)$, then im$(A)$ denotes the range of $A$ (the space spanned by the column vectors of $A$) and ker$(A)$ \textcolor[rgb]{1,0,0}{denotes the kernel (null space)} of $A$.
    \item Finally, for any positive integer $n, I_n \in M(n,n;K)$ denotes the $n \times n$ identity matrix.
\end{itemize}

\subsection{Definition}

For $A \in M(m,n;K)$, a pseudoinverse of $A$ is defined as a matrix $A^{+} \in M(n,m;K)$ satisfying all of the following four criteria:
\begin{enumerate}
    \item $AA^+A = A$ ($AA^+$ need not be the general identity matrix, but it maps all column vectors of $A$ to themselves);
    \item $A^+AA^+ = A^+$ ($A^+$ is a weak inverse for the multiplicative semigroup);
    \item $(AA^+)^* = AA^+$ ($AA^+$ is Hermitian); and 
    \item $(A^+A)^* = A^+A$ ($A^+A$ is also Hermitian).
\end{enumerate}

Matrix $A^+$ exists for any matrix $A$, but when the latter has full rank, $A^+$ can be expressed as a \textcolor[rgb]{1,0,0}{simple algebra formula}.

In particular, when $A$ has \emph{full column rank} (and thus matrix $A^*A$ is invertible), $A^+$ can be computed as:
\begin{equation}
    A^+ = (A^*A)^{-1} A^*.
    \label{eq0.2.1}
\end{equation}
This particular pseudoinverse constitutes a \emph{\textcolor[rgb]{1,0,0}{left inverse}}, since, in this case, $A^+A = I$.

When $A$ has \emph{full row rank} (matrix $AA^*$ is invertible), $A^+$ can be computed as:
\begin{equation}
    A^+ = A^*(AA^*)^{-1}.
    \label{eq0.2.2}
\end{equation}
This is a \emph{\textcolor[rgb]{1,0,0}{right inverse}}, as $AA^+=I$.

\subsection{Properties}

\begin{itemize}
    \item If $A$ has real entries, then so does $A^+$.
    \item If $A$ is invertible, its pseudoinverse is its inverse. That is: $A^+ = A^{-1}$.
    \item The pseudoinverse of a zero matrix is its transpose.
    \item The pseudoinverse of the pseudoinverse is the original matrix: $(A^+)^+ = A$.
    \item Pseudoinverse commutes with transposition, conjugation, and taking the conjugate transpose:
        \begin{equation}
            (A^T)^+ = (A^+)^T, \overline{A}^+ = \overline{A^+}, (A^*)^+ = (A^+)^*.
            \label{eq0.2.3}
        \end{equation}
    \item The pseudoinverse of a scalar multiple of $A$ is the reciprocal multiple of $A^+$:
        \begin{equation}
            (\alpha A)^+ = \alpha^{-1} A^+ for \alpha \neq 0.
            \label{eq0.2.4}
        \end{equation}
\end{itemize}

\subsubsection{Identities}
\begin{eqnarray}
    A^{+} &=& A^{+} \quad A^{+*} \quad A^{*} \notag \\
    A^{+} &=& A^{*} \quad A^{+*} \quad A^{+} \notag\\
    A^{} &=& A^{+*} \quad A^{*} \quad A^{} \notag\\
    A^{} &=& A^{} \quad A^{*} \quad A^{+*} \notag\\
    A^{*} &=& A^{*} \quad A^{} \quad A^{+} \notag\\
    A^{*} &=& A^{+} \quad A^{} \quad A^{*} 
    \label{eq0.2.5}
\end{eqnarray}


\subsection{Reduction to Hermitian case}
The computation of the pseudoinverse is reducible to its construction in the Hermitian case. This is possible through the equivalences:
\begin{itemize}
    \item $A^+ = (A^*A)^+A^*$
    \item $A^+ = A^*(AA^*)^+$
\end{itemize}
as $A^*A$ and $AA^*$ are obviously Hermitian.

\subsection{Products}

If $A \in M(m,n;K)$, $B \in M(n,p;K)$ and either, 
\begin{itemize}
    \item $A$ has orthonormal columns (i.e. $A^*A = I_n$) or, 
    \item $B$ has orthonormal rows (i.e. $BB^* = I_n$) or,
    \item $A$ has all columns linearly independent (full column rank) and $B$ has all rows linearly independent (full row rank) or,
    \item $B = A^*$ (i.e. $B$ is the conjugate transpose of $A$),
\end{itemize}
then $(AB)^+ = B^+A^+$.

The last property yields the equivalences:

\begin{eqnarray}
    \begin{gathered}
        (AA^*)^+ = A^{+*}A^+  \notag\\
        (A^*A)^+ = A^+ A^{+*}
    \end{gathered}
    \label{eq0.2.6}
\end{eqnarray}

\section{Basic Algorithms}

The algorithms are divided into three categories: \emph{\textcolor[rgb]{1,0,0}{optimization methods, greedy methods, and thresholding-based methods}}. 

\subsection{Optimization Methods}

An \emph{\textcolor[rgb]{1,0,0}{optimization problem}} is a problem of the type

\[
    \mathop{\text{minimize}}\limits_{x \in \mathbb{R}^N} F_0(x) \quad \text{ subject to } F_i(x) \leq b_i,\ i \in [n],
\]
where the function $F_0 : \mathbb{R}^N \rightarrow \mathbb{R}$ is called an \emph{\textcolor[rgb]{1,0,0}{objective function}} and the functions $F_1,\dots,F_n : \mathbb{R}^N \rightarrow \mathbb{R}$ are called \emph{\textcolor[rgb]{1,0,0}{constrained functions}}. This general framework also encompasses equality constraints of the type $G_i(x) = c_i$. Since the equality $G_i(x) = c_i$ is equivalent to the inequalities $G_i(x) \leq c_i$ and $-G_i(x) \leq -c_i$. If $F_0, F_1, \dots, F_n$ are all convex functions, then the problem is called a \emph{\textcolor[rgb]{1,0,0}{convex optimization problem}}. If $F_0, F_1, \dots, F_n$ are all linear functions, then the problem is called a \emph{\textcolor[rgb]{1,0,0}{linear program}}. The sparse recovery problem is in fact an optimization problem, since it translates into
\begin{equation}
    \mathop{\mathrm{minimize}}\|z\|_0 \quad \text{subject to } \mathbf{A}z = y. \tag{$P_0$}
    \label{eq0.p0}
\end{equation}
This is a nonconvex problem and NP-hard in general. However, keeping in mind that $\|z\|_q^q$ opproaches $\|z\|_0$ as $q>0$ tends to zero, we can approximate \cref{eq0.p0} by the problem
\begin{equation}
    \mathop{\mathrm{minimize}}\|z\|_q \quad \text{subject to } \mathbf{A}z = y. \tag{$P_q$}
    \label{eq0.pq}
\end{equation}

and \emph{\textcolor[rgb]{1,0,0}{basis pursuit or $\ell_1$-minimization}}:
\begin{equation}
    \mathop{\mathrm{minimize}} \|z\|_1 \quad \text{subject to } \mathbf{A}z = y. \tag{$P_1$}
    \label{eq0.p1}
\end{equation}

\begin{mdframed}
    \label{bp}
    \begin{center}
        \textbf{\textcolor[rgb]{1,0,0}{Basis pursuit}}
    \end{center}
    \emph{Input:} measurement matrix $\mathbf{A}$, measurement vector $y$.

    \emph{Instruction:} 
    \begin{equation}
        x^{\sharp} = \mathop{\mathrm{argmin}} \|z\|_1 \quad \text{subject to } \mathbf{A}z = y. \tag{BP}
        \label{eqbp}
    \end{equation}

    \emph{Output:} the vector $x^{\sharp}$.
\end{mdframed}

\begin{theorem}
    \label{th0.3.1}
    Let $\mathbf{A} \in \mathbb{R}^{m \times N}$ be a measurement matrix with columns $a_1, \dots, a_N$. Assuming the uniqueness of a minimizer $x^{\sharp}$ of 
    \[
        \mathop{\mathrm{minimize}}\limits_{z \in \mathbb{R}^N} \|z\|_1 \quad \text{subject to } \mathbf{A}z = y,
    \]
    the system $\{a_j, j\in \mathop{\mathrm{supp}}(x^{\sharp})\}$ is linearly independent, and in particular 
    \[
        \|x^{\sharp}\|_0 = \mathop{\mathrm{card}}(\mathop{\mathrm{supp}}(x^{\sharp})) \leq m.
    \]
\end{theorem}

A general $\ell_1$-minimization taking measurement error into account:
\begin{equation}
    \mathop{\mathrm{minimize}}\|z\|_1 \quad \text{subject to } \|\mathbf{A}z-y\|_2 \leq \eta \tag{$P_{1,\eta}$}
    \label{eq0.p1n}
\end{equation}
Given a vector $z \in \mathbb{C}^N$, we introduce its real and imaginary parts $\mathbf{u,v} \in \mathbb{R}^N$ and a vector $\mathbf{c} \in \mathbb{R}^N$ such that $c_j \geq \left|z_j\right| = \sqrt{u_j^2 + v_j^2}$ for all $j \in [N]$. The problem \cref{eq0.p1n} is then equivalent to the following problem with optimization variables $\mathbf{c,u,v} \in \mathbb{R}^N$:
\begin{equation}
    \mathop{\mathrm{minimize}}\limits_{\mathbf{c,u,v} \in \mathbb{R}^N} \sum\limits_{j=1}^N c_j \quad \text{subject to } \left\|\left[ 
        \begin{array}{cc}
            Re(\mathbf{A}) & -Im(\mathbf{A}) \\
            Im(\mathbf{A}) & Re(\mathbf{A})
        \end{array}
        \right]
        \left[ 
         \begin{array}{c}
            \mathbf{u}\\
            \mathbf{v}
        \end{array}
    \right]\right\|_2 \leq \eta, \tag{$P_{1,\eta}'$}
    \label{eq0.p1n'}
\end{equation}
\[
    \begin{array}{c}
        \sqrt{u_1^2 + v_1^2} \leq c_1, \\
        \vdots \\
        \sqrt{u_N^2 + v_N^2} \leq c_N.
    \end{array}
\]
This is an instance of a \emph{\textcolor[rgb]{1,0,0}{second-order cone problem}};

\begin{mdframed}
    \label{qbp}
    \begin{center}
        \textbf{\textcolor[rgb]{1,0,0}{Quadratically constrained basis pursuit}}
    \end{center}
    \emph{Input:} measurement matrix $\mathbf{A}$, measurement vector $y$, noise level $\eta$.

    \emph{Instruction:}
    \begin{equation}
        x^{\sharp} = \mathop{\mathrm{argmin}} \|z\|_1 \qquad \text{subject to } \|\mathbf{A}z - y\|_2 \leq \eta. \tag{$BP_{\eta}$}
        \label{eqbpn}
    \end{equation}

    \emph{Output:} the vector  $x^{\sharp}$.
\end{mdframed}
    The solution $x^{\sharp}$ of 
    \begin{equation}
        \mathop{\mathrm{minimize}}\limits_{z \in \mathbb{C}^{N}}\|z\|_1 \qquad \text{subject to }\|\mathbf{A}z - y\|_2 \leq \eta
        \label{eq0.3.1}
    \end{equation}
    for some parameter $\lambda \geq 0$,
    \begin{equation}
        \mathop{\mathrm{minimize}}\limits_{z \in \mathbb{C}^N} \lambda\|z\|_1 + \|\mathbf{A}z - y\|_2^2.
        \label{eq0.3.2}
    \end{equation}
    The solution of \cref{eq0.3.1} is also related to the output of the \emph{LASSO}, which consists in solving, for some parameter $\tau \geq 0$,
    \begin{equation}
        \mathop{\mathrm{minimize}}\limits_{z \in \mathbb{C}^N} \|\mathbf{A}z - y\|_2 \qquad \text{subject to }\|z\|_1 \leq \tau.
        \label{eq0.3.3}
    \end{equation}
    Precisely, some links between the three approaches are given below.


\begin{proposition}
    \label{pr0.3.2}
    \begin{enumerate}[(a)]
        \item If $x$ is a minimizer of the basis pursuit denoising \cref{eq0.3.2} with $\lambda > 0$, then there exists $\eta = \eta_x \geq 0$ such that $x$ is a minimizer of ther quadratically constrained basis pursuit \cref{eq0.3.1}.
        \item If $x$ is unique minimizer of the quadratically constrained basis pursuit \cref{eq0.3.1} with $\eta \geq 0$, then there exists $\tau = \tau_x \geq 0$ such that $x$ is a unique minimizer of the LASSO \cref{eq0.3.3}.
        \item If $x$ is a minimizer of the LASSO \cref{eq0.3.3} with $\tau \geq 0$, then there exists $\lambda = \lambda_x \geq 0$ such that $x$ is a minimizer of the basis pursuit denoising \cref{eq0.3.2}.
    \end{enumerate}
\end{proposition}

Another type of $\ell_1$-minimization problem is the \emph{\textcolor[rgb]{1,0,0}{Dantzig selector}},
\begin{equation}
    \mathop{\mathrm{minimize}}\limits_{z \in \mathbb{C}^N} \|z\|_1 \qquad \text{subject to } \|\mathbf{A}^*(\mathbf{A}z - y)\|_{\infty} \leq \tau.
    \label{eq0.3.4}
\end{equation}
This is again a convex optimization problem. The intuition for the constraints is that the residual $r = \mathbf{A}z -y $ should have small correlation with all columns $a_j$ of the matrix $\mathbf{A}$---indeed, $\|\mathbf{A}^*(\mathbf{A}z -y)\|_{\infty} = \max_{j \in [N]} \left|\left<r, a_j\right>\right|$.  

\subsection{Greedy Methods}

\begin{mdframed}
    \label{OMP}
    \begin{center}
        \textbf{Orthogonal matching pursuit (OMP)}
    \end{center}
    \emph{Input:} measurement matrix $\mathbf{A}$, measurement vector $y$.

    \emph{Initialization: } $S^0 = \cancel{0}, x^0 = 0$. 
    
    \emph{Iteration:} repeat until a stopping criterion is met at $n = \bar{n}$:
    \begin{equation}
        S^{n+1} = S^n \cup \{j_{n+1}\}, j_{n+1} := \mathop{\mathrm{argmax}}\limits_{j \in [N]} \left\{ \left|(\mathbf{A}^*(y-\mathbf{A}x^n))_j\right| \right\}, \tag{$OMP_1$}
        \label{eqOMP1}
    \end{equation}
    \begin{equation}
        x^{n+1} = \mathop{\mathrm{argmin}}\limits_{z \in \mathbb{C}^N} \left\{ \|y - \mathbf{A}z\|_2,\mathop{\mathrm{supp}}(z) \subset S^{n+1} \right\}. \tag{$OMP_2$}
        \label{OMP2}
    \end{equation}

    \emph{Output:} the $\bar{n}$-sparse vector $x^{\sharp} = x^{\bar{n}}$.
\end{mdframed}

The projection step \cref{OMP2} is the most costly part of the orthogonal matching pursuit algorithm, which can be accelerated by using the $QR$-decomposition of $\mathbf{A}_{S_n}$. 

The choice of the index $j_{n+1}$ is dictated by a greedy strategy where one aims to reduce the $\ell_2$-norm of the residual $y - \mathbf{A}x^n$ as much as possible at each iteration. 

\begin{lemma}
    \label{lm0.3.3}
    Let $\mathbf{A} \in \mathbb{C}^{m \times N}$ be a matrix with $\ell_2$-normalized columns. Given $S \subset [N]$, $v$ supported on $S$, and $j \in [N]$, if 
    \[
        w := \mathop{\mathrm{argmin}}\limits_{z \in \mathbb{C}^N} \left\{ \|y-\mathbf{A}z\|_2. \mathop{\mathrm{supp}}(z) \subset S \cup \{j\} \right\},
    \]
    then
    \[
        \|y - \mathbf{A}w\|_2^2 \leq \|y-\mathbf{A}v\|_2^2- \left|(\mathbf{A}^*(y - \mathbf{A}v))_j\right|^2.
    \]
\end{lemma}

\begin{proof}
    Since any vector of the form $v + te_j$ with $t \in \mathbb{C}$ is supported on $S \cup \{j\}$, we have
    \[
        \|y - \mathbf{A}w\|_2^2 \leq \min\limits_{t \in \mathbb{C}} \|y - \mathbf{A}(v+te_j)\|_2^2.
    \]
    Writing $t = \rho e^{i\theta}$ with $\rho \geq 0$ and $\theta \in [0,2\pi)$, we compute
        \begin{eqnarray*}
            \|y - \mathbf{A}(v+te_j)\|_2^2 &=& \|y- \mathbf{A}v - t\mathbf{A}e_j\|_2^2 \\
            &=& \|y - \mathbf{A}v\|_2^2 + \left|t\right|^2 \|\mathbf{A}e_j\|_2^2 - 2 \mathop{\mathrm{Re}}\left( \bar{t}\left<y - \mathbf{A}v, \mathbf{A}e_j\right> \right) \\
            &=& \|y - \mathbf{A}v\|_2^2 + \rho^2 - 2\mathop{\mathrm{Re}}\left( \rho e^{-i\theta}(\mathbf{A}^*(y-\mathbf{A}v))_j \right) \\
            &\geq& \|y - \mathbf{A}v\|_2^2 + \rho^2 - 2\rho\left|(\mathbf{A}^*(y-\mathbf{A}v))_j\right|,
        \end{eqnarray*}
        with equality for a property chosen $\theta$. As a quadratic polynomial in $\rho$, the latter expression is minimized when $\rho = \left|(\mathbf{A}^* (y - \mathbf{A}u))_j\right|$. This shows that
        \[
            \min\limits_{t \in \mathbb{C}} \|y-\mathbf{A}(v + t e_j)\|_2^2 = \| y - \mathbf{A} v\|_2^2 - \left|(\mathbf{A}^*(y - \mathbf{A}u))_j\right|^2,
        \]
        which concludes the proof.
\end{proof}

The step \cref{OMP2} also reads as 
\[
    x_{S^{n+1}}^{n+1} = \mathbf{A}_{S^{n+1}}^{\dagger}y,
\]
where $x_{S^{n+1}}^{n+1}$ denotes the restriction of $x^{n+1}$ to its support set $S^{n+1}$ and where $\mathbf{A}_{S^{n+1}}^{\dagger}$ is the pseudo-inverse of $\mathbf{A}_{S^{n+1}}$. This simply says that $z = x_{S^{n+1}}^{n+1}$ is a solution of $\mathbf{A}_{S^{n+1}}^{n+1}\mathbf{A}_{S^{n+1}}z = \mathbf{A}_{S^{n+1}}^*y$. 

\begin{lemma}
    \label{lm0.3.1}
    Given an index set $S \subset [N]$, if 
    \[
        v := \mathop{\mathrm{argmin}}\limits_{z \in \mathbb{C}^N} \left\{ \|y - \mathbf{A}z\|_2, \mathop{\mathrm{supp}}(z) \subset S \right\},
    \]
    then 
    \begin{equation}
        (\mathbf{A}^*(y - \mathbf{A}v))_S = 0.
        \label{eq0.3.5}
    \end{equation}
\end{lemma}

\begin{proposition}
    \label{pr0.3.5}
    Given a matrix $\mathbf{A} \in \mathbb{C}^{m \times N}$, every nonzero vector $x \in \mathbb{C}^N$ supported on a set $S$ of size $s$ is recovered from $y = \mathbf{A}x$ after at most $s$ iterations of orthogonal matching pursuit if and only if the matrix $\mathbf{A}_S$ is injective and 
    \begin{equation}
        \max\limits_{j \in S}\left|(\mathbf{A}^*r)_j\right| > \max\limits_{\ell \in \overline{S}} \left|(\mathbf{A}^*r)_{\ell}\right|
        \label{eq0.3.6}
    \end{equation}
    for all nonzero $r \in \left\{ \mathbf{A}z, \mathop{\mathrm{supp}}(z) \subset S \right\}$.
\end{proposition}

\begin{remark}
    \label{rmk0.3.6}
    A more concise way to formulate the necessary and sufficient conditions of \cref{pr0.3.5} is the \textcolor[rgb]{1,0,0}{exact recovery condition}, which reads
    \begin{equation}
        \|\mathbf{A}_S^{\dagger}\mathbf{A}_{\overline{S}}\|_{1 \rightarrow 1} < 1;
        \label{eq0.3.7}
    \end{equation}
    see A.1 for the definition of matrix norms. Implicitly, the existence of the pseudo-inverse $\mathbf{A}_S^{\dagger} = (\mathbf{A}_S^*\mathbf{A}_S)^{-1} \mathbf{A}_S^*$ is equivalent to the injectivity of $\mathbf{A}_S$. 
\end{remark}

\begin{mdframed}
    \label{CoSaMP}
    \begin{center}
        \textbf{\textcolor[rgb]{1,0,0}{Compressive sampling matching pursuit (CoSaMP)}}
    \end{center}
        \emph{Input:} measurement matrix $\mathbf{A}$, measurement vector $y$, sparsity level $s$.

        \emph{Initialization:} $s$-sparse vector $x^0$, typically $x^0 = 0$.

        \emph{Iteration:} repeat until a stopping criterion is met at $n = \bar{n}$:
        \begin{equation}
            U^{n+1} = \mathop{\mathrm{supp}}(x^n) \cup L_{2s}(\mathbf{A}^*(y - \mathbf{A}x^n)), \tag{CoSaMP$_1$} 
            \label{eqcosamp1}
        \end{equation}
        \begin{equation}
            u^{n+1} = \mathop{\mathrm{argmin}}\limits_{z \in \mathbb{C}^N} \left\{ \|y - \mathbf{A}z\|_{2}, \mathop{\mathrm{supp}}(z) \subset U^{n+1} \right\}, \tag{CoSaMP$_2$}
            \label{eqcosamp2}
        \end{equation}
        \begin{equation}
            x^{n+1} = H_s(u^{n+1}). \tag{CoSaMP$_3$}
            \label{eqcosamp3}
        \end{equation}
        
        \emph{Output:} the $s$-sparse vector $x^{\sharp} = x^{\bar{n}}$.
\end{mdframed}

\subsection{Thresholding-Based Methods}

\begin{mdframed}
    \label{basicthresholding}
\begin{center}
    \textbf{\textcolor[rgb]{1,0,0}{Basic thresholding}}
\end{center}
    \emph{Input:} measurement matrix $\mathbf{A}$, measurement vector $y$, sparsity level $s$.

    \emph{Instruction:} 
    \begin{equation}
        S^{\sharp} = L_s(\mathbf{A}^*y), \tag{BT$_1$}
        \label{eqbt1}
    \end{equation}
    \begin{equation}
        x^{\sharp} = \mathop{\mathrm{argmin}}\limits_{z \in \mathbb{C}^N} \left\{ \|y = \mathbf{A}z\|_2, \mathop{\mathrm{supp}}(z) \subset S^{\sharp} \right\}. \tag{BT$_2$}
        \label{eqbt2}
    \end{equation}

    \emph{Output:} the $s$-sparse vector $x^{\sharp}$.
\end{mdframed}

\begin{proposition}
    \label{pr0.3.7}
    A vector $x \in \mathbb{C}^{N}$ supported on a set $S$ is recovered from $y = \mathbf{A}x$ via basic thresholding if and only if 
    \begin{equation}
        \min\limits_{j \in S}\left|(\mathbf{A}^* y)_j\right| > \max\limits_{\ell \in \overline{S}} \left|(\mathbf{A}^* y )_{\ell}\right|.
        \label{eq0.3.10}
    \end{equation}
\end{proposition}
\begin{proof}
    It is clear that the vector $x$ is recovered if and only if the index set $S^{\sharp}$ defined in \cref{eqbt1} coincides with the set $S$, that is to say, if and only if any entry of $\mathbf{A}^*y$ on $S$ is greater than any entry of $\mathbf{A}^*y$ on $\overline{S}$. This is property \cref{eq0.3.10}.
\end{proof}

The more elaborate \emph{\textcolor[rgb]{1,0,0}{iterative hard thresholding algorithm}} is an iterative algorithm to solve the rectangular system $\mathbf{A}z = y$, knowing that the solution is $s$-sparse. We shall solve the square system $\mathbf{A}^*\mathbf{A}z = \mathbf{A}^*y$ instead, which can be interpreted as the fixed-point equation $z = (\mathbf{Id} - \mathbf{A}^*\mathbf{A})z + \mathbf{A}^*y$. Classical iterative methods suggest the \emph{\textcolor[rgb]{1,0,0}{fixed-point iteration}} $x^{n+1} = (\mathbf{Id} - \mathbf{A}^*\mathbf{A})x^n + \mathbf{A}^*y$. Since we target $s$-sparse vectors, we only keep the \emph{\textcolor[rgb]{1,0,0}{$s$ largest absolute entries of $(\mathbf{Id} - \mathbf{A}^*\mathbf{A})x^n + \mathbf{A}^*y = x^n + \mathbf{A}^*(y - \mathbf{A}x^n)$ at each iteration}}. The resulting algorithm:

\begin{mdframed}
    \label{IHT}
    \begin{center}
        \textbf{\textcolor[rgb]{1,0,0}{Iterative hard thresholding (HIT)}}
    \end{center}
    \emph{Input:} measurement matrix $\mathbf{A}$, measurement vector $y$, sparsity level $s$.

    \emph{Initialization:} $s$-sparse vector $x^0$, typically $x^0 = 0$.

    \emph{Iteration:} repeat until a stopping criterion is met at $n = \bar{n}$:
    \begin{equation}
        x^{n+1} = H_s(x^n + \mathbf{A}^*(y - \mathbf{A}x^n)). \tag{IHT}
        \label{eqiht}
    \end{equation}
    
    \emph{Output:} the $s$-sparse vector $x^{\sharp} = x ^{\bar{n}}$.
\end{mdframed}

The above algorithm does not require computation of any orthogonal projection. If the orthogonal projection needs to be paid attention, like in the greedy methods, \emph{\textcolor[rgb]{1,0,0}{it makes sense to look at the vector with the same support as $x^{n+1}$ that best fits the measurements}}. This leads to the \emph{\textcolor[rgb]{1,0,0}{hard thresholding pursuit algorithm}}:
\begin{mdframed}
    \label{HTP}
    \begin{center}
        \textbf{\textcolor[rgb]{1,0,0}{Hard thresholding pursuit (HTP)}}
    \end{center}
    \emph{Input:} measurement matrix $\mathbf{A}$, measurement vector $y$, sparsity level $s$.

    \emph{Initialization:} $s$-sparse vector $x^0$, typically $x^0 = 0$.

    \emph{Iteration:} repeat until stopping criterion is met an $n = \bar{n}$:
    \begin{equation}
        S^{n+1} = L_s(x^n + \mathbf{A}^*(y = \mathbf{A}x^n)), \tag{HTP$_1$}
        \label{eqhtp1}
    \end{equation}
    \begin{equation}
        x^{n+1} = \mathop{\mathrm{argmin}}\limits_{z \in \mathbb{C}^N} \left\{ \|y - \mathbf{A}z\|_2, \mathop{\mathrm{supp}}(z) \subset S^{n+1} \right\}. \tag{HTP$_2$}
        \label{eqhtp2}
    \end{equation}

    \emph{Output:} the $s$-sparse vector $x^{\sharp} = x^{\bar{n}}$.
\end{mdframed}

\begin{mdframed}
    \label{subspacepursuit}
    \begin{center}
        \textbf{\textcolor[rgb]{1,0,0}{Subspace Pursuit}}
    \end{center}

    \emph{Input:} measurement matrix $\mathbf{A}$, measurement vector $y$, sparsity level $s$.

    \emph{Initialization:} $s$-sparse vector $x^0$, typically $x^0 = 0, S^0 = \mathop{\mathrm{supp}}(x^0)$.

    \emph{Iteration:} repeat until a stopping criterion is met at $n = \bar{n}$:
    \begin{equation}
        U^{n+1} = S^n \cup L_s(\mathbf{A}^*(y-\mathbf{A}x^n)), \tag{SP$_1$}
        \label{eqsp1}
    \end{equation}
    \begin{equation}
        u^{n+1} = \mathop{\mathrm{argmin}}\limits_{z \in \mathbb{C}^N} \left\{ \|y-\mathbf{A}z\|_2, \mathop{\mathrm{supp}}(z) \subset U^{n+1} \right\}, \tag{SP$_2$}
        \label{eqsp2}
    \end{equation}
    \begin{equation}
        S^{n+1} = L_{s} (u^{n+1}), \tag{SP$_3$}
        \label{eqsp3}
    \end{equation}
    \begin{equation}
        x^{n+1} = \mathop{\mathrm{argmin}}\limits_{z \in \mathbb{C}^N} \left\{ \|y - \mathbf{A}z\|_2, \mathop{\mathrm{supp}}(z) \subset S^{n+1} \right\}. \tag{SP$_4$}.
        \label{eqsp4}
    \end{equation}

    \emph{Output:} the $s$-sparse vector $x^{\sharp} = x^{\bar{n}}$.
\end{mdframed}

\subsection{Notes}

\subsubsection{Which Algorithm Should One Choose?}
\begin{itemize}
    \item The minimal number $m$ of measurements for a sparsity $s$ and a signal length $N$ may vary with each algorithm. Comparing the recovery rates and identifying the best algorithm is a matter of numerical tests. 
        For this criterion, the recovery performance of \emph{basic thresholding} is significantly worse than the one of other algorithms, although it is the fastest algorithm since it identifies the support in only one step.
    \item The speed of the algorithm is also a criterion, and it is also a matter of numerical tests. If the sparsity $s$ is quite small, then \emph{orthogonal matching pursuit} is extremely fast because the speed essentially depends on \emph{the number of iterations, which typically equals $s$ when the algorithm succeeds}. \emph{Compressive sampling matching pursuit and hard thresholding pursuit} are fast for small $s$, because each step involves the computation of an orthogonal projection relative to $\mathbf{A}_S$ with small $S \subset [N]$. But if the sparsity $s$ is not that small compared to $N$, then orthogonal matching pursuit may nonetheless require a significant time. The same applies to the homotopy method which builds the support of an $\ell_1$-minimizer iteratively. The runtime of iterative hard thresholding is almost not influenced by the sparsity $s$ at all.

        Basis pursuit \cref{bp}, per se, is not an algorithm, so the runtime depends on the actual algorithm that is used for the minimization. For \emph{\textcolor[rgb]{1,0,0}{Chambolle and Pock's primal dual algorithm}}, which constructs a sequence converging to an $\ell_1$-minimizer, the sparsity $s$ has no serious influence on the speed. Hence, for \textcolor[rgb]{1,0,0}{mildly large $s$}, it can be significantly faster than orthogonal matching pursuit. A point of view of regarding the orthogonal matching pursuit are always faster than $\ell_1$-minimization --- is only true for small sparsity. \textcolor[rgb]{1,0,0}{The iteratively reweighted least squares method} may also be a good alternative for mildly large sparsity.

    \item Another Important feature of an algorithm is the \emph{\textcolor[rgb]{1,0,0}{possibility to exploit fast matrix-vector multiplication routines}} that are available for $\mathbf{A}$ and $\mathbf{A}^*$. In principle, any of the proposed methods can be sped up in this case, but the task is complicated if \emph{orthogonal projection} steps are involved. Fast matrix-vector multiplications are easily integrated in the \emph{iterative hard thresholding algorithm and in Chambolle and Pock's primal dual algorithm for $\ell_1$-minimization}. The acceleration achieved in this context depends on the algorithm, and the fastest algorithm should again be determined by numerical tests in the precise situation.
\end{itemize}

\section{Basis Pursuit}

\subsection{Null Space Property}

The null space property is a necessary and sufficient condition for exact recovery of sparse vectors via basis pursuit. 

\begin{definition}
    \label{def0.4.1}
    A matrix $\mathbf{A} \in \mathbb{K}^{m \times N}$ is said to satisfy the \textcolor[rgb]{1,0,0}{null space property} relative to a set $S \subset [N]$ if 
    \begin{equation}
        \|v_S\|_1 < \|v_{\bar{S}}\|_1 \quad \text{for all } v \in \mathop{\mathrm{ker}}\mathbf{A} \backslash \{0\}.
        \label{eq0.4.1}
    \end{equation}
\end{definition}

It is to satisfy the null space property of order $s$ if it satisfies the null space property relative to any set $S \subset [N]$ with card$(S) \leq s$.

\begin{remark}
    \label{rmk0.4.2}
    It is important to observe that, for a given $v \in \mathop{\mathrm{ker}}\mathbf{A} \backslash \{0\}$, the condition $\|v_S\|_1 < \|v_{\bar{S}}\|_1$ holds for any set $S \subset [N]$ with card$(S) \leq s$ as soon as it holds for an index set of $s$ largest (in modulus) entries of $v$.
\end{remark}

\begin{mdframed}
    \begin{definition}
        \label{def2.2ofbook}
        For $p>0$, the $\ell_p$-error of best $s$-term approximation to a vector $x \in \mathbb{C}^{N}$ is defined by
        \[
            \sigma_s(x)_p := \inf \left\{ \|x-z\|_p, z \in \mathbb{C}^N \text{ is $s$-sparse} \right\}.
        \]
    \end{definition}
\end{mdframed}


\begin{remark}
    \label{rmk0.4.3}
    There are two convenient reformulations of the null space property. The first one is obtained by adding $\|v_S\|_1$ to both sides of the inequality $\|v_S\|_1 < \|v_{\bar{S}}\|_1$. Thus, the null space property relative to $S$ reads 
    \begin{equation}
        2\|v_S\|_1 < \|v\|_1 \quad \text{for all } v \in \mathop{\mathrm{ker}} \mathbf{A} \backslash \{0\}.
        \label{eq0.4.2}
    \end{equation}
    The second one is obtained by choosing $S$ as an index set of $s$ largest (in modulus) entries of $v$ and this time by adding $\|v_{\bar{S}}\|_1$ to both sides of the inequality. Thus, the null space property of order $s$ reads
    \begin{equation}
        \|v\|_1 < 2 \sigma_s(v)_1 \quad \text{for all } v \in \mathop{\mathrm{ker}}\mathbf{A} \backslash \{0\},
        \label{eq0.4.3}
    \end{equation}
    where we recall from \cref{def2.2ofbook} that, for $p>0$, the $\ell_p$-error of \textcolor[rgb]{1,0,0}{\emph{best $s$-term approximation}} to $x \in \mathbb{K}^N$ is defined by 
    \[
        \sigma_s(x)_p = \inf\limits_{\|z\|_0 \leq s} \|x-z\|_p.
    \]
\end{remark}

\begin{theorem}
    \label{th0.4.4}
    Given a matrix $\mathbf{A} \in \mathbb{K}^{m \times N}$, every vector $x \in \mathbb{K}^N$ supported on a set $S$ is the unique solution of \cref{eq0.p1} with $y = \mathbf{A}x$ if and only if $\mathbf{A}$ satisfies the null space property relative to $S$.
\end{theorem}

\begin{remark}
    \label{rmk0.4.6}
    \begin{enumerate}[(a)]
        \item This theorem \cref{th4.5frombook} shows that for every $y = \mathbf{A}x$ with $s$-sparse vector $x$ the $\ell_1$-minimization strategy \cref{eq0.p1} actually solves the $\ell_0$-minimization problem \cref{eq0.p0} when the null space property of order $s$ holds. Indeed, assume that every $s$-sparse vector $x$ is recovered via $\ell_1$-minimization from $y = \mathbf{A}x$. Let $z$ be the minimizer of the $\ell_0$-minimization problem \cref{eq0.p0} with $y = \mathbf{A}x$ then $\|z\|_0 \leq \|x\|_0$ so that also $z$ is $s$-sparse. But since every $s$-sparse vector is the unique $\ell_1$-minimizer, it follows that $x=z$.
        \item It is desirable for any reconstruction scheme to preserve sparse recovery if some measurements are rescaled, reshuffled, or added. Basis pursuit actually features such properties. Indeed, mathematically speaking, these operations consist in replacing the original measurement matrix $\mathbf{A}$ by new measurement matrices $\hat{\mathbf{A}}$ and $\hat{\mathbf{A}}$ defined by 
            \[
                \hat{\mathbf{A}} := \mathbf{GA}, \quad \text{where $\mathbf{G}$ is some invertible $m\times m$ matrix},
            \]
            \[
                \hat{\mathbf{A}} := \left[ \frac{\mathbf{A}}{\mathbf{B}} \right], \quad \text{where $\mathbf{B}$ is some $m' \times N$ matrix}.
            \]
            One can observe that ker$\hat{\mathbf{A}} = $ ker$\mathbf{A}$ and ker$\hat{\mathbf{A}} \subset $ ker$\mathbf{A}$, hence the null space property for the matrices $\hat{\mathbf{A}}$ and $\hat{\mathbf{A}}$ remains fulfilled if it is satisfied for the matrix $\mathbf{A}$. It is not true that the null space property remains valid if we multiply on the right by an invertible matrix.
    \end{enumerate}
\end{remark}

The \emph{\textcolor[rgb]{1,0,0}{real null space relative to a set $S$}}:
\begin{equation}
    \sum\limits_{j \in S} \left|v_j\right| < \sum\limits_{\ell \in \bar{S}} \left|v_{\ell}\right| \quad \text{for all } v \in \text{ker}_{\mathbb{R}} \mathbf{A}, v \neq 0,
    \label{eq0.4.4}
\end{equation}
and, on the other hand, to the \emph{\textcolor[rgb]{1,0,0}{complex null space property relative to $S$}}:
\begin{equation}
    \sum\limits_{j \in S} \sqrt{v_j^2 + w_j^2} < \sum\limits_{\ell \in \bar{S}} \sqrt{v_{\ell}^2 + w_{\ell}^2} \quad \text{for all } v,w \in \text{ker}_{\mathbb{R}} \mathbf{A},(v,w) \neq (0,0).
    \label{eq0.4.5}
\end{equation}
The real and complex version are in fact equivalent. These vectors can be interpreted as real or as complex vectors. The followed theorem explains why we usually work in the complex setting.
\begin{theorem}
    \label{th0.4.7}
    Given a matrix $\mathbf{A} \in \mathbb{R}^{m\times N}$, the real null space property \cref{eq0.4.4} relative to a set $S$ is equivalent to the complex null space property \cref{eq0.4.5} relative to this set $S$.

    In particular, the real null space property of order $s$ is equivalent to the complex null space property of order $s$.
\end{theorem}







