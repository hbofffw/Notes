\chapter{``Compressed Sensing'' Notes}
\label{chapter3}
\section{abstract}
Suppose is an unknown vector in $\mathbf{R}^2$ (a digital image or signal); we plan to measure $n$ general linear functionals of $x$ and then reconstruct. If $x$ is known to be compressible by transform coding with a known transform, and we reconstruct via the nonlinear procedure defined here, the number of measurements $n$ can be dramatically smaller than the size $m$. Thus, certain natural classes of images with $m$ pixels need only $n = O(m^{1/4}\log^{5/2}(m))$ nonadaptive nonpixel samples for faithful recovery, as opposed to the usual $m$ pixel samples.

More specifically, suppose $x$ has a sparse representation in some orthonormal basis (e.g., wavelet, Fourier) or tight frame (e.g., curvelet, Gabor)—so the coefficients belong to an $\ell_p$ ball for $0<p \leq 1$. The $N$ most important coefficients in that expansion allow reconstruction with $\ell_2$ error $O(N^{1/2-1/p})$. It is ossible to design \textcolor[rgb]{1,0,0}{$n=O(N\log(m))$ nonadaptive measurements} allowing reconstruction with accuracy comparable to that attainable with direct knowledge of the $N$ most important coefficients. Moreover, a good approximation to those $N$ important coefficients is extracted from the $n$ measurements by solving a linear program—Basis Pursuit in signal processing. The nonadaptive measurements have the character of “random” linear combinations of basis/frame elements. Our results use the notions of optimal recovery, of $n$-widths, and information-based complexity. We estimate the Gel'fand $n$-widths of $\ell_p$ balls in high-dimensional Euclidean space in the case $0<p \leq 1$, and give a criterion identifying near-optimal subspaces for Gel'fand $n$-widths. We show that “most” subspaces are near-optimal, and show that convex optimization (Basis Pursuit) is a near-optimal way to extract information derived from these near-optimal subspaces.

    假设x是$\mathbf{R}^m$（一个数字图像或者信号）中的一个位置向量；我们计划测量x中的n个线性方程，然后重构它。如果x可被一个已知的变换编码进行压缩，并且可以通过本文定义的非线性过程进行重构，测量数量n可以远小于其实际大小m。这样，相比常用的采样方法，确定的含有m像素的自然图像，仅需要$n=O(m^{1/4}\log^{2/5}(m))$非自适应非像素采样就可以进行可靠恢复。
	
    更明确的，假设x在一些正交基(例如，小波，傅里叶)和紧框架\footnote{tight frame}(curvelet, Garbor)存在一个稀疏表示---所以系数属于一个$\ell_p$球，$0<p\leq 1$。该扩展中，最重要的N个系数在$\ell_2$错误满足$O(N^{1/2-1/p})$时可以重构。

\section{Introduction}
\subsection{Transform Compression Background}
\textcolor[rgb]{1,0,0}{Transform coefficients} $\theta_i = \left<x,\psi_i\right>$, a vector $x$ is the object of interest $x \in \mathbf{R}^m$; ($\psi_i: i=1, \cdots, m$) is an orthonormal basis for $\mathbf{R}^m$. There are assumed sparse in the sense that, for some $0<p<2$ and for some $R>0$
\begin{equation}
    \|\theta\|_p \equiv \left(\sum\limits_i|\theta_i|^p\right)^{1/p} \leq R.
    \label{eq3.1.1}
\end{equation}

The key implication of $\ell_p$ constraint is sparsity of the transform coefficients. Indeed, wehave trivially that, if $\theta_N$ denotes the vector $\theta$ with everything except the $N$ largest coefficients set to $0$
\begin{equation}
    \|\theta - \theta_N \|_2 \leq \zeta_{2,p} \cdot (N+1)^{1/2-1/p}
    \label{eq3.1.2}
\end{equation}
for $N = 0,1,2,\cdots$, with a constant $\zeta_{2,p}$ depending only on $p \in (0,2)$. Thus, for example, to approximate $\theta$ with error $\epsilon$, we need to keep only the $N \asymp \epsilon^{(p-2)/2p}$ biggest terms in $\theta$. 

\subsection{Optimal Recovery/Information-Based Complexity Background}
Designing an information operator $I_n:X \longmapsto \mathbf{R}^n$ which samples $n$ pieces of information about $x$, as well as an algorithm $A_n:\mathbf{R}^n \longmapsto \mathbf{R}^m$ which offers an approximate reconstruction of $x$, are of interest. The \emph{\textcolor[rgb]{1,0,0}{information operator}} takes the form  
\begin{equation*}
    I_n(x) = \left(\left<\xi_1,x\right>,\cdots,\left<\xi_n,x\right>\right)
\end{equation*}
where the $\xi_i$ are \emph{\textcolor[rgb]{1,0,0}{sampling kernels}}, not \emph{\textcolor[rgb]{0,0,1}{necessarily}} sampling pixels or other simple features of the signal; they are nonadaptive, i.e., fixed independently of $x$. The algorithm $A_n$ is an unspecified, possibly nonlinear reconstruction operator. 

The minimax $\ell_2$ error is considered as a standard of 
\begin{equation*}
    E_n(X) = \inf\limits_{A_n,I_n} \sup\limits_{x \in X} \|x-A_n(I_n(x))\|_2.
\end{equation*}
So here, all possible methods of nonadaptive linear sampling are allowed, and all possible methods of reconstruction are allowed. 

In this application, the class $X$ of objects of interest is the set of object $x = \sum_i\theta_i\psi_i$ where $\theta = \theta(x)$ obeys \cref{eq3.1.1} for a given $p$ and $R$. Denote then
\begin{equation*}
    X_{p,m}(R) = \{x:\|\theta(x)\|_p \leq R\} 
\end{equation*}
The goal is to evaluate $E_n(X_{p,m}(R))$ and to have practical schemes which come close to attaining it.


\subsection{Four Surprises}
\begin{theorem}
    Let $(n, m_n)$ be a sequence of problem sizes with $n<m_n, n \rightarrow \infty$, and $m_n \sim An^{\gamma}, \gamma > 1, A>0$. Then for $0<p\leq 1$ there is $C_p = C_p(A,\gamma)>0$ so that
    \begin{equation}
        E_n(X_{p,m}(R)) \leq C_p \cdot R \cdot (n/\log (m_n))^{1/2-1/p}
        \label{eq3.1.3}.
    \end{equation}
    \label{th3.1}
\end{theorem}

\begin{enumerate}
    \item First, compare \cref{eq3.1.2} with \cref{eq3.1.3}. Under the calibration $n = N\log(m_n)$, the forms are similar. In words: the quality of approximation to $x$ which could be gotten by using the $n \approx N \log(m)$ pieces of \emph{\textcolor[rgb]{1,0,0}{nonadaptive}} information provided by $I_n$. The surprise is that we could not know in advance which transform coefficients are likely to be the important ones in this approximation, yet the optimal information operator $I_n$ is nonadaptive, depending at most on the class $X_{p,m}(R)$ and not on the specific object. In some sense this nonadaptive information is just as powerful as knowing the $N$ best transform coefficients.
    \item The information operator is measuring very complex ``holographic'' functionals which in some sense mix together all the coefficients in a big soup. (\emph{\textcolor[rgb]{1,0,0}{Holographic}} is a process where a three-dimensional (3-D) image generates by interferometry \footnote{干扰法} a two-dimensional (2-D) transform. Each value in the 2-D transform domain is influenced by each part of the whole 3-D object. \emph{\textbf{\textcolor[rgb]{1,0,0}{The 3-D object can later be reconstructed by interferometry from all or even a part of the 2-D transform domain. We perceive an analogy here, in which an object is transformed to a compressed domain, and each compressed domain component is a combination of all parts of the original object.}}})
    \item  If the class of information operators is enlarged to allow adaptive ones, e.g., operators in which certain measurements are made in response to earlier measurements, we could scarcely do better. Define the minimax error under adaptive information $E_n^{Adapt}$ allowing adaptive operators 
        \begin{equation*}
            I_n^A = (\left<\xi_1,x\right>,\left<\xi_{2,x},x\right>,\cdots,\left<\xi_{n,x},x\right>)
        \end{equation*}
        where, for $i\geq 2$, each kernel $\xi_{j,x}$ is allowed to depend on the information $\left<\xi_{j,x},x\right>$ gathered at previous stages $1 \leq j < i$. Formally setting
        \begin{equation*}
            E_n^{Adapt}(X) = \inf\limits_{A_n,I_n^A} \sup\limits_{x \in X} \|x-A_n \left(I_n^A(x)\right)\|_2.
        \end{equation*}
        \begin{theorem}
            For $0<p\leq 1$, there is $C_p > 0$ so that for $R > 0$
            \begin{equation*}
                E_n(X_{p,m}(R)) \leq 2^{1/p} \cdot E_n^{Adapt}(X_{p,m}(R)).
            \end{equation*}
            \label{th3.2}
        \end{theorem}
    \item In the already-interesting case $p=1$, \cref{th3.1,th3.2} are easily derivable from known results in OR/IBC and approximation theory! But the derivations are indirect; so although they have what seem to the author as fairly important implications, very little seems known at present about good nonadaptive information operators or about concrete algorithms matched to them. 
\end{enumerate}

The goal in this paper is to give direct arguments which cover the case $0<p \leq 1$ of highly compressible objects, to give direct information about near-optimal information operators and about concrete, computationally tractable algorithms for using this near-optimal information. 

\subsection{Geometry and Widths}
\label{n-widths}
The phenomenon described in \cref{th3.1} concerns the geometry of \textcolor[rgb]{1,0,0}{high-dimensional convex and nonconvex ``balls''}. To see the connection, note that the class $X_{p,n}(R)$ is the image, under orthogonal transformation, of an $\ell_p$ ball. If $p=1$ this is convex and symmetric about the origin, as well as being orthosymmetric\footnote{正对称的} with respect to the axes provided by the wavelet basis; if $p>1$, this is again symmetric about the origin and orthosymmetric, while \textcolor[rgb]{0,0,1}{not being convex, but still star-shped}.

\begin{definition}
    \label{def3.1.1}
    The \textbf{Gel'fand} \emph{n-}\textbf{width} of $X$ with respect to the $\ell_2^m$ norm is defined as 
    \begin{equation*}
        d^n(X;\ell_2) = \inf\limits_{V_n}\sup\left\{\|x\|_2: x \in V_n^{\bot} \cap X\right\}
    \end{equation*}
    where the infimum is over $n-$dimensional linear subspaces of $\mathbf{R}^m$, and $V_n^{\bot}$ denotes the orthocomplement\footnote{正交补，正补余} of $V_n$ with respect to the standard Euclidean inner product.
\end{definition}
