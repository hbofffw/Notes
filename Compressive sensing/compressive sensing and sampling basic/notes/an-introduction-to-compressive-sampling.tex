\chapter{Study on ``An Introduction To Compressive Sampling''\cite{Candes2008}}
CS theory asserts that one can  recover certain signals and images from far fewer samples or measurements than traditional methods use. To make this possible, CS relies on two principles: \emph{sparsity}, which pertains to the signals of interest, and \emph{incoherence}, which pertains to the sensing modality.
\begin{enumerate}
	\item \textbf{\emph{Sparsity}} express the idea that the "information rate" of a continuous time signal may be much smaller than suggested by tis bandwidth, or that a discrete-time signal depends on a number of degrees of freedom which is comparably much smaller than its (finite) length. More precisely, CS exploits the fact that many natural signals are sparse or compressible in the sense that ey have concise representations when expressed in the proper basis $\Psi$
	\item \textbf{\emph{Incoherence}} extends the duality between time and frequency and expresses the idea that objects having a sparse representation in $\Psi$ must be spread out in the domain in which they are acquired, just as a Dirac or a spike in the time domain is spread out in the frequency domain. Put differently, incoherence says that  \textcolor[rgb]{1,0,0}{unlike the signal of interest, the sampling/sensing waveforms have an extremely dense representation in $\Psi$(密集表示？)}
	\begin{quote}
		非相关性拓展了时间与频率之间的二元性。并且表述了一个思想，即对象存在$\Psi$中有一个稀疏表达，该稀疏表达必须延伸至获取它们所在的域，就像时域上一个冲激响应函数(Dirac)或脉冲频域上的分布。非相关性说明了不像对于我们感兴趣的信号，采集/感知波形在$\Psi$中有着极其致密的表达。
	\end{quote}
\end{enumerate}
The \emph{\textbf{crucial observation}} is that one can design efficient sensing or sampling protocols that capture the useful information content embedded in a sparse signal and condense it into a small amount of data. \emph{\textbf{These protocols}} are \textcolor[rgb]{1,0,0}{nonadaptive} and simply require \textcolor[rgb]{1,0,0}{correlating the signal with a small number of fixed waveforms that are incoherent with the sparsifying basis}.

\emph{\textbf{What is the most remarkable about these sampling protocols is}} that they allow a sensor to very efficiently capture the information in a sparse signal \textcolor[rgb]{1,0,0}{without trying to comprehend that signal}.

CS is a very simple and efficient signal acquisition protocol which samples--\textcolor[rgb]{1,0,0}{in a signal independent fashion}--\textcolor[rgb]{1,0,0}{at a low rate and later uses computational power for reconstruction} from what appears to be an incomplete set of measurements.

CS is a concrete protocol for sensing and compressing data simultaneously.

\section{The Sensing Problem}
The sensing mechanisms is discussed in which information about a \textcolor[rgb]{1,0,0}{signal $f(t)$} is obtained by linear functionals recording the values
\begin{equation}
\label{eq1}
y_k=\left\langle f, \varphi_k \right\rangle,\qquad k=1,\cdots,m.
\end{equation}
The object wish be acquired is \textcolor[rgb]{1,0,0}{simply correlated with waveforms $\varphi_k(t)$}. This is a standard setup.
\begin{enumerate}
	\item If the \textcolor[rgb]{1,0,0}{sensing waveforms} are \textbf{Dirac delta functions(spikes,冲激响应)}, then y is a vector of sampled values of $f$ in the time or space domain.
	\item If the sensing waveforms are \textbf{indicator functions}($
		1_A(x)=\begin{cases}
		1 & \mbox{若$x\in A$},\\
		0 & \mbox{若$x\notin A$}.
		\end{cases}
	$) of pixels, then y is the	image data typically collected by sensors in a digital camera.
	\item If the sensing waveforms are sinusoids, then y is a vector of Fourier coefficients;
\end{enumerate}this is the sensing modality used in \emph{magnetic resonance imaging} (MRI).
The \emph{undersampled\footnote{欠采样}} situations, in which the number m of available measurements is much smaller than dimension n of the signal f, are extremely common for a variety of reasons:
\begin{enumerate}
	\item number of sensors may be limited;
	\item the measurements may be extremely expensive as in certain imaging processes via \emph{neutron scattering(中子散射)}.
\end{enumerate}

This state of affairs looks rather daunting, as one would need to solve an \textbf{underdetermined linear system of equations(欠定线性系统方程组)}.
\begin{quote}
	Letting $A$ denote the $m \times n$ sensing matrix with the vectors $\varphi_1^*,\cdots,\varphi_m^*$ as rows (\textcolor[rgb]{1,0,0}{$a_*$} is the \textbf{\textcolor[rgb]{1,0,0}{complex transpose}(复转置, complex conjugate transpose--复共轭转置)} of $a$), the process of recovering $f\in\mathbb{R}^n$ from $y=Af\in\mathbb{R}^m$ is \emph{ill-posed(不适定的)} in general when $m<n$： there are infinitely many candidate signals $\tilde{f}$ for which $A\tilde{f}=y$.
	
	By relying on naturally existed realistic models of objects f, a way out could perhaps be imagined.

        Shannon theory tells us that, \textcolor[rgb]{1,0,0}{if $f(t)$ actually has very low bandwidth, then a small number of (uniform) samples will suffice for recovery}.
\end{quote}

\section{Incoherence and The Sensing of Sparse Signals}
\subsection{Sparsity}
\textbf{\textcolor[rgb]{1,0,0}{Many natural signals have concise representations when expressed in a convenient basis}}. Although nearly all the image pixels have nonzero values, the wavelet coefficients offer a concise summary: \emph{most coefficients are small, and the relatively few large coefficients capture most of the information}.

Mathematically, a vector $f\in\mathbb{R}^n$ is expanded in an \textbf{orthonormal basis (标准正交基 such as a wavelet basis)} $\Psi=\left[\psi_1\psi_2\cdots\psi_n\right]$ as follows:
\begin{equation}
\label{eq2}
f(t)=\sum\limits_{i=1}^{n}x_i\psi_i(t), \qquad t=1,\cdots,N.
\end{equation}
\begin{enumerate}
	\item where x is the \emph{coefficient sequence} of f, $x_i=\left<f,\psi_i\right>$.
	\item It will be convenient to express $f$ as $\Psi x$ (where $\Psi$ is the $n \times n$ matrix with $\psi_1,\cdots,\psi_n$ as columns).
\end{enumerate}

The implication of sparsity:\textbf{when a signal has a sparse expansion, one can discard the small coefficients without much perceptual loss}.

Formally, consider $f_S(t)$ obtained by keeping only the terms corresponding to the $S$ largest values of $(x_i)$ in the expansion \cref{eq2}. By definition, $f_S:=\Psi x_S$, $x_S$ is the vector or coefficients $(x_i)$ with all but teh largest $S$ set to zero. \emph{S-sparse} denotes that objects with at most $S$ nonzero entries.

Since $\Psi$ is an orthonormal basis (or ``orthobasis''), we have $\parallel f-f_S \parallel_{\ell2}=\parallel x-x_S\parallel_{\ell2}$, and if $x$ is sparse or compressible in the sense that the sorted \textbf{magnitudes of the $(x_i)$ decay quickly}, then $x$ is well approximated by $x_S$ and, therefore, \textcolor[rgb]{1,0,0}{the error $\parallel f-f_S \parallel_{\ell2}$ is small}.

A simple method for data compression would be to compute $x$ from $f$ and then (adaptively) encode the locations and values of the $S$ significant coefficients. Such a process requires knowledge of all the $n$ coefficients $x$, as the locations of the significant pieces of information may not be known in advance (they are signal dependent). More generally, sparsity is a fundamental modeling tool which permits efficient fundamental signal processing. Sparsity \emph{has significant bearings on(与……紧密相关)} the acquisition process itself. \textcolor[rgb]{1,0,0}{\textbf{Sparsity determines how efficiently one can acquire signals nonadaptively}}.


\subsection{Incoherent Sampling}
Given a pair $(\Phi,\Psi)$ of orthobases of $\mathbb{R}^n$.
\begin{itemize}
	\item \textbf{$\Phi$ is used for sensing the object $f$ as in \cref{eq1}}--\textcolor[rgb]{1,0,0}{\emph{\textbf{sensing basis}}}.
	\item \textbf{$\Psi$ is used to represent $f$}--\textcolor[rgb]{1,0,0}{\emph{\textbf{representation basis}}}
\end{itemize}
The restriction to pairs of orthobases is \textbf{not essential} and will \textbf{merely simplify our treatment}.
\begin{definition}
	\label{def1.1}
	The coherence between  the sensing basis $\Phi$ and the representation basis $\Psi$ is
	\begin{equation}
	\label{eq1.3}
	\mu(\Phi,\Psi)=\sqrt{n}\cdot \max_{1 \leq k,j \leq n}\left|\left< \varphi_k,\psi_j \right>\right|.
	\end{equation}
\end{definition}

In plain English, the coherence measures the largest correlation between any two elements of $\Phi$ and $\Psi$, see also \cite{Donoho2001}. If $\Phi$ and $\Psi$ contain correlated elements, the coherence is large. Otherwise, it is small. As for how large and how small, it follows from linear algebra that $\mu(\Phi,\Psi)\in\left[ 1,\sqrt{n} \right]$.

Compressive sampling is mainly concerned with \emph{low coherence pairs}, and examples of such pairs are given. In first example, $\Phi$ is the \emph{canonical or spike basis(标准或者冲击响应基) $\varphi(t)=\delta(t-k)$} and \emph{$\Psi$ is the Fourier basis, \textbf{$\Psi_j(t)=n^{-1/2}e^{i2\pi jt/n}$}}. $\Phi$ is the sensing matrix, this corresponds to the classical sampling scheme in time or space. The time-frequency pair obeys $\mu(\Phi,\Psi)=1$, therefore, we have \textcolor[rgb]{1,0,0}{\textbf{\emph{maximal incoherence}}}. Further, spikes and sinusoids are maximally incoherent \emph{not just in one dimension but in any dimension}, (in two dimensions, three dimensions, etc.)

In second example, wavelets bases for $\Psi$ and noiselets\cite{Coifman2001} are taken. The coherence between noiselets and Haar wavelets is $\sqrt{2}$ and that between \emph{Daubechies D4 and D8 wavelets} is, respectively, about 2.2 and 29 across a wide range of sample sizes n. This extends to higher dimensions as well. (Noiselets are also maximally incoherent with spikes and incoherent with the Fourier basis.)

The interest in \textbf{noiselets} comes from the fact that:
\begin{enumerate}
	\item they are incoherent with systems providing sparse representations of image data and other typers of data;
	\item they come with very fast algorithms;
	\item the noiselet transform runs in $O(n)$ time, and just like the Fourier transform, the noiselet matrix does not need to be sotred to be applied to a vector.
\end{enumerate}
This is \textbf{\textcolor[rgb]{1,0,0}{\emph{of crucial practical importance for numerical efficient CS implementations}}}.

Finally, random matrices are largely incoherent with any fixed basis $\Psi$. Select an orthobasis $\Phi$ uniformly at random, which can be done by
\begin{quote}
	\emph{orthonormalizing $n$ vectors sampled independently and uniformly on the unit sphere}.
	\[\Longrightarrow\]
	\emph{With high probability, the coherence between $\varPhi$ and $\varPsi$ is about $\sqrt{2\log n}$}
\end{quote}
By extension, random waveforms $(\varphi_k(t))$ with independent identically distributed (i.i.d.)\label{i.i.d}\footnote{独立同分布(independent identically distributed,IID)} entries, e.g., Gaussian or $\pm1$ binary entries, will also exhibit a very low coherence
with any fixed representation $\Psi$.
\begin{quote}
	Note the rather strange implication here; if sensing with incoherent systems is good, then efficient mechanisms ought to acquire correlations with random waveforms, e.g., white noise!
	应注意一个相当奇怪的涵义，即如果非相关系统的感知效果较好，那么有效的机制应该是获取随机波形的相关性， 例如，白噪声。
\end{quote}

\subsection{Undersampling and Sparse Signal Recovery}
It is only to get to observe a subset of the \emph{n coefficients of f} and collect the data
\begin{equation}
\label{eq4}
y_k=\left<f,\varphi_k\right>, \qquad k \in M,
\end{equation}
where $M \subset \{1,\cdots,n\}$ is a subset of cardinality $m < n$.

So in this paper\cite{Candes2008}, the author decided to recover the signal by $\ell_1-norm$ minimization; the proposed reconstruction $f^*$ is given by $f^*=\Psi x^*$ is the solution to the \emph{convex optimization}\footnote{凸优化} program $(\|x\|_{\ell_1}:=\sum_i\left|x_i\right|)$
\begin{equation}
\label{eq5}
\min_{\tilde{x}\in\mathbb{R}^n}\|\tilde{x}\|_{\ell_1} \qquad \text{subject to} \qquad y_k=\left<\varphi_k,\Psi\tilde{x}\right>, \qquad \forall k \in M.
\end{equation}
That is, among all objects $\tilde{f}=\Psi\tilde{x}$ consist with the data, we pick that whose coefficient sequence has minimal $\ell_1$ norm.\footnote{As is well known, minimizing $\ell_1$ subject to linear equality constraints can easily be recast as a linear program making available a host of ever more efficient solution algorithms.}

A leading early application of $\ell_1$ norm as a \emph{sparsity-promoting}\footnote{稀疏推进？促进？} function was reflection seismology\footnote{地震学}, in which a sparse reflection function(indicating meaningful changes between subsurface layers\footnote{有较大意义的地下层变化指示}) was sought from bandlimited data\cite{Claerbout1973,Santosa1986}. However, $\ell_1$-minimization is not the only way to recover sparse solutions; other methods, \emph{\textcolor[rgb]{1,0,0}{such as greedy algorithms}}\cite{Tropp2007}, have also been proposed.

\begin{theorem}\cite{Candes2007}
	\label{th1.1}
	Fix $f \in \mathbb{R}^n$ and suppose that coefficient sequence $x$ of $f$ in the basis $\Psi$ is \emph{S-sparse}. Select $m$ measurements in the $\Phi$ domain uniformly at random. Then if
	\begin{equation}
	\label{eq1.6}
	m \geqslant C \cdot \mu^2(\Phi,\Psi) \cdot S \cdot \log n
	\end{equation}
	for some positive constant $C$, the solution to \cref{eq5} is exact with overwhelming probability. (It is shown that the probability of success exceeds $1-\delta$ if $m \geqslant C \cdot \mu^2(\Phi,\Psi)\cdot S\cdot \log(n/\delta)$. In addition, the result is only guaranteed for nearly all sign sequences $x$ with a fixed support, see \cite{Candes2007} for details.)
\end{theorem}

Three comments are made:
\begin{enumerate}[1)]
	\item The role of the coherence is completely transparent: \emph{the smaller the coherence, the fewer samples are needed}, so in the previous section the emphasis was on low coherence systems.
	\item One suffers no information loss by measuring just about any set of \emph{m} coefficients which may be far less than the signal size apparently demands. \textcolor[rgb]{1,0,0}{If $\mu(\Phi,\Psi)$(\cref{eq1.3}) is equal or close to one, then on the order of $S \log n$ samples suffice instead of n}.
	\item The signal $f$ can be exactly recovered from our condensed data set by minimizing a \emph{convex functional}\footnote{凸泛函} which does not assume any knowledge about the number of nonzero coordinates of $x$, their locations, or their amplitudes which we assume are all completely unknown a priori. We just run the algorithm if the signal happens to be sufficiently sparse, exact recover occurs.
\end{enumerate}

The theorem indeed suggests a very concrete acquisition protocol: \emph{\textcolor[rgb]{1,0,0}{sample nonadaptively in an incoherent domain (在非相干域进行非自适应采样)}} and \emph{\textcolor[rgb]{1,0,0}{invoke linear programming after acquisition step (采集步骤过后调用线性规划（编程？）)}}. Then the signal in a compressed form would be essentially acquired. All that is needed is a decoder to ``decompress'' this data; \textcolor[rgb]{1,0,0}{\textbf{this is the role of $\ell_1$ minimization}}.

This random incoherence sampling theorem extends an earlier result about the sampling of spectrally sparse signals\cite{Candes2006}, which showed that randomness
\begin{enumerate}[1)]
	\item \emph{can be a very effective sensing mechanism and}
	\item \emph{is amenable to rigorous proof,}
\end{enumerate}and thus perhaps \textbf{\textcolor[rgb]{1,0,0}{triggered the many CS developments}} we have witnessed and continue to witness today. 

Suppose that we are interested in sampling ultra-wideband but spectrally sparse signals of the form $f(t)=\sum_{j=0}^{n-1}x_je^{i2\pi jt/n}，t=0,\cdots,n-1$, where $n$ is very large but where the number of nonzero components $x_j$ is less than or equal to $S$(which should be considered comparably small). Because the active set is not necessarily a subset of consecutive integers, the \emph{Nyquist/Shannon theory is mostly unhelpful} (since one cannot restrict the bandwidth a priori, one may be led to believe that all n time samples are needed).  In this special instance, \cref{th1.1} claims that one can reconstruct a signal \emph{with arbitrary and unknown frequency} support of size $S$ from \emph{on the order of $S \log n$}\footnote{近似$S \log n$} time samples, see \cite{Candes2006}. What is more, these samples do not have to be carefully chosen; almost any sample set of this size will work. For other types of theoretical results in this direction using completely different ideas see \cite{11,12,13}.

The point of the role played by probability is that to get useful and powerful results, one needs to resort to a probabilistic statement since one can not hope for comparable results holding for all measurement sets of size $m$. The reason: there are special sparse signals that vanish nearly everywhere in the $\Phi$ domain.
\begin{quote}
	In other words, one can find sparse signals $f$ and very large subsets of size almost $n$ (e.g., $n-S$) for which $y_k=\left<f,\varphi_k\right>=0$\footnote{稀疏后的采集数据，为0表明该位置稀疏采样值为0} for all $k \in M$.
\end{quote}
\emph{Dirac comb}\footnote{狄拉克(冲激响应)梳状函数} examples are discussed in \cite{14} and \cite{Candes2006}. 
\begin{itemize}
	\item Given such subsets, one would get to see a stream of zeros and no algorithm whatsoever would of course be able reconstruct the signal.
	\item The \cref{th1.1} guarantees that the fraction of sets for which exact recovery does not occur is truly negligible (a large negative power of $n$).
\end{itemize} 
Thus, we only have to tolerate a probability of failure that is extremely small. \textcolor[rgb]{1,0,0}{For practical purposes, the probability of failure is zero provided that the sampling size is sufficiently large.}

Through the study of special sparse signals discussed above, it is shown that one needs at least \textcolor[rgb]{1,0,0}{on the order of $\mu^2 \cdot S \cdot \log n$} samples as well.\footnote{There exist subsets of cardinality $2s$ in the time domain which can reconstruct any s-sparse signal in the frequency domain.} Take $2S$ consecutive time points, see \cref{secwhatiscompressivesampling},  \cite{11,12}. With fewer samples than $S \log n$， no matter how intractable, the probability that information may be lost is just \textcolor[rgb]{1,0,0}{too high and reconstruction by any method}. 

In summary, \emph{\textbf{when the coherence is one, we do not need more than $S \log n$ samples but we cannot do with fewer either}}.

The example in Figure 1(c)\cite{Candes2008}, the considered sparse image has only 25000 nonzero wavelet coefficients, with 96000 incoherent measurements to recovery information, the \cref{eq5} is solved (see \cite{Candes2007} for the particulars of these measurements). 
\begin{quote}
	The minimum-$\ell_1$ recovery is perfect; that is, $f^*=f$.
\end{quote}

There is \emph{de facto\footnote{事实上}} a known four-to-one practical rule which says that for exact recovery, one needs about four incoherent samples per unknown nonzero term.
\section{Robust Compressive Sampling}
In order to powerfully recover sparse signals from just a few measurements, CS needs to be able to deal with both \emph{nearly sparse signals} and with \emph{noise}. 

This section examines two issues simultaneously:
\begin{enumerate}
	\item First, general objects of interest are not exactly sparse but approximately sparse. The issue here is whether or not it is possible to obtain accurate reconstructions of such objects from highly undersampled measurements.
	\item Second, in any real application measured data will invariably be corrupted by at least a small amount of noise as sensing devices do not have infinite precision. 
\end{enumerate}

It will ease the exposition to consider the abstract problem of recovering a vector $x \in \mathbb{R}^n$ from data
\begin{equation}
\label{eq7}
y=Ax+z,
\end{equation}
where $A$ IS an $m \times n$ ``sensing matrix'' giving us information about $x$, and $z$ is a stochastic or deterministic unknown error term. 
\begin{eqnarray*}
 & f = \Psi x  \\
 \& \quad & y = R\Phi f \\
 & \text{(where R is the $m \times n$ matrix extracting the sampled coordinates in M)} \\
 & \Longrightarrow \\
 & y=Ax,\ where\ A=R\Phi\Psi.
\end{eqnarray*}
Hence, one can work with the abstract model \cref{eq7} bearing in mind that x may be the coefficient sequence of the object in a proper basis.

\subsection{Restricted Isometries (\cref{s2.3.2,s5.2.4})}
\label{s1.3.1}
In this section, \textbf{a key notion that has proved to be very useful to study the general robustness of CS} is  introduced; the so-called \emph{\textcolor[rgb]{1,0,0}{\textbf{restricted isometry property (RIP)\label{RIP}}}}\footnote{约束等距性，论文\cite{15}看下}\cite{15}.
\begin{definition}
	\label{def2}
	For each integer $S=1,2,\cdots,$ define the isometry constant $\delta_S$ of a matrix $A$ as the smallest number such that
	\begin{equation}
	\label{eq8}
	(1-\delta_S)\|x\|_{\ell_2}^2 \leq \|Ax\|_{\ell_2}^2 \leq (1+\delta_S)\|x\|_{\ell_2}^2
	\end{equation}
	holds for all S-sparse vectors x.
\end{definition}

It can be loosely said that a matrix $A$ \textcolor[rgb]{1,0,0}{obeys the RIP of order} $S$ if $\delta_S$ is \textcolor[rgb]{1,0,0}{not} too close to one.
\begin{center}
 \Large\textbf{\emph{\textcolor[rgb]{1,0,0}{Note}}},
\end{center}
\normalsize
\begin{align*}
\begin{gathered}
\text{This property \cref{RIP} holds,}\\
\Longrightarrow	\\
A \text{ approximately preserves the Euclidean length of $S$-sparse signals,}\\
\Longrightarrow\\
\text{implies that $S$-sparse vecotrs cannot be in the null space of $A$.}
\end{gathered}	
\end{align*}
This is useful as otherwise there would be no hope of reconstructing these vectors. An equivalent description of the RIP　is to say that all subsets of $S$ columns taken from $A$ are in fact \emph{nearly orthogonal}\footnote{the columns of $A$ cannot be exactly orthogonal since we have more columns than rows}.

To see connection between the RIP and CS, imagine $S$-sparse signals with $A$ wish to be acquired. Suppose that $\delta_{2S}$ is sufficiently less than one.
\begin{quote}
	This implies that all pairwise distances between $S$-sparse signals must be well preserved in the measurement space. That is 
	\begin{equation*}
	(1-\delta_{2S})\|x_1-x_2\|_{\ell_2}^2 \leq \|Ax_1-Ax_2\|_{\ell_2}^2 \leq (1+\delta_{2S})\|x_1-x_2\|_{\ell_2}^2
	\end{equation*}
	holds for all $S$-sparse vectors $x_1,\ x_2$.
\end{quote}





\subsection{General Signal Recovery From Undersampled Data}
If the RIP (\cref{RIP}) holds, then the following linear program gives an accurate reconstruction:
\begin{equation}
\label{eq9}
\min_{\tilde{x}\in\mathbb{R}^n}\|\tilde{x}\|_{\ell_1} \qquad \mbox{subject to} \qquad A\tilde{x}=y(=Ax).
\end{equation}

\begin{theorem}\cite{16}
	\label{th2}
	Assume that $\delta_{2S}<\sqrt{2}-1$. Then the solution $x^*$ to \cref{eq9} obeys
	\begin{eqnarray}
	\label{eq10}
%	\begin{gathered}
	\|x^*-x\|_{\ell_2} &\leq& C_0 \cdot \|x-x_S\|_{\ell_1}/\sqrt{S} \quad and \nonumber\\
	\|x^*-x\|_{\ell_1} &\leq& C_0 \cdot \|x-x_S\|_{\ell_1}
%	\end{gathered}
	\end{eqnarray}
	for some constant $C_0$, where $X_S$ is the vector $x$ with all but the largest $S$ components set to 0. \footnote{As stated, this result is due to the first author\cite{17} and yet unpublished, see also \cite{16} and \cite{18}}.
\end{theorem}

The conclusions of \cref{th2} are stronger than those of \cref{th1}. \textcolor[rgb]{1,0,0}{If $x$ is \textbf{not} $S$-sparse, then $x=x_S$ and, thus, the recovery is exact.} If $x$ is not $S$-sparse, then \cref{eq10} asserts that the quality of the recovered signal is as good as if one knew ahead of time the location of the $S$ largest values of $x$ and decided to measure those directly. The reconstruction is nearly as good as that provided by an oracle which, with full and perfect knowledge about x, extracts the S most significant pieces of information for us.
 
What is missing at this point is the relationship between $S$\footnote{the number of components one can effectively recover} obeying the hypothesis and $m$ the number of measurements or rows of the matrix. To derive powerful results, we would like to find \textcolor[rgb]{1,0,0}{\textbf{matrices obeying the RIP with values of $S$ close to $m$}}. 
 
\subsection{Robust Signal Recovery From Noisy Data}
It is given noisy data as in \cref{eq7} and use $\ell_1$ minimization with relaxed constraints for reconstruction:
\begin{equation}
\label{eq11}
\min\|\tilde{x}\|_{\ell_1} \qquad \mbox{subject to} \qquad \|A\tilde{x}-y\|_{\ell_2} \leq \epsilon
\end{equation}
where $\epsilon$ bounds the amount of noise in the data. (One could also consider recovery programs such as the \emph{Dantzig selector} \cite{19} or a combinatorial optimization program proposed by Haupt and Nowak \cite{20}; both algorithms have provable results \textbf{\emph{in the case where the noise is Gaussian with bounded variance}}.) Problem \cref{eq11} is often called the \textcolor[rgb]{1,0,0}{\textbf{LASSO}} after \cite{21}; see also \cite{22}. To the best of the author's knowledge, it was first proposed in \cite{Santosa1986}. This is again a convex problem (a second-order cone program) and can be solved efficiently.

\begin{theorem}\cite{16}
	\label{th3}
	Assume that $\delta_{2S}<\sqrt{2}-1$. Then the solution $x^*$ to \cref{eq11} obeys
	\begin{equation}
	\label{eq12}
	\|x^*-x\|_{\ell_2} \leq C_0 \cdot \|x-x_S\|_{\ell_1}/ \sqrt{S}+C_1 \cdot \epsilon
	\end{equation}
	for some constants $C_0$ and $C_1$. (Unpublished as stated and is a variation on the result found in \cite{16}).
\end{theorem}

This can hardly be simpler. The reconstruction error is bounded by the sum of two terms:
\begin{enumerate}[1)]
	\item The first is the error which would occur if one had noiseless data.
	\item The second is just proportional to the noise level.
\end{enumerate}
Further, $C_0$ and $C_1$ are typically small. With $\delta_{2S}=1/4$ for example, $C_0\leq 5.5$ and $C_1\leq 6$.

This last result establishes CS as a practical and robust sensing mechanism. It works with all kinds of not necessarily sparse signals, and it handles noise gracefully. What remains to be done is to design efficient sensing matrices obeying the RIP. This is \emph{the subject of}\footnote{以……为主题。 on the subject of--论及；以……为课题} the next section\cref{sectionrandomsensing}.

\section{Random Sensing}
\label{sectionrandomsensing}
Returning to the RIP, we would like to find sensing matrices with the property that column vectors 	taken from arbitrary subsets are nearly orthogonal. The larger these subsets, the better.

This is where randomness re-enters the pictures. Consider the following sensing matrices:
\begin{enumerate}[i)]
	\item form $A$ by sampling $n$ column vectors uniformly at random on the unit sphere of $\mathbb{R}^m$;
	\item form $A$ by sampling i.i.d.\cref{i.i.d} entries from the normal distribution with mean 0 and variance $1/m$;
	\item form $A$ by sampling a random projection $P$ as in ``Incoherent Sampling'' and normalize: $A=\sqrt{n/m}P$;
	\item form $A$ by sampling i.i.d.\cref{i.i.d} entries from a symmetric Bernoulli distribution ($P(A_{i,j}=\pm 1/ \sqrt{m})=1/2$) or other sub-Gaussian distribution.
\end{enumerate}
With overwhelming probability, all these matrices obey the RIP\cref{RIP} (i.e. the condition of our theorem\cref{th3}) provided that
\begin{equation}
\label{eq13}
m \geq C \cdot S\log(n/S),
\end{equation}
where C is some constant depending on each instance. The claims for i)-iii) use fairly standard results in probability theory; arguments in iv) are moire subtle\footnote{\cite{23}看一下}\cite{23,24}. In all these examples, the probability of sampling a matrix not obeying the RIP when \cref{eq13} holds is exponentially small in m. Interestingly, there are no measurement matrices and no reconstruction algorithm whatsoever which can give the conclusions of \cref{th2} with substantially fewer samples than the left-hand side of \cref{eq13} \cite{Candes2006,3}. In that sense, using randomized matrices together with $\ell_1$ minimization is a near-optimal sensing strategy.

One can also establish the RIP for pairs of orthobases as in ``Incoherence and the Sensing of Sparse Signals''. With $A=R\Phi\Psi$ where\textcolor[rgb]{1,0,0}{\emph{ $R$ extracts $m$ coordinates uniformly at random}}, it is sufficient to have
\begin{equation}
\label{eq14}
m \geq C \cdot S(\log n)^4,
\end{equation}
for the property to hold with large probability\cite{Candes2006,25}. If one wants a probability of failure no larger than $O(n^{-\beta})$ for some $\beta>0$, then the best known exponent in \cref{eq14} is \textbf{five} instead of \textbf{four} (it is believed that \cref{eq14} holds with just $\log n$).\textcolor[rgb]{1,0,0}{\textbf{\emph{This proves that one can stably and accurately reconstruct nearly sparse signals from dramatically undersampled data in an incoherent domain.}}}

Finally, the RIP can also hold for sensing matrices $A=\Phi\Psi$, where $\Psi$ is an \textcolor[rgb]{1,0,0}{arbitrary orthobasis }and $\Phi$ is an \textcolor[rgb]{1,0,0}{$m \times n$ measurement matrix drawn randomly from a suitable distribution}.

If one fixes $\Psi$ and populates $\Phi$ as in i)–iv), then with overwhelming probability, the matrix $A=\Phi\Psi$ obeys the RIP provided that \cref{eq13} is satisfied, where again $C$ is some constant depending on each instance.

These random measurement matrices $\Phi$ are in a sense \textcolor[rgb]{1,0,0}{\emph{universal}} \cite{23}; the sparsity basis need not even be known when designing the measurement system!


\section{What is Compressive Sampling}
\label{secwhatiscompressivesampling}
At the compression stage massive amount of data are collected only to be--in large part--discarded.

Extremely wasteful process of massive data acquisition followed by compression:

\begin{enumerate}
	\label{wastefulcompress}
	\item one acquires a high-resolution pixel array $f$,
	\item computes the complete set of transform coefficients,
	\item encode the largest coefficients and discard all the others,
	\item essentially ending up with $f_S$.
\end{enumerate}

CS operates very differently, and performs as ``if it were possible to directly acquire just the important information about the object of interest''.
\begin{eqnarray*}
\begin{gathered}
\mbox{Take about $O(S\log(n/S))$ random projections as in ``Random Sensing''},\\
\Longrightarrow\\
\mbox{enough information to reconstruct the signal with accuracy at least as good as that provided by $f_S$},\\
\Longrightarrow\\
\mbox{the best $S$-term approximation--the best compressed representation--of the object.}\\
Or,
\end{gathered}\\
\end{eqnarray*}
\begin{quote}
	CS measurement protocols essentially translate analog data into an already compressed digital form so that one can—-at least in principle-—obtain \textbf{\textcolor[rgb]{1,0,0}{super-resolved signals}}\footnote{超分辨信号} from just a few sensors. 
\end{quote}

There are some superficial similarities between CS and ideas in coding theory and more precisely with the theory and practice of Reed-Solomon (RS) codes \cite{26}. \textbf{One can adapt ideas from coding theory to establish the following:}
\begin{quote}
	one can uniquely reconstruct any $S$-sparse signal from the data of its first $2S$ \emph{Fourier coefficients}, $y_k=\sum_{t=0}^{n-1}x_te^{-i2\pi kt/n}$, $k=0,1,2,\cdots,2S-1,$ or from any set of $2S$ consecutive frequencies for that matter (the computational cost for the recovery is essentially that of solving an S × S Toeplitz system
	and of taking an n-point fast Fourier transform).
\end{quote}

But this technique is not suitable to sense compressible signals. There are two main reasons:
\begin{enumerate}
	\item First, the problem is that RS decoding is an algebraic technique, which cannot deal with nonsparse signals (the decoding finds the support by rooting a polynomial);
	\item second, the problem of finding the support of a signal --- even when the signal is exactly sparse --- from its first 2S Fourier coefficients is extraordinarily ill posed (the problem is the same as that of extrapolating a high degree polynomial from a small number of highly clustered values). 
\end{enumerate}
Tiny perturbations of these coefficients will give completely different answers so that with finite precision data, reliable estimation of the support is practically impossible.

Whereas purely algebraic methods 
\begin{enumerate}
	\item \textbf{ignore the conditioning of information operators},
	\item \textbf{having well-conditioned matrices}, 
\end{enumerate} 
which are crucial for accurate estimation, is a \textbf{\textcolor[rgb]{1,0,0}{central concern in CS as evidenced by the role played by the RIP}}.
\section{Applications}
\begin{description}
	\item[Data Compression] The sparse basis $\Psi$ may be unknown at the encoder or impractical to implement for data compression. As what is discussed in ``Random Sensing''. However, because a randomly designed $\Phi$ need not be with regards to the structure of $\Psi$, it can be considered a universal encoding strategy. (The knowledge and ability to implement $\Psi$ are required only for the decoding or recovery of f.) This universality may be particularly helpful for distributed source coding in multi-signal settings such as sensor networks \cite{27}.
	\item[Channel Coding] As explained in \cite{15}, CS principles (sparsity, randomness, and convex optimization) can be turned around and applied to design \emph{\textcolor[rgb]{1,0,0}{fast error correcting codes over the reals to protect from errors}} during transmission.
	\item[Inverse Problems]  In still other situations, the only way to acquire $f$ may be to use a measurement system $\Phi$ of a certain modality. However, assuming a sparse basis $\Psi$ exists for $f$ that is also incoherent with $\Phi$, then efficient sensing will be possible. One such application involves MR angiography \cite{Candes2006} and other types of MR setups \cite{28}, where $\Phi$ records a subset of the Fourier transform, and the desired image $f$ is sparse in the time or wavelet domains. 
	\item[Data Acquisition] Here, it could be helpful to design physical sampling devices that directly record discrete, low-rate incoherent measurements of the incident analog signal.
\end{description}

A CS camera that collects incoherent measurements using a digital micromirror array (and requires just one photosensitive element instead of millions) could significantly expand these capabilities. (See \cite{29} and an article by Duarte et al. in this issue.)

Two specific architectures for A/I \footnote{analog-to-information} was proposed, in which a discrete, low-rate sequence of incoherent measurements can be acquired from a high-bandwidth analog signal. \textbf{\textcolor[rgb]{1,0,0}{\emph{Each measurement $y_k$ can be interpreted as the inner product $\left<f,\varphi_k\right>$ of the incident analog signal $f$ against an analog measurement waveform $\varphi_k$}}}\label{key}. As in the discrete CS
framework, our preliminary results suggest that analog signals obeying a sparse or compressible model (in some analog dictionary $\Psi$ ) can be captured efficiently using these devices at a rate proportional to their information level instead of their Nyquist rate. There are challenges one must address when applying the discrete CS methodology to the recovery of sparse analog signals. The two architectures are as follows:
\begin{description}
	\item [Nonuniform Sampler (NUS)] The first architecture simply digitizes the signal at randomly or pseudo-randomly sampled time points. That is,
	\[
	y_k=f(t_k)=\left<f,\delta_{t_k}\right>.
	\]
	In effect, these time points are obtained by jittering nominal (low-rate) sample points located on a regular lattice. 
	\begin{quote}
		Due to the incoherence between spikes and sines, this architecture can be used to sample signals having sparse frequency spectra far below their Nyquist rate.
	\end{quote}
	There are tremendous benefits associated with a reduced sampling rate, as this provides added circuit settling time and has the effect of reducing the noise level.
	\item [Random Modulation Preintegration (RMPI)] The second architecture is \emph{\textcolor[rgb]{1,0,0}{applicable to a wider variety of sparsity domains}}, most notably those signals having a sparse signature in the time-frequency plane. \emph{\textcolor[rgb]{1,0,0}{Whereas}} it may not be possible to digitize an analog signal at a very high rate, it may be quite possible to change its polarity at a high rate.
	\begin{quote}
		The idea of the RMPI architecture is then to multiply the signal by a pseudo-random sequence of $\pm1s$, integrate the product over time windows, and digitize the integral at the end of each time interval. 
	\end{quote}
	This is a parallel architecture and one has several of these random multiplier-integrator pairs running in parallel using distinct sign sequences. In effect, the RMPI architecture correlates the signal with a \emph{\textcolor[rgb]{1,0,0}{bank of sequences of $\pm1$, one of the random CS measurement processes known to the universal, and therefore the RMPI measurements will be incoherent with any fixed time-frequency dictionary}} such as the Gabor dictionary described below.
\end{description}

For each of the above architectures, the author have confirmed numerically (and in some cases physically) that the system is robust to \emph{circuit nonidealities} such as  \emph{thermal noise}, \emph{clock timing errors}, \emph{interference}, and \emph{amplifier nonlinearities}.

The application of A/I architectures to realistic acquisition scenarios will require continued development of CS algorithms and theory. An final discrete example is concluded:
\begin{quote}
	Take $f$ to be a one-dimensional signal of length $n=512$ that contains two modulated pulses. \textbf{From this signal}, we collect $m=30$ measurements using an $m \times n$ measurement matrix $\Phi$ populated with i.i.d. Bernoulli $\pm1$ entries. This is an unreasonably small amount of data corresponding to an undersampling factor of over 17. \textbf{For reconstruction} we consider a \textcolor[rgb]{1,0,0}{Gabor dictionary $\Psi$} that consists of a variety of sine waves time limited by Gaussian windows, with different locations and scales. Overall the dictionary is approximately $43\times$ overcomplete and does not contain the two pulses that comprise of $f$. 
\end{quote}

There is more information on these directions in \cite{30}\footnote{看一下该文章\cite{30}}.。
